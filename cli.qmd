---
title: "CLI"
pagetitle: "CLI"
description: "Command Line Interface Tools"
image: "/static/cli.jpg"
---

```{python}
import pandas as pd

f = "./static/commands.xlsx"
```

---

# Operating Systems

## General

[SS64](https://ss64.com/)<br>
Reference guide containing syntax and examples for the most prevalent computing commands (Database and Operating System).<br>

### SSH

`ssh <username>@<host ip>`

## Mac

### Movie > Gif

`ffmpeg -i file.mov -s 600x400 -pix_fmt rgb24 -r 20 -f gif - | gifsicle --optimize=3 --delay=3 > file.gif`

## Windows

### CMD
[Microsoft Docs](https://docs.microsoft.com/en-us/windows-server/administration/windows-commands/cmd)<br>
Command line syntax

**Syntax**: `cmd [/c|/k] [/s] [/q] [/d] [/a|/u] [/t:{<b><f> | <f>}] [/e:{on | off}] [/f:{on | off}] [/v:{on | off}] [<string>]`

**Multiple commands**: `"<command1>&&<command2>&&<command3>"`

#### Parameters
```{python}
t1 = pd.read_excel(f, sheet_name="windows-commands")
t1.to_html(index=False)
```

### Powershell 

<div style="display:inline;">`r fa("hard-hat")` `r fa("hammer")` Under construction...</div>

---

## Linux

### Commands
```{python}
t2 = pd.read_excel(f, sheet_name="linux-commands")
t2.to_html(index=False)
```

### File System
```{python}
t3 = pd.read_excel(f, sheet_name="linux-filesystem")
t2.to_html(index=False)
```

### Permissions

General syntax: `_rwxrwxrwx 1 owner group`<br>

`_ | rwx | rwx | rwx` <Special><Owner><Group><All Users>

`r` = Read<br>
`w` = Write<br>
`x` = Execute

`chown` = Change ownership<br>
`chmod` = Change permissions<br>

`sudo chown <user>:<group> <file>`

`sudo chown 664 <file>`

### Package Manager

```{python}
t4 = pd.read_excel(f, sheet_name="linux-packages")
# Show all rows in table (no pagination)
t4.to_html(index=False)
```

### Server Setup

Initial Linux server setup on Digital Ocean

1. Login to server as root
1. Create new admin user: `$ adduser <username>`
1. Add user to sudo group: `usermod -aG sudo <username>`
1. `$ exit`
1. Reload SSH connection
1. Login as new user
1. `$ sudo vi /etc/ssh/sshd_config`
  1. Configuration for putty/incoming SSH connections
  1. Change `PermitRootLogin` to `no`
1. `$ sudo reboot`
1. On boot, can see linux kernel used for current build/image: `GNU/Linux 5.4.0-51-generic`
1. Check for package updates `$ apt list --upgradable`
1, Upgrade packages `$ sudo apt upgrade`
1. Show system info: `$ uname -a`
1. Even though you updated the kernel, need to reboot to take effect: `$ sudo reboot`
1. Just to be sure: `$ sudo apt update & sudo apt upgrade`
1. Uncomplicated firewall: `$ ufw status verbose`
1. Allow SSH connections: `$ sudo ufw allow OpenSSH`
  1. Can also do `$ sudo ufw allow 'Apache Full'` if web server needed. Opens port 80 and 443.
1. Turn on firewall: `$ sudo ufw enable`
1. Check: `$ sudo ufw status verbose`
1. Shutdown for backup/snapshot `$ sudo shutdown -h now`
1. Navigate to provider website and restart machine

---

# Docker

## Resources

- [CheatSheet](./static/Docker-CheatSheet.pdf)
- [Convert Docker command to docker-compose.yml](https://www.composerize.com/)

## Image vs. Container

**Image** - Application we want to run

**Container** - Instance of that image running as a process

---

## Docker Basics

### Create an Nginx container

`docker run -p 80:80 -d --name webhost nginx`

1. Downloads Nginx from Docker Hub
1. Starts new container from that image
1. Opened port 80 on host IP
1. Routes port 80 traffic to the container IP, port 80
1. View container at [http://localhost:80](http://localhost:80)

### Other examples

`docker run -p 80:80 -d --name nginx nginx`

`docker run -p 8080:80 -d --name httpd httpd`

`docker run -p 3306:3306 --platform linux/amd64 -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql`

Create a JupyterLab instance and attach your current directory as a volume: `docker run -it --rm -p 8888:8888 -v $(PWD):/home/jovyan jupyter/pyspark-notebook`

### Processes and configurations

Check processes running inside a container: `docker top <container>`

Container configuration: `docker <container> inspect`

Check container stats (memory, cpu, network): `docker stats <container>`

### Getting a shell inside containers

Start a new container interactively: `docker run -it <container>`

Run commands in existing container: `docker exec -it <container>`

#### Example: Start a container interactively and launch bash within it

1. Start container and launch bash: `docker run -it --name ubuntu ubuntu bash`
1. Run some bash command: `apt-get install -y curl`
1. Exit the container: `exit`
1. Start and re-enter the container: `docker start -ai ubuntu`

#### Example: Launch shell in running container

`docker exec -it <container> bash`

### Pull an image from docker hub

`docker pull <imagename>`

---

## Docker Networks

-   Each container is connected to a private virtual network (called "bridge").
-   Each virtual network routes through NAT firewall on host IP.
-   All containers on a virtual network can talk to each other without `-p`
-   **_Best practice_**: Create a new virtual network for each app.
-   You can skip virtual networks and use the host IP (`--net=host`).

Get container IP: `docker inspect --format '{{ .NetworkSettings.IPAddress }}' <container>`

### Publishing (#:#)

example: 8080:80

**left number**: published/host port

**right number**: listening/container port

_Traffic passing through port 8080 on the HOST will be directed to port 80 on the container._

### DNS

Docker uses container names as host names.

Dont rely on IPs for inter-communication.

**_Best Practice_** Always use custom networks.

#### Assignment

Check different curl versions within current versions of Ubuntu and CentOS.

Run "curl --version" on both operating systems.

##### Steps

**ubuntu**: `apt-get update && apt-get install curl`

**centos**: `yum update curl`

Then...

`curl --version`

Also:

Check out command `docker --rm`

## Dockerfiles

Recipe for creating images 

Each Dockerfile stanza such as "RUN", "CMD", etc. are stored as a single image layer. Docker caches each layer by giving it a unique SHA (hash), so whenever the image is (re)built, it can check to see if a layer has changed, and if not, it will use the cached layer.

Docker builds images top down, so it is best practice to structure the Dockerfile in such a way that lines which will change the most are at the bottom, and lines that will change the least are at the top. If a line is changed (ie. source code changes) Docker will rebuild that line, and thus each line after that will also need to be rebuilt.

## Keeping the Docker system clean

`docker system prune`
- all stopped containers
- all networks not used by at least one container
- all dangling images
- all dangling build cache

## Volumes an Bind Mounts

**Volumes** - Special location outside of container UFS

**Bind Mounts** - Link container path to host path

Build an image and ***named*** volume (persistent): `docker run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql:/var/lib/mysql --platform linux/amd64 mysql`

---

# Git

- [https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners](https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners)
- [https://docs.gitlab.com/ee/gitlab-basics/start-using-git.html](https://docs.gitlab.com/ee/gitlab-basics/start-using-git.html)

## Initialize
1) Launch Git Bash
2) Navigate to project directory
3) initialize git repository in the folder root: `git init`
4) create new file in directory: `touch filename.extension`
5) list files in root: `ls`
6) check which files git recognizes: `git status`


## Staging
A commit is a record of what files you have changed since the last time you made a commit. 
Essentially, you make changes to your repo (for example, adding a file or modifying one) 
and then tell git to put those files into a commit.
Commits make up the essence of your project and allow you to go back to the state of a project at any point.

So, how do you tell git which files to put into a commit? 
This is where the staging environment or index come in. 
When you make changes to your repo, git notices that a file has changed but 
won't do anything with it (like adding it in a commit).

To add a file to a commit, you first need to add it to the staging environment. 
To do this, you can use the `git add <filename>` command.

Once you've used the git add command to add all the files you want to the staging environment, 
you can then tell git to package them into a commit using the git commit command. 
Note: The staging environment, also called 'staging', is the new preferred term for this, 
but you can also see it referred to as the 'index'.

1) Add files to the staging environment: `git add filename.extension`
2) Check staging environment for new files: `git status`


## Commit Locally
`git commit -m "Your message about the commit"`


## Branches
Say you want to make a new feature but are worried about making changes to the main project 
while developing the feature. This is where git branches come in. 

Branches allow you to move back and forth between 'states' of a project. 
For instance, if you want to add a new page to your website you can create a new branch just 
for that page without affecting the main part of the project. Once you're done with the page, 
you can merge your changes from your branch into the master branch. 
When you create a new branch, Git keeps track of which commit your branch 'branched' off of, 
so it knows the history behind all the files. 

1) `git checkout -b <my branch name>`
2) Show list of branches: `git branch`


## Commit to Github
1) Create new repo on GitHub
2) `git remote add origin <url produced on github for new repo>`
3) `git push -u origin [master/main]`


## Push a Branch to Github
`git push origin <my-new-branch>`

You might be wondering what that "origin" word means in the command above. 
What happens is that when you clone a remote repository to your local machine, git creates an alias for you. 
In nearly all cases this alias is called "origin." It's essentially shorthand for the remote repository's URL. 
So, to push your changes to the remote repository, 
you could've used either the command: git push git@github.com:git/git.git yourbranchname or git push origin yourbranchname


## Pull Request
A pull request (or PR) is a way to alert a repo's owners that you want to make some changes to their code. 
It allows them to review the code and make sure it looks good before putting your changes on the master branch.


## Get Changes on Github
`git pull origin master`

check all new commits: `git log`


## View Differences
1) run: `git diff`


## Remove a Branch
### Locally
`git branch -d <branch_name>`

### Remote
`git push <remote_name> --delete <branch_name>`

## Remove tracked file/directory
### File
`git rm --cached <file>`

### Directory
`git rm --cahced -r dir/`

---

# psql
[Cheat sheet](https://quickref.me/postgres)
```{python}
t4 = pd.read_excel(f, sheet_name="psql-commands")
t4.to_html(index=False)
```

## Export table to CSV

- `\copy table TO '<path>' CSV`
- `\copy table(col1,col1) TO '<path>' CSV`
- `\copy (SELECT...) TO '<path>' CSV`


## Backup

Use pg_dumpall to backup all databases

`$ pg_dumpall -U postgres > all.sql`

Use pg_dump to backup a database

`$ pg_dump -d mydb -f mydb_backup.sql`

- &nbsp; `-a` &nbsp; Dump only the data, not the schema
- &nbsp; `-s` &nbsp; Dump only the schema, no data
- &nbsp; `-c` &nbsp; Drop database before recreating
- &nbsp; `-C` &nbsp; Create database before restoring
- &nbsp; `-t` &nbsp; Dump the named table(s) only
- &nbsp; `-F` &nbsp; Format (`c`: custom, `d`: directory, `t`: tar)

Use `pg_dump -?` to get the full list of options


## Restore

### psql

`$ psql -U user mydb < mydb_backup.sql`

### pg_restore

`$ pg_restore -d mydb mydb_backup.sql -c`

- `-U   Specify a database user`
- `-c   Drop database before recreating`
- `-C   Create database before restoring`
- `-e   Exit if an error has encountered`
- `-F   Format (c: custom, d: directory, t: tar, p: plain text sql(default))`

Use pg_restore -? to get the full list of options
