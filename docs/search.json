[
  {
    "objectID": "cli.html",
    "href": "cli.html",
    "title": "CLI",
    "section": "",
    "text": "import pandas as pd\n\nf = \"./static/commands.xlsx\""
  },
  {
    "objectID": "cli.html#general",
    "href": "cli.html#general",
    "title": "CLI",
    "section": "General",
    "text": "General\nSS64 Reference guide containing syntax and examples for the most prevalent computing commands (Database and Operating System).\n\nSSH\nssh <username>@<host ip>"
  },
  {
    "objectID": "cli.html#mac",
    "href": "cli.html#mac",
    "title": "CLI",
    "section": "Mac",
    "text": "Mac\n\nMovie > Gif\nffmpeg -i file.mov -s 600x400 -pix_fmt rgb24 -r 20 -f gif - | gifsicle --optimize=3 --delay=3 > file.gif"
  },
  {
    "objectID": "cli.html#windows",
    "href": "cli.html#windows",
    "title": "CLI",
    "section": "Windows",
    "text": "Windows\n\nCMD\nMicrosoft Docs Command line syntax\nSyntax: cmd [/c|/k] [/s] [/q] [/d] [/a|/u] [/t:{<b><f> | <f>}] [/e:{on | off}] [/f:{on | off}] [/v:{on | off}] [<string>]\nMultiple commands: \"<command1>&&<command2>&&<command3>\"\n\nParameters\n\nt1 = pd.read_excel(f, sheet_name=\"windows-commands\")\nt1.to_html(index=False)\n\n\n\n\n\n\n\n\n\nParameter\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\n/c\n\n\n\nCarries out the command specified by string and then stops.\n\n\n\n\n\n\n\n/k\n\n\n\nCarries out the command specified by string and continues.\n\n\n\n\n\n\n\n/s\n\n\n\nModifies the treatment of string after /c or /k.\n\n\n\n\n\n\n\n/q\n\n\n\nTurns the echo off.\n\n\n\n\n\n\n\n/d\n\n\n\nDisables execution of AutoRun commands.\n\n\n\n\n\n\n\n/a\n\n\n\nFormats internal command output to a pipe or a file as American National Standards Institute (ANSI).\n\n\n\n\n\n\n\n/u\n\n\n\nFormats internal command output to a pipe or a file as Unicode.\n\n\n\n\n\n\n\n/t:{<b><f> <f>}\n\n\n\nSets the background (b) and foreground (f) colors.\n\n\n\n\n\n\n\n/e:on\n\n\n\nEnables command extensions.\n\n\n\n\n\n\n\n/e:off\n\n\n\nDisables commands extensions.\n\n\n\n\n\n\n\n/f:on\n\n\n\nEnables file and directory name completion.\n\n\n\n\n\n\n\n/f:off\n\n\n\nDisables file and directory name completion.\n\n\n\n\n\n\n\n/v:on\n\n\n\nEnables delayed environment variable expansion.\n\n\n\n\n\n\n\n/v:off\n\n\n\nDisables delayed environment variable expansion.\n\n\n\n\n\n\n\n<string>\n\n\n\nSpecifies the command you want to carry out.\n\n\n\n\n\n\n\n/?\n\n\n\nDisplays help at the command prompt.\n\n\n\n\n\n\n\n\n\n\n\n\n\nPowershell\n\nr fa(\"hard-hat\") r fa(\"hammer\") Under construction…"
  },
  {
    "objectID": "cli.html#linux",
    "href": "cli.html#linux",
    "title": "CLI",
    "section": "Linux",
    "text": "Linux\n\nCommands\n\nt2 = pd.read_excel(f, sheet_name=\"linux-commands\")\nt2.to_html(index=False)\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\nDetail\n\n\n\n\n\n\n\n\n\n\n\nsudo\n\n\n\nRun as administrator (superuser do)\n\n\n\n\n\n\n\napt-get\n\n\n\nInstall/uninstall programs\n\n\n\n\n\n\n\nclear\n\n\n\nClear terminal\n\n\n\n\n\n\n\npwd\n\n\n\nPrint working directory\n\n\n\n\n\n\n\ncd\n\n\n\nChange directory\n\n\n\n\n\n\n\ncd /\n\n\n\nChange directory to absolute path from root\n\n\n\n\n\n\n\ncd ~\n\n\n\nHome directory\n\n\n\n\n\n\n\ncd ./\n\n\n\nSame directory\n\n\n\n\n\n\n\ncd ~/Documents\n\n\n\n/home/<username>/Documents\n\n\n\n\n\n\n\nls\n\n\n\nList contents of directory\n\n\n\n\n\n\n\nls -l\n\n\n\nShows contents and their permissions\n\n\n\n\n\n\n\nls -p\n\n\n\nShows type of content\n\n\n\n\n\n\n\nls ~/Documents\n\n\n\nExample\n\n\n\n\n\n\n\nsu <username>\n\n\n\nSwitch user\n\n\n\n\n\n\n\nsudo su\n\n\n\nSwitch to root user (full control of everything)\n\n\n\n\n\n\n\nxdg-open .\n\n\n\nOpen default file manager in current directory\n\n\n\n\n\n\n\ncaja .\n\n\n\nOpen Caja in current directory\n\n\n\n\n\n\n\ncp [source] [destination]\n\n\n\nCopy file\n\n\n\n\n\n\n\ntouch <file>.<ext>\n\n\n\nCreate file\n\n\n\n\n\n\n\nmkdir\n\n\n\nMake directory\n\n\n\n\n\n\n\nmv <source> <destination>\n\n\n\nMove file\n\n\n\n\n\n\n\ncopy <source> <destination>\n\n\n\nCopy file\n\n\n\n\n\n\n\nfind <directory> -type f -name “*.txt”\n\n\n\nFind files with .py extension\n\n\n\n\n\n\n\ngrep\n\n\n\nsearch for content in file(s)\n\n\n\n\n\n\n\ngrep -n -I <file>\n\n\n\nsearch for content in files, list line number, ignore case\n\n\n\n\n\n\n\nfind <directory> -type f -name “*.txt” | grep “sometext”\n\n\n\nSearch for “.txt” files that contain “sometext” in them\n\n\n\n\n\n\n\nrm\n\n\n\nremove file\n\n\n\n\n\n\n\nrm -d\n\n\n\nremove empty directory\n\n\n\n\n\n\n\nrm -r\n\n\n\nremove directory and files\n\n\n\n\n\n\n\n\n\n\n\n\nFile System\n\nt3 = pd.read_excel(f, sheet_name=\"linux-filesystem\")\nt2.to_html(index=False)\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\nDetail\n\n\n\n\n\n\n\n\n\n\n\nsudo\n\n\n\nRun as administrator (superuser do)\n\n\n\n\n\n\n\napt-get\n\n\n\nInstall/uninstall programs\n\n\n\n\n\n\n\nclear\n\n\n\nClear terminal\n\n\n\n\n\n\n\npwd\n\n\n\nPrint working directory\n\n\n\n\n\n\n\ncd\n\n\n\nChange directory\n\n\n\n\n\n\n\ncd /\n\n\n\nChange directory to absolute path from root\n\n\n\n\n\n\n\ncd ~\n\n\n\nHome directory\n\n\n\n\n\n\n\ncd ./\n\n\n\nSame directory\n\n\n\n\n\n\n\ncd ~/Documents\n\n\n\n/home/<username>/Documents\n\n\n\n\n\n\n\nls\n\n\n\nList contents of directory\n\n\n\n\n\n\n\nls -l\n\n\n\nShows contents and their permissions\n\n\n\n\n\n\n\nls -p\n\n\n\nShows type of content\n\n\n\n\n\n\n\nls ~/Documents\n\n\n\nExample\n\n\n\n\n\n\n\nsu <username>\n\n\n\nSwitch user\n\n\n\n\n\n\n\nsudo su\n\n\n\nSwitch to root user (full control of everything)\n\n\n\n\n\n\n\nxdg-open .\n\n\n\nOpen default file manager in current directory\n\n\n\n\n\n\n\ncaja .\n\n\n\nOpen Caja in current directory\n\n\n\n\n\n\n\ncp [source] [destination]\n\n\n\nCopy file\n\n\n\n\n\n\n\ntouch <file>.<ext>\n\n\n\nCreate file\n\n\n\n\n\n\n\nmkdir\n\n\n\nMake directory\n\n\n\n\n\n\n\nmv <source> <destination>\n\n\n\nMove file\n\n\n\n\n\n\n\ncopy <source> <destination>\n\n\n\nCopy file\n\n\n\n\n\n\n\nfind <directory> -type f -name “*.txt”\n\n\n\nFind files with .py extension\n\n\n\n\n\n\n\ngrep\n\n\n\nsearch for content in file(s)\n\n\n\n\n\n\n\ngrep -n -I <file>\n\n\n\nsearch for content in files, list line number, ignore case\n\n\n\n\n\n\n\nfind <directory> -type f -name “*.txt” | grep “sometext”\n\n\n\nSearch for “.txt” files that contain “sometext” in them\n\n\n\n\n\n\n\nrm\n\n\n\nremove file\n\n\n\n\n\n\n\nrm -d\n\n\n\nremove empty directory\n\n\n\n\n\n\n\nrm -r\n\n\n\nremove directory and files\n\n\n\n\n\n\n\n\n\n\n\n\nPermissions\nGeneral syntax: _rwxrwxrwx 1 owner group\n_ | rwx | rwx | rwx \nr = Read w = Write x = Execute\nchown = Change ownership chmod = Change permissions\nsudo chown <user>:<group> <file>\nsudo chown 664 <file>\n\n\nPackage Manager\n\nt4 = pd.read_excel(f, sheet_name=\"linux-packages\")\n# Show all rows in table (no pagination)\nt4.to_html(index=False)\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\nDetail\n\n\n\n\n\n\n\n\n\n\n\napt-get\n\n\n\nPackage manager (works with .deb files)\n\n\n\n\n\n\n\napt-get install <program>\n\n\n\nInstall a program (if exists in apt-get repository)\n\n\n\n\n\n\n\napt-get remove\n\n\n\nRemove a program\n\n\n\n\n\n\n\napt-cache search <program>*\n\n\n\nSearch for a program in repository (* = wildcard)\n\n\n\n\n\n\n\napt-cache policy <program>\n\n\n\nCheck if program is installed\n\n\n\n\n\n\n\nsudo dpkg -i ~/Downloads/<package>.deb\n\n\n\nInstall .deb file/package\n\n\n\n\n\n\n\nsudo apt-get upgrade\n\n\n\nUpgrade all packages\n\n\n\n\n\n\n\n\n\n\n\n\nServer Setup\nInitial Linux server setup on Digital Ocean\n\nLogin to server as root\nCreate new admin user: $ adduser <username>\nAdd user to sudo group: usermod -aG sudo <username>\n$ exit\nReload SSH connection\nLogin as new user\n$ sudo vi /etc/ssh/sshd_config\nConfiguration for putty/incoming SSH connections\nChange PermitRootLogin to no\n$ sudo reboot\nOn boot, can see linux kernel used for current build/image: GNU/Linux 5.4.0-51-generic\nCheck for package updates $ apt list --upgradable 1, Upgrade packages $ sudo apt upgrade\nShow system info: $ uname -a\nEven though you updated the kernel, need to reboot to take effect: $ sudo reboot\nJust to be sure: $ sudo apt update & sudo apt upgrade\nUncomplicated firewall: $ ufw status verbose\nAllow SSH connections: $ sudo ufw allow OpenSSH\nCan also do $ sudo ufw allow 'Apache Full' if web server needed. Opens port 80 and 443.\nTurn on firewall: $ sudo ufw enable\nCheck: $ sudo ufw status verbose\nShutdown for backup/snapshot $ sudo shutdown -h now\nNavigate to provider website and restart machine"
  },
  {
    "objectID": "cli.html#resources",
    "href": "cli.html#resources",
    "title": "CLI",
    "section": "Resources",
    "text": "Resources\n\nCheatSheet\nConvert Docker command to docker-compose.yml"
  },
  {
    "objectID": "cli.html#image-vs.-container",
    "href": "cli.html#image-vs.-container",
    "title": "CLI",
    "section": "Image vs. Container",
    "text": "Image vs. Container\nImage - Application we want to run\nContainer - Instance of that image running as a process"
  },
  {
    "objectID": "cli.html#docker-basics",
    "href": "cli.html#docker-basics",
    "title": "CLI",
    "section": "Docker Basics",
    "text": "Docker Basics\n\nCreate an Nginx container\ndocker run -p 80:80 -d --name webhost nginx\n\nDownloads Nginx from Docker Hub\nStarts new container from that image\nOpened port 80 on host IP\nRoutes port 80 traffic to the container IP, port 80\nView container at http://localhost:80\n\n\n\nOther examples\ndocker run -p 80:80 -d --name nginx nginx\ndocker run -p 8080:80 -d --name httpd httpd\ndocker run -p 3306:3306 --platform linux/amd64 -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql\nCreate a JupyterLab instance and attach your current directory as a volume: docker run -it --rm -p 8888:8888 -v $(PWD):/home/jovyan jupyter/pyspark-notebook\n\n\nProcesses and configurations\nCheck processes running inside a container: docker top <container>\nContainer configuration: docker <container> inspect\nCheck container stats (memory, cpu, network): docker stats <container>\n\n\nGetting a shell inside containers\nStart a new container interactively: docker run -it <container>\nRun commands in existing container: docker exec -it <container>\n\nExample: Start a container interactively and launch bash within it\n\nStart container and launch bash: docker run -it --name ubuntu ubuntu bash\nRun some bash command: apt-get install -y curl\nExit the container: exit\nStart and re-enter the container: docker start -ai ubuntu\n\n\n\nExample: Launch shell in running container\ndocker exec -it <container> bash\n\n\n\nPull an image from docker hub\ndocker pull <imagename>"
  },
  {
    "objectID": "cli.html#docker-networks",
    "href": "cli.html#docker-networks",
    "title": "CLI",
    "section": "Docker Networks",
    "text": "Docker Networks\n\nEach container is connected to a private virtual network (called “bridge”).\nEach virtual network routes through NAT firewall on host IP.\nAll containers on a virtual network can talk to each other without -p\nBest practice: Create a new virtual network for each app.\nYou can skip virtual networks and use the host IP (--net=host).\n\nGet container IP: docker inspect --format '{{ .NetworkSettings.IPAddress }}' <container>\n\nPublishing (#:#)\nexample: 8080:80\nleft number: published/host port\nright number: listening/container port\nTraffic passing through port 8080 on the HOST will be directed to port 80 on the container.\n\n\nDNS\nDocker uses container names as host names.\nDont rely on IPs for inter-communication.\nBest Practice Always use custom networks.\n\nAssignment\nCheck different curl versions within current versions of Ubuntu and CentOS.\nRun “curl –version” on both operating systems.\n\nSteps\nubuntu: apt-get update && apt-get install curl\ncentos: yum update curl\nThen…\ncurl --version\nAlso:\nCheck out command docker --rm"
  },
  {
    "objectID": "cli.html#dockerfiles",
    "href": "cli.html#dockerfiles",
    "title": "CLI",
    "section": "Dockerfiles",
    "text": "Dockerfiles\nRecipe for creating images\nEach Dockerfile stanza such as “RUN”, “CMD”, etc. are stored as a single image layer. Docker caches each layer by giving it a unique SHA (hash), so whenever the image is (re)built, it can check to see if a layer has changed, and if not, it will use the cached layer.\nDocker builds images top down, so it is best practice to structure the Dockerfile in such a way that lines which will change the most are at the bottom, and lines that will change the least are at the top. If a line is changed (ie. source code changes) Docker will rebuild that line, and thus each line after that will also need to be rebuilt."
  },
  {
    "objectID": "cli.html#keeping-the-docker-system-clean",
    "href": "cli.html#keeping-the-docker-system-clean",
    "title": "CLI",
    "section": "Keeping the Docker system clean",
    "text": "Keeping the Docker system clean\ndocker system prune - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache"
  },
  {
    "objectID": "cli.html#volumes-an-bind-mounts",
    "href": "cli.html#volumes-an-bind-mounts",
    "title": "CLI",
    "section": "Volumes an Bind Mounts",
    "text": "Volumes an Bind Mounts\nVolumes - Special location outside of container UFS\nBind Mounts - Link container path to host path\nBuild an image and named volume (persistent): docker run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql:/var/lib/mysql --platform linux/amd64 mysql"
  },
  {
    "objectID": "cli.html#initialize",
    "href": "cli.html#initialize",
    "title": "CLI",
    "section": "Initialize",
    "text": "Initialize\n\nLaunch Git Bash\nNavigate to project directory\ninitialize git repository in the folder root: git init\ncreate new file in directory: touch filename.extension\nlist files in root: ls\ncheck which files git recognizes: git status"
  },
  {
    "objectID": "cli.html#staging",
    "href": "cli.html#staging",
    "title": "CLI",
    "section": "Staging",
    "text": "Staging\nA commit is a record of what files you have changed since the last time you made a commit. Essentially, you make changes to your repo (for example, adding a file or modifying one) and then tell git to put those files into a commit. Commits make up the essence of your project and allow you to go back to the state of a project at any point.\nSo, how do you tell git which files to put into a commit? This is where the staging environment or index come in. When you make changes to your repo, git notices that a file has changed but won’t do anything with it (like adding it in a commit).\nTo add a file to a commit, you first need to add it to the staging environment. To do this, you can use the git add <filename> command.\nOnce you’ve used the git add command to add all the files you want to the staging environment, you can then tell git to package them into a commit using the git commit command. Note: The staging environment, also called ‘staging’, is the new preferred term for this, but you can also see it referred to as the ‘index’.\n\nAdd files to the staging environment: git add filename.extension\nCheck staging environment for new files: git status"
  },
  {
    "objectID": "cli.html#commit-locally",
    "href": "cli.html#commit-locally",
    "title": "CLI",
    "section": "Commit Locally",
    "text": "Commit Locally\ngit commit -m \"Your message about the commit\""
  },
  {
    "objectID": "cli.html#branches",
    "href": "cli.html#branches",
    "title": "CLI",
    "section": "Branches",
    "text": "Branches\nSay you want to make a new feature but are worried about making changes to the main project while developing the feature. This is where git branches come in.\nBranches allow you to move back and forth between ‘states’ of a project. For instance, if you want to add a new page to your website you can create a new branch just for that page without affecting the main part of the project. Once you’re done with the page, you can merge your changes from your branch into the master branch. When you create a new branch, Git keeps track of which commit your branch ‘branched’ off of, so it knows the history behind all the files.\n\ngit checkout -b <my branch name>\nShow list of branches: git branch"
  },
  {
    "objectID": "cli.html#commit-to-github",
    "href": "cli.html#commit-to-github",
    "title": "CLI",
    "section": "Commit to Github",
    "text": "Commit to Github\n\nCreate new repo on GitHub\ngit remote add origin <url produced on github for new repo>\ngit push -u origin [master/main]"
  },
  {
    "objectID": "cli.html#push-a-branch-to-github",
    "href": "cli.html#push-a-branch-to-github",
    "title": "CLI",
    "section": "Push a Branch to Github",
    "text": "Push a Branch to Github\ngit push origin <my-new-branch>\nYou might be wondering what that “origin” word means in the command above. What happens is that when you clone a remote repository to your local machine, git creates an alias for you. In nearly all cases this alias is called “origin.” It’s essentially shorthand for the remote repository’s URL. So, to push your changes to the remote repository, you could’ve used either the command: git push git@github.com:git/git.git yourbranchname or git push origin yourbranchname"
  },
  {
    "objectID": "cli.html#pull-request",
    "href": "cli.html#pull-request",
    "title": "CLI",
    "section": "Pull Request",
    "text": "Pull Request\nA pull request (or PR) is a way to alert a repo’s owners that you want to make some changes to their code. It allows them to review the code and make sure it looks good before putting your changes on the master branch."
  },
  {
    "objectID": "cli.html#get-changes-on-github",
    "href": "cli.html#get-changes-on-github",
    "title": "CLI",
    "section": "Get Changes on Github",
    "text": "Get Changes on Github\ngit pull origin master\ncheck all new commits: git log"
  },
  {
    "objectID": "cli.html#view-differences",
    "href": "cli.html#view-differences",
    "title": "CLI",
    "section": "View Differences",
    "text": "View Differences\n\nrun: git diff"
  },
  {
    "objectID": "cli.html#remove-a-branch",
    "href": "cli.html#remove-a-branch",
    "title": "CLI",
    "section": "Remove a Branch",
    "text": "Remove a Branch\n\nLocally\ngit branch -d <branch_name>\n\n\nRemote\ngit push <remote_name> --delete <branch_name>"
  },
  {
    "objectID": "cli.html#remove-tracked-filedirectory",
    "href": "cli.html#remove-tracked-filedirectory",
    "title": "CLI",
    "section": "Remove tracked file/directory",
    "text": "Remove tracked file/directory\n\nFile\ngit rm --cached <file>\n\n\nDirectory\ngit rm --cahced -r dir/"
  },
  {
    "objectID": "cli.html#export-table-to-csv",
    "href": "cli.html#export-table-to-csv",
    "title": "CLI",
    "section": "Export table to CSV",
    "text": "Export table to CSV\n\n\\copy table TO '<path>' CSV\n\\copy table(col1,col1) TO '<path>' CSV\n\\copy (SELECT...) TO '<path>' CSV"
  },
  {
    "objectID": "cli.html#backup",
    "href": "cli.html#backup",
    "title": "CLI",
    "section": "Backup",
    "text": "Backup\nUse pg_dumpall to backup all databases\n$ pg_dumpall -U postgres > all.sql\nUse pg_dump to backup a database\n$ pg_dump -d mydb -f mydb_backup.sql\n\n  -a   Dump only the data, not the schema\n  -s   Dump only the schema, no data\n  -c   Drop database before recreating\n  -C   Create database before restoring\n  -t   Dump the named table(s) only\n  -F   Format (c: custom, d: directory, t: tar)\n\nUse pg_dump -? to get the full list of options"
  },
  {
    "objectID": "cli.html#restore",
    "href": "cli.html#restore",
    "title": "CLI",
    "section": "Restore",
    "text": "Restore\n\npsql\n$ psql -U user mydb < mydb_backup.sql\n\n\npg_restore\n$ pg_restore -d mydb mydb_backup.sql -c\n\n-U   Specify a database user\n-c   Drop database before recreating\n-C   Create database before restoring\n-e   Exit if an error has encountered\n-F   Format (c: custom, d: directory, t: tar, p: plain text sql(default))\n\nUse pg_restore -? to get the full list of options"
  },
  {
    "objectID": "dbms.html",
    "href": "dbms.html",
    "title": "DBMS",
    "section": "",
    "text": "Reference\npg_dump -h [host] -d [database] -U [user] -s [schema only] -W [force password] > <file>.sql\n\n\n\nColumns\n\ntable_schema: PK schema name\ntable_name: PK table name\nconstraint_name: PK constraint name\nposition: index of column in table (1, 2, …). 2 or higher means key is composite (contains more than one column)\nkey_column: PK column name\n\nRows\n\nOne row represents one primary key column\nScope of rows: columns of all PK constraints in a database\nOrdered by table schema, table name, column position\n\nselect * from (\n    -- Main query. Returns all tables\n    select kcu.table_schema,\n           kcu.table_name,\n           tco.constraint_name,\n           kcu.ordinal_position as position,\n           kcu.column_name as key_column\n    from information_schema.table_constraints tco\n    join information_schema.key_column_usage kcu \n         on kcu.constraint_name = tco.constraint_name\n         and kcu.constraint_schema = tco.constraint_schema\n         and kcu.constraint_name = tco.constraint_name\n    where tco.constraint_type = 'PRIMARY KEY'\n    order by kcu.table_schema,\n             kcu.table_name,\n             position\n) main\nwhere table_name = 'd_location'\n\n\n\nColumns\n\nforeign_table: foreign table schema and name\nrel: relationship symbol implicating direction\nprimary_table: primary (rerefenced) table schema and name\nfk_columns: list of FK colum names, separated with “,”\nconstraint_name: foreign key constraint name\n\nRows\n\nOne row represents one foreign key.\nIf foreign key consists of multiple columns (composite key) it is still represented as one row.\nScope of rows: all foregin keys in a database.\nOrdered by foreign table schema name and table name.\n\n\n\n\ndrop table if exists dependencies cascade;\ncreate temporary table dependencies as\nselect \n        tc.table_name as child,\n        tu.table_name as parent\nfrom \n        information_schema.table_constraints as tc\n        inner join information_schema.constraint_table_usage as tu\n             on tu.constraint_name = tc.constraint_name\nwhere \n        tc.constraint_type = 'FOREIGN KEY'\n        and tc.table_name <> tu.table_name;\n\nwith recursive dep_depth as (\n select\n  dep.child,\n  dep.parent,\n  1 as lvl\n from\n  dependencies as dep\n union all\n select\n  dep.child,\n  dep.parent,\n  dd.lvl + 1 as lvl\n from\n  dep_depth as dd\n  inner join dependencies as dep on dep.parent = dd.child\n )\nselect\n table_name,\n table_order\nfrom (\n select\n  dd.parent as table_name,\n  max(lvl) as table_order\n from\n  dep_depth as dd\n group by\n  table_name\n union\n select\n  dd.child as table_name,\n  max(lvl) + 1 as level\n from\n  dep_depth as dd\n  left join dependencies as dp on dp.parent = dd.child\n where\n  dp.parent is null\n group by\n  dd.child\n ) as all_levels\n order by table_order;\nselect * from (\n    -- Main query. Returns FK relationships for all tables\n    select \n            kcu.table_schema as table_schema,\n            kcu.table_name as foreign_table,\n           '>-' as relationship,\n           rel_tco.table_name as primary_table,\n           string_agg(kcu.column_name, ', ') as fk_columns,\n           kcu.constraint_name\n    from information_schema.table_constraints tco\n    join information_schema.key_column_usage kcu\n              on tco.constraint_schema = kcu.constraint_schema\n              and tco.constraint_name = kcu.constraint_name\n    join information_schema.referential_constraints rco\n              on tco.constraint_schema = rco.constraint_schema\n              and tco.constraint_name = rco.constraint_name\n    join information_schema.table_constraints rel_tco\n              on rco.unique_constraint_schema = rel_tco.constraint_schema\n              and rco.unique_constraint_name = rel_tco.constraint_name\n    where tco.constraint_type = 'FOREIGN KEY'\n    group by kcu.table_schema,\n             kcu.table_name,\n             rel_tco.table_name,\n             rel_tco.table_schema,\n             kcu.constraint_name\n    order by kcu.table_schema,\n             kcu.table_name\n) main\nwhere primary_table = 'd_location'\n\n\n\ncreate database <new_db_name> owner <user_or_group> template <name_of_db_to_use_as_template>;\n-- show search_path;\nset search_path to <default_schema>,public;\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\n\n-- Database Creation\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncreate database <new_db_name> owner <user_or_group> template <name_of_db_to_use_as_template>;\n-- show search_path;\nset search_path to idb, public;\n\ngrant connect, temporary on database <new_db_name> to public;\ngrant all on database <new_db_name> to <user>;\ngrant all on database <new_db_name> to <group>;\n\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\ncreate schema staging;\n\n-- Add a unique constraint to e_analyte.full_name and e_analyte.cas_rn so that\n--  no full name or cas_rn can be used for more than one analyte\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nalter table e_analyte\n  add constraint uc_fullname unique(full_name),\n  add constraint uc_casrn unique(cas_rn);\n\n\n\nselect\n  a.*, b.*\nfrom\n  table1 as a\n  left join (\n    select * from dblink(\n      'dbname=<database>',\n      'select col1, col2, col3 from <table>'\n    ) as d (\n      col1 text, col2 text, col3 text\n    )\n  ) as b\n  on a.col1 = b.col2\n\n\n\n{sql, eval=F, inclute=T, class.source='fold-show'} select * from (     select *, row_number() over(         partition by             col1, col2, col3         order by col1 desc     ) rowid     from sometable ) someid where rowid > 1;\n\n\n\n{sql, eval=F, inclute=T, class.source='fold-show'} SELECT     n.nspname as SchemaName,     c.relname as RelationName,     CASE c.relkind         WHEN 'r' THEN 'table'         WHEN 'v' THEN 'view'         WHEN 'i' THEN 'index'         WHEN 'S' THEN 'sequence'         WHEN 's' THEN 'special'         END as RelationType,     pg_catalog.pg_get_userbyid(c.relowner) as RelationOwner,                  pg_size_pretty(pg_relation_size(n.nspname ||'.'|| c.relname)) as RelationSize FROM pg_catalog.pg_class c LEFT JOIN pg_catalog.pg_namespace AS n ON n.oid = c.relnamespace     WHERE  c.relkind IN ('r','s')      AND  (n.nspname !~ '^pg_toast' and nspname like 'pg_temp%') ORDER BY pg_relation_size(n.nspname ||'.'|| c.relname) DESC;\n\n\n\nhttps://dataedo.com/kb/query/postgresql/list-tables-with-most-relationships\nselect * from\n(select relations.table_name as table_name, -- schema name and table name\n       count(relations.table_name) as relationships, -- number of table relationships\n       count(relations.referenced_tables) as foreign_keys, -- number of foreign keys in a table\n       count(relations.referencing_tables) as references, -- number of foreign keys that are refering to this table\n       count(distinct related_table) as related_tables, -- number of related tables\n       count(distinct relations.referenced_tables) as referenced_tables, -- number of different tables referenced with FKs (multiple FKs can refer to one table, so number of FKs might be different than number of referenced tables)\n       count(distinct relations.referencing_tables) as referencing_tables -- number of different tables that are refering to this table (similar to referenced_tables)\nfrom(\n     select pk_tco.table_schema || '.' || pk_tco.table_name as table_name,\n            fk_tco.table_schema || '.' || fk_tco.table_name as related_table,\n            fk_tco.table_name as referencing_tables,\n            null::varchar(100) as referenced_tables\n     from information_schema.referential_constraints rco\n     join information_schema.table_constraints fk_tco\n          on rco.constraint_name = fk_tco.constraint_name\n          and rco.constraint_schema = fk_tco.table_schema\n     join information_schema.table_constraints pk_tco\n          on rco.unique_constraint_name = pk_tco.constraint_name\n          and rco.unique_constraint_schema = pk_tco.table_schema\n    union all\n    select fk_tco.table_schema || '.' || fk_tco.table_name as table_name,\n           pk_tco.table_schema || '.' || pk_tco.table_name as related_table,\n           null as referencing_tables,\n           pk_tco.table_name as referenced_tables\n    from information_schema.referential_constraints rco\n    join information_schema.table_constraints fk_tco \n         on rco.constraint_name = fk_tco.constraint_name\n         and rco.constraint_schema = fk_tco.table_schema\n    join information_schema.table_constraints pk_tco\n         on rco.unique_constraint_name = pk_tco.constraint_name\n         and rco.unique_constraint_schema = pk_tco.table_schema\n) relations\ngroup by table_name\norder by relationships asc) results\n\nwhere substring(table_name, 5, 2) = 'd_'; -- substring(string, start_position, length)\n\n\n\nselect * from pg_catalog.current_database()\n\n\n\nselect * from current_role\nselect * from current_user\n\n\n\nselect * from pg_catalog.pg_backend_pid()\n\n\n\nselect \n    pg_get_userbyid(p.proowner) as owner,\n    n.nspname as function_schema,\n    p.proname as function_name,\n    l.lanname as function_language,\n    case when l.lanname = 'internal' then p.prosrc\n        else pg_get_functiondef(p.oid)\n        end as definition,\n    pg_get_function_arguments(p.oid) as function_arguments,\n    t.typname as return_type\nfrom pg_proc p\n    left join pg_namespace n on p.pronamespace = n.oid\n    left join pg_language l on p.prolang = l.oid\n    left join pg_type t on t.oid = p.prorettype \nwhere n.nspname not in ('pg_catalog', 'information_schema')\nand n.nspname = 'idb'\norder by function_schema, function_name;\n\n\n\nselect * from pg_stat_activity\nwhere usename != '' and usename != 'postgres'\norder by usename, pid\n\n\n\nhttps://www.postgresql.org/docs/9.6/catalog-pg-aggregate.html\n-- pg_proc contains data for aggregate functions as well as plain functions\nselect * from pg_proc\n-- pg_aggregate is an extension of pg_proc.\nselect * from pg_aggregate\n\n\n\nSELECT rolname FROM pg_roles;\n\n\n\nSELECT table_schema,table_name FROM information_schema.tables ORDER BY table_schema,table_name;\n\n\n\nSELECT column_name\nFROM   information_schema.columns\nWHERE  table_schema = 'schema'\nAND    table_name = 'table';\n\n\n\nUPDATE tablename\nSET columnname = someothervalue\nFROM ...\nWHERE ...\n\n\n\nReference\nCREATE MATERIALIZED VIEW view_name\nAS\nquery\nWITH [NO] DATA;\nWhen you refresh data for a materialized view, PostgreSQL locks the entire table therefore you cannot query data against it. To avoid this, you can use the CONCURRENTLY option.\nWith CONCURRENTLY option, PostgreSQL creates a temporary updated version of the materialized view, compares two versions, and performs INSERT and UPDATE only the differences.\nREFRESH MATERIALIZED VIEW CONCURRENTLY view_name;\n\n\n\nWITH myconstants (analyte_search) as (\n   values ('%Hexachlorocyclopentadiene%')\n)\n\nSELECT *\nFROM e_analyte, myconstants\nWHERE analyte ilike analyte_search\n   OR full_name ilike analyte_search\n   OR aliases ilike analyte_search;\n\n\n\nseq_key bigint NOT NULL DEFAULT nextval('seq_key'::regclass)\n\nALTER SEQUENCE seq_key RESTART WITH 3;\n\n\n\nThis could be refined further by creating a function.\nwith\ndbrows as (\n    select * from dblink(\n        'dbname=chemcrit',\n        'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n        'dbname=ahtna',\n        'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=arkema_ph',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=bae_north',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=bayer_ldw',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=bcsa',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=c840_livent',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=cabotroad',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=centralia',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=centredale',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=eos',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=evraz_inwater',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=frenchtown_mill',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=gemt_columbus',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=gemt_meridian',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=gemt_springfield',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=reddog',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=shoreham',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=solvay',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n  union\n    select * from dblink(\n    'dbname=three_m_mb',\n    'select analyte, full_name, chem_class, aliases, cas_rn from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text)\n),\nconst (param) as (\n    values ('%solid%')\n)\nselect analyte, full_name, chem_class, cas_rn, aliases\nfrom dbrows, const\nwhere analyte ilike param\nor full_name ilike param\nor aliases ilike param\norder by chem_class, analyte;"
  },
  {
    "objectID": "dm.html",
    "href": "dm.html",
    "title": "Data Management",
    "section": "",
    "text": "EPA Guidance on Systematic Planning Using the Data Quality Objectives Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElement\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nOrganization\n\n\n\nIdentification and involvement of the project manager, sponsoring organization and responsible official, project personnel, stakeholders, scientific experts, etc. (e.g., all customers and suppliers).\n\n\n\n\n\n\n\nProject Goal\n\n\n\nDescription of the project goal, objectives, and study questions and issues.\n\n\n\n\n\n\n\nSchedule\n\n\n\nIdentification of project schedule, resources (including budget), milestones, and any applicable requirements (e.g., regulatory requirements, contractual requirements).\n\n\n\n\n\n\n\nData Needs\n\n\n\nIdentification of the type of data needed and how the data will be used to support the project’s objectives.\n\n\n\n\n\n\n\nCriteria\n\n\n\nDetermination of the quantity of data needed and specification of performance criteria for measuring quality.\n\n\n\n\n\n\n\nData Collection\n\n\n\nDescription of how and where the data will be obtained (including existing data) and identification of any constraints on data collection.\n\n\n\n\n\n\n\nQuality Assurance (QA)\n\n\n\nSpecification of needed QA and quality control (QC) activities to assess the quality performance criteria (e.g., QC samples for both field and laboratory, audits, technical assessments, performance evaluations, etc.).\n\n\n\n\n\n\n\nAnalysis\n\n\n\nDescription of how the acquired data will be analyzed (either in the field or the laboratory), evaluated (i.e., QA review/verification/validation), and assessed against its intended use and the quality performance criteria.\n\n\n\n\n\n\n\n\n\n\n\nWhen specifying the project goal (element #2 in Table 1), a key activity is to determine the key questions which the study will address once data and information are properly collected and analyzed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nSoundness\n\n\n\nThe extent to which the scientific and technical procedures, measures, methods or models employed to generate the information are reasonable for, and consistent with, the intended application.\n\n\n\n\n\n\n\nApplicability and Utility\n\n\n\nThe extent to which the information is relevant for the Agency’s intended use.\n\n\n\n\n\n\n\nClarity and Completeness\n\n\n\nThe degree of clarity and completeness with which the data, assumptions, methods, quality assurance, sponsoring organizations and analyses employed to generate the information are documented.\n\n\n\n\n\n\n\nUncertainty and Variability\n\n\n\nThe extent to which the variability and uncertainty (quantitative and qualitative) in the information or the procedures, measures, methods or models are evaluated and characterized.\n\n\n\n\n\n\n\nEvaluation and Review\n\n\n\nThe extent of independent verification, validation, and peer review of the information or of the procedures, measures, methods or models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchieving clarity in a project’s development becomes straightforward when using systematic planning, as almost every element of the planning process contributes to understanding how the project’s assumptions, methods, and proposed analyses will be conducted.\nPlanning for analyzing the data and information before collection clearly meets the intent of the GAFs.\nClear statements on the goals of the project developed through systematic planning leads to a better understanding of purpose and credibility of the results.\nSystematic planning leads to a clear statement of information needs and how the information will be collected, and leads to transparency in data quality.\n\n\n\n\n\n\n\n\nThe interaction amongst a multidisplinary team results in a clear understanding of the problem and the options available. Organizations that have used the DQO Process have found the structured format facilitated good communicaitons, documentation, and data collection design, all of which facilitated rapid peer review and approval.\n\n\nThe structure of the DQO Process provides a convenient way to document activities and decisions and to communicate the data collection design to others.\nThe DQO Process is an effective planning tool that can save resources by making data collection operations more resource-effective.\nThe DQO Process enables data users and technical experts to participate collectively in planning and to specify their needs prior to data collection. The DQO Process helps to focus studies by encouraging data users to clarify vague objectives and document clearly how scientific theory motivating this project is applicable to the intended use of the data.\nThe DQO Process provides a method for defining performance requirements appropriate for the intended use of the data by considering the consequences of drawing incorrect conclusions and then placing tolerable limits on them.\nThe DQO Process encourages good documentation for a model-based approach to investigate the objectives of a project, with discussion on how the key parameters were estimated or derived, and the robustness of the model to small perturbations. Upon implementing the DQO Process, your environmental programs can be strengthened"
  },
  {
    "objectID": "dm.html#resources",
    "href": "dm.html#resources",
    "title": "Data Management",
    "section": "Resources",
    "text": "Resources\n\nexecSQL\nPostgreSQL Tutorial\nPostgreSQL Tips\nDevDocs.io"
  },
  {
    "objectID": "dm.html#character-encoding",
    "href": "dm.html#character-encoding",
    "title": "Data Management",
    "section": "Character Encoding",
    "text": "Character Encoding\nWe deal with character encoding issues when importing data to databases all the time. All databases are encoded in UTF-8, and the most common non-ASCII, non-UTF-8 format is Microsoft’s custom encoding, which is CP-1252 (also known as win-1252 and a few other things). The character encoding of a file can’t necessarily be definitively determined by examination, but the Linux command-line tool file generally does a good job. If you want to change the encoding of a file rather than importing it in a known format, the Linux command-line tool iconv will do that for you. There’s a Python library on PyPI named chardet that will also diagnose file encodings.\nFor data managers, the workflow is to first guess that encoding errors on data import are due to the file being cp-1252. That covers about 90% of cases. Our import tool also automatically diagnoses instances where a file starts with a byte order mark (BOM), which covers most of the rest of the cases. For the remainders, Geany is usually the quickest way to check the file encoding.\nEverything that comes out of our databases is always in UTF-8, so I, at least, don’t ordinarily have encoding issues when importing data to R. For those who use data from other sources, it is a good idea to document a recommended workflow and set of tools."
  },
  {
    "objectID": "dm.html#summarization",
    "href": "dm.html#summarization",
    "title": "Data Management",
    "section": "Summarization",
    "text": "Summarization\nChemistry data frequently is summarized for use in analyses or for presentation using tables or maps. Summarization is ordinarily performed when there are multiple concentration values measured for a sample, or for a specific location, date, and depth. Multiple concentration values result from field or laboratory replications, from field splits created for quality control evaluations, and sometimes from sample reanalyses. Although field splits and laboratory replicates are created to support data quality assessments, all of the valid results that are produced are informative, are ordinarily stored in the project database, and are used to produce the most accurate possible estimate of the true concentration in a sample. When there are replicate results for a sample, the data will be averaged in a stepwise, or hierarchical, fashion. Because each level of the hierarchy represents a different source of variation, all the results at a single level are averaged together before results are averaged across levels. The different levels of replication, and the source of variation that each represents, are as follows:\n\nAverage across lab replicates\nAverage across multiple analyses (if they exist) for the same lab sample. This will average across multiple methods and reanalyses (different SDGs)\nAverage across multiple lab samples (if they exist) for the same sample number (split) and lab. Multiple lab samples may exist for the same sample number if the laboratory has created new laboratory sample IDs for reanalyses.\nAverage across multiple splits (if they exist) for the same lab and interpretive sample. Multiple sample numbers (splits) may exist for the same lab, or the same sample number may exist for different labs. Splits sent to different labs potentially have an additional source of variability in addition to variability between splits sent to the same lab.\nAverage across laboratories for the same main sample. This summarizes data by main sample (sample_id), which is typically the lowest level of data summarization to be used for data interpretation.\n\nBy default, data are summarized by successive averaging across these levels of replication, in the order given above. During the averaging process, data validation qualifiers and significant digits must be propagated. The rules for propagating the data validation qualifiers U (undetected), J (estimated), and R (rejected) are as follows:\n\nIf both detected and undetected data are to be averaged, then undetected data lower than the highest detected value will be taken at one-half the detection limit and averaged with the detected data, and the result will be identified as detected. Non-detects that are higher than the highest detected value will be omitted from the average.\nIf all data to be averaged are undetected, the result will be taken to be the lowest detection limit, and will be identified as undetected.\nIf J-qualified data are averaged with non-J-qualified data, the result will be J-qualified.\nIf R-qualified data are averaged with non-R-qualified data, the result will be R-qualified.\n\nSignificant digits are propagated so that the place (in the sense of one’s place, ten’s place, etc.) of the least significant digit of the average is equal to the highest place of the least significant digit of any of the values that are averaged.\nThese rules are built into custom aggregate functions in IDB that use the measurement_result data type.\nThese default data handling rules will be applied if no project-specific alternate rules are specified. The project manager, project technical staff, and data manager should evaluate, at the start of a project, whether an alternative approach is needed. Alternate data summarization rules should be summarized in the project plan or in the data management plan, if it exists. (Data managers: if not documented elsewhere, record this information in the Data Manager’s Manual.)\nNote that the handling of nondetects during hierarchical averaging and the presentation of nondetects in data summaries may be different. Regardless of whether nondetects are taken at half the detection limit or the full detection limit when averaging, the summarized result may be presented with either the full detection limit or half the detection limit. Reporting nondetects at the full detection limit should ordinarily be done when preparing data tables for reports or other deliverables. Data analyses to be conducted by Integral may be carried out using either half or full detection limits. The method of reporting nondetects should be specified when requesting data summaries."
  },
  {
    "objectID": "dm.html#chemistry",
    "href": "dm.html#chemistry",
    "title": "Data Management",
    "section": "Chemistry",
    "text": "Chemistry\n\nResources\n\nHazardous Waste Test Methods\nNational Environmental Methods Index\nSubstance Registry Service\nEIM Valid Values\nVerification and Validation\nQualifiers\nData Review\nChemical Lists\nPCBs\nWashington Water Resources Data Defs\nMeasurement Basis Conversions\nhttps://pubs.usgs.gov/wdr/WDR-WA-03-1/pdf/ADR_N.pdf\nhttp://www.eccsmobilelab.com/resources/literature/?Id=117\nConversions\n\n\n\nTEFs\n\nVan den Berg source document\nVan den Berg TEF table\n\n\n\nChemical Groups\n\nDioxin & Furans\nReference\n\n\nPCBs\nLearn about PCBs\n\nGeneral\nPCBs are a group of man-made organic chemicals consisting of carbon, hydrogen and chlorine atoms. The number of chlorine atoms and their location in a PCB molecule determine many of its physical and chemical properties. PCBs have no known taste or smell, and range in consistency from an oil to a waxy solid.\nPCBs belong to a broad family of man-made organic chemicals known as chlorinated hydrocarbons. PCBs were domestically manufactured from 1929 until manufacturing was banned in 1979. They have a range of toxicity and vary in consistency from thin, light-colored liquids to yellow or black waxy solids. Due to their non-flammability, chemical stability, high boiling point and electrical insulating properties, PCBs were used in hundreds of industrial and commercial applications including:\n\nElectrical, heat transfer and hydraulic equipment\nPlasticizers in paints, plastics and rubber products\nPigments, dyes and carbonless copy paper\nOther industrial applications\n\nCommercial Uses for PCBs\nAlthough no longer commercially produced in the United States, PCBs may be present in products and materials produced before the 1979 PCB ban. Products that may contain PCBs include:\n\nTransformers and capacitors\nElectrical equipment including voltage regulators, switches, re-closers, bushings, and electromagnets\nOil used in motors and hydraulic systems\nOld electrical devices or appliances containing PCB capacitors\nFluorescent light ballasts\nCable insulation\nThermal insulation material including fiberglass, felt, foam, and cork\nAdhesives and tapes\nOil-based paint\nCaulking\nPlastics\nCarbonless copy paper\nFloor finish\n\nThe PCBs used in these products were chemical mixtures made up of a variety of individual chlorinated biphenyl components known as congeners. Most commercial PCB mixtures are known in the United States by their industrial trade names, the most common being Arochlor.\nRelease and Exposure of PCBs\nToday, PCBs can still be released into the environment from:\n\nPoorly maintained hazardous waste sites that contain PCBs\nIllegal or improper dumping of PCB wastes\nLeaks or releases from electrical transformers containing PCBs\nDisposal of PCB-containing consumer products into municipal or other landfills not designed to handle hazardous waste\nBurning some wastes in municipal and industrial incinerators\n\nPCBs do not readily break down once in the environment. They can remain for long periods cycling between air, water and soil. PCBs can be carried long distances and have been found in snow and sea water in areas far from where they were released into the environment. As a consequence, they are found all over the world. In general, the lighter the form of PCB, the further it can be transported from the source of contamination.\nPCBs can accumulate in the leaves and above-ground parts of plants and food crops. They are also taken up into the bodies of small organisms and fish. As a result, people who ingest fish may be exposed to PCBs that have bioaccumulated in the fish they are ingesting.\nThe National Center for Health Statistics, a division of the Centers for Disease Control and Prevention, conducts the National Health and Nutrition Examination Surveys (NHANES). NHANES is a series of U.S. national surveys on the health and nutrition status of the noninstitutionalized civilian population, which includes data collection on selected chemicals. Interviews and physical examinations are conducted with approximately 10,000 people in each two-year survey cycle. PCBs are one of the chemicals where data are available from the NHANES surveys.\n\n\nPCB Congeners\nA PCB congener is any single, unique well-defined chemical compound in the PCB category. The name of a congener specifies the total number of chlorine substituents, and the position of each chlorine. For example: 4,4’-Dichlorobiphenyl is a congener comprising the biphenyl structure with two chlorine substituents - one on each of the #4 carbons of the two rings. In 1980, a numbering system was developed which assigned a sequential number to each of the 209 PCB congeners.\n\n\nPCB Homologs\nHomologs are subcategories of PCB congeners that have equal numbers of chlorine substituents. For example, the tetrachlorobiphenyls are all PCB congeners with exactly 4 chlorine substituents that can be in any arrangement.\n\n\nPCB Aroclor\nAroclor is a PCB mixture produced from approximately 1930 to 1979. It is one of the most commonly known trade names for PCB mixtures. There are many types of Aroclors and each has a distinguishing suffix number that indicates the degree of chlorination. The numbering standard for the different Aroclors is as follows:\n\nThe first two digits usually refer to the number of carbon atoms in the phenyl rings (for PCBs this is 12)\nThe second two numbers indicate the percentage of chlorine by mass in the mixture. For example, the name Aroclor 1254 means that the mixture contains approximately 54% chlorine by weight.\n\n\n\n\n\nQualifiers\n\nLabs may apply whatever flags they want to a result. Some data qualifiers are defined by EPA’s Functional Guidelines documents, which describe how data validation is to be conducted, and the use and interpretation of U, J, and R qualifiers is pretty universal (but older standards for Puget Sound data used E instead of J). Because the U, J, and R qualifiers are pretty universal and have implications for data usability, they are the only ones that are represented as Boolean fields in the meas_value column. All lab flags are put into the lab_flags column, and there is no lookup table for them, and there is no defined use for them. Similarly, the validator qualifiers (U, J, R, and possibly others) are put in the validator_flags column. If any of those three common qualifiers is in the validator_flags column, then the corresponding flags in meas_value should be set. The lab_conc_qual column is something of a relic, left over from the days when data were commonly provided in EPA’s Contract Laboratory Program (CLP) data format, which had a corresponding column. The lab_conc_qual column was meant to either contain “U” or be null. We don’t ordinarily use that column any more. Of the qualifiers you listed above, other than U, J, and R, N is commonly used to flag a tentatively identified compound, which means that the analyte code itself is uncertain. The d_labresult.tic column is meant to hold that information. The tic column is not part of the measurement_result data type because it is not used in any way during data aggregation (averaging or summing). I see that Jerry added other flags and qualifiers to the e_concqual table, but needn’t—really shouldn’t—be there. The e_concqual dictionary should have only “U” defined. It may seem odd to define a lookup table for only one value when a check constraint on the concentration qualifier columns could be used instead, but it’s easier to check relational constraints than to check check constraints programmatically.\n\n\n\nDuplicates\n\n“Duplicate” is a somewhat ambiguous term, but in practice it most commonly refers to field duplicates, which we ordinarily refer to as splits to avoid that ambiguity. Some QC data, particularly spikes, are frequently duplicated, so when we have lab QC data we may have values for spikes and spike duplicates. When we receive lab results in one big flat table that includes both analytical results for natural samples and results of lab QC samples, the word or code “DUP” in a column header or table cell could mean a couple of different things. Without seeing the original data source, I’m not sure where the “DUP” code in the “labqc_samp” column of your “d_labsample” table came from. I’m going to assume that it refers to a field duplicate, and not a spike duplicate.\n\n\nIdeally, samples are submitted to the laboratory “blind” so that the laboratory does not know which field samples are duplicates of one another. This is to prevent them from seeing that there’s a lot of variation between some pair of duplicates and deciding to re-run one or both of them. If the lab is producing highly variable data, we don’t want them to be able to hide it. Unfortunately, many field sampling programs use a suffix of “-D” or “-DUP” or something like that on the sample ID, so the lab knows which samples are field duplicates. If they know, they may pass that information back in their EDD.\n\n\nAlthough field duplicates are used as a QC check on laboratory performance, they are not lab QC samples themselves. They are just normal field samples (which have been split), and don’t need to have a laboratory QC sample ID assigned to them. Thus, field duplicates should not be listed in the “d_labqcsamp” table, so that table looks fine as it appears below. The same is true for the “d_labresult” table.\n\nThere are a couple of things to be changed about the “d_labsample” table as shown below:\n\nThe values in the “labqc_samp” column should be identifiers that appear in the “d_labqcsamp” table, not codes. The codes for the lab QC type should be in the d_labqcsamp.qc_type column, and neither “Natural” nor “DUP” should be used there.\nThe “d_labsample” table should have values in the “study_id” and “sample_no” columns, or a value in the “labqc_samp” column, but not both. There are other invalid combinations of columns also. The “d_labsample” table may have any one of the following tables as a parent: d_sampsplit, d_fldqcsplit, d_labqcsamp, d_bioaccum_samp, d_samptreatsplit, or d_bioasrepsamp. The “ck_one_sample” check constraint on the table enforces this rule. Check constraints like this are not run by the upsert scripts, so a set of staged data may pass all the checks performed by the upsert script and yet the INSERT into d_labsample will fail.\n\n\n\nMeasurement Basis\n\nR tool\nOrgMassSpecR\n\n\nGeneral\n\nData for soil and sediment are almost always reported on a dry-weight basis. If there’s anything to indicate a different basis, that deserves a closer look. Almost the only legitimate reason for a different basis for soil or sediment samples is when a leaching procedure has been applied (e.g., the Toxicity Characteristic Leaching Procedure, or TCLP); in those cases the data may be reported as the concentration in the leachate, so the basis may be “Wet” or “Whole” or “Unfiltered” – anything indicating an unfractionated liquid sample.\nData for tissues should ordinarily be reported in wet weight. Organisms’ homeostasis means that they maintain a nearly constant moisture content in their tissues, whereas the same is not true of materials like soil or sediment. If tissue data are reported in dry weight, check it carefully: labs can be sloppy about that.\nWater data are where things can be complex, because often water samples are filtered or centrifuged to remove particulates, which results in the water samples have a ‘dissolved’ basis. If the particulates are analyzed, and the results are then expressed in terms of the volume of the original sample, then the data will have a ‘particulate’ basis. Unfiltered, or whole, water, should have a basis of ‘Unfilt’, ‘Whole’, or sometimes ‘Wet’. Either of the first two of these are preferred, “Wet” is better used as a counterpart to “Dry” for soil, sediment, or tissue samples.\nThere are variations in the way things have been done in different databases. You may find that the measurement basis code for whole water samples differs from one to another, as in the third bullet above.\nIDB v.8 now has the “fraction” code, which is intended to be used to distinguish dissolved, particulate, and whole fractions of a water sample. In IDB v.8, the measurement basis for water samples will almost always be ‘Whole’. Sometimes sediment or soil samples are fractionated too, e.g., by sieving, and the fraction code should be used in those cases too, so the measurement basis will always be ‘dry’ in those cases.\nThere is an implicit association between measurement bases and units. For example, if the measurement basis is “Dry”, the units should not be “mg/L” because “…/L” implies a liquid, not a solid.\nThe measurement basis refers to the form of the sample material, which is represented in the denominator of concentration units. So codes of “Sediment” “Arsenic”, “mg/kg”, “Dry” should be read as “mg of arsenic per kg of dry sediment.”\n\n\n\nWet Weight\n\nWet weight (or as-is) basis means no calculation has been made to compensate for the moisture content of a sample. Wet weight refers to the weight of animal tissue or other substance including its contained water. (See also “Dry weight”)\n\n\n\nDry Weight\n\nDry weight basis means the lab has measured moisture content of a sample and calculated concentrations based on the percent solids present. Dry weight refers to the weight of animal tissue after it has been dried in an oven at 65°C until a constant weight is achieved. Dry weight represents total organic and inorganic matter in the tissue. (See also “Wet weight”).\n\n\n\nLipid\n\nLipid is any one of a family of compounds that are insoluble in water and that make up one of the principal components of living cells. Lipids include fats, oils, waxes, and steroids. Many environmental contaminants such as organochlorine pesticides are lipophilic.\n\n\n\n\n\nConversions\n\nWet to Dry\n\\[DryWt = \\frac{WetWt}{Percent Solids} * 100\\]\n\n\nDry to Wet\n\\[WetWt = DryWt * \\frac{PercentSolids}{100}\\]\n\n\nOrganic Carbon Normalization\n\\[OCnorm = \\frac{DryWt}{\\frac{PercentTOC}{100}}\\]\n\nDryWt & WetWt = concentration PercentSolids = percentage (no decimal)\n\n\n\nResource\n\n\n\n\nCalcs\n\n\n\n\n\n\n\nAnalytical Blanks\n\n\nTrip Blank\nThe trip blank is designed to identify levels of contamination from the exposure of the reagent or sorbent bed to the same atmospheres exposed to the analyte reagent or sorbent bed. The trip blank is prepared in the laboratory with the other reagents or adsorbents prior to shipping to the field. However, the trip blank is never exposed to the field atmospheres. It is simply sent along with the field samples to and from the site. The trip blank identified areas of exposure such as shipping temperatures and pressures, laboratory preparation of field samples and laboratory preparation of field samples for analysis.\n\nField Blank\nThe field blank is similar to the trip blank in that it is also prepared during the preparation of the field reagents or adsorbents. However, the field blank is exposed to the same atmospheres in the field as the field samples. This means that the field blank is opened during the charging of impingers or sorbents in the sample train. The field blank is also exposed during the exchanging of cartridges in SW-846, Method 0030 or when field reagents are being exchanged during a test run. In summary, field blanks consist of additional sample collection media (e.g., sorbent tubes, reagents, filters) which are transported to the monitoring site, exposed briefly at the site when the samples are exposed (but no stack gas is actually pulled through these blanks), and transported back to the laboratory for analysis, similar to a field sample. At least one field blank should be collected and analyzed for each test series.\n\n\nLaboratory Blank\nThe laboratory blank is a sample of the reagents or sorbents used during the sample train reagent preparation or recovery. The laboratory blank is a sample of the extraction solvent, the rinses used during sample recovery, or a sample from the batch of sorbent used to preparing sampling cartridges. Laboratory blanks include both method blanks and instrument blanks. Method blanks are carried through all steps of the measurement process (from extraction through analysis). A method blank is typically analyzed with each sample batch. Instrument blanks are used to demonstrate that an instrument system is free of contamination. Instrument blanks are typically analyzed prior to sample analysis and following the analysis of highly contaminated samples.\n\n\nReagent Blank\nThe reagent blank is a sample of the solvents used during recovery of the sample train after the test is completed. You recall, reagent blanks for both multi-metal and chromium +6 require that the reagent blank be the same volume as the renses used to recover the samples, from probe to impinger. This is because the blank value is substracted from the sample to obtain a final concentration.\n\n\nDiagram\n\n\n\n\n\nDetection Limits\nPresentation\n\nWhat affects detection limits?\n\nSample size\nConcentration of other constituents\nSample clean-up\nMethodology\nLab Performance\n\nExperience\nExtraction technique\nInstrument type and maintenance\n\n\ndetection_limit - the lowest possible value an instrument/method can sense a compound is present (think of it like a whisper - you can barely hear it, but know its there). This is better known as the “method detection limit”\nquantification_limit - the limit in which an instrument/method can actually start to quantify the amount of something which is present. If the result is between the detection_limit and the quantification_limit, the result is estimated, because the instrument/method cant confidently identify the amount of something until it reaches the quantification_limit.\nreporting_limit - usually project or dataset specific. this limit is used for data analysis/statistics. the reporting_limit is equal to either the detection_limit or quantification_limit. This is better known as the “reporting detection limit”.\n\n\nMethod Detection Limit (MDL)\n\nStatistically determined\nThe minimum concentration that can be measured with 99% confidence that the concentration is greater than zero\nConcentrations near MDL are estimates\nLaboratory, instrument, matrix, method, and analyte specific\nConcentrations at MDL expected to be a false positive 1% of the time, but false negatives 50% of the time\n\n\n\nMethod Reporting Limit (MRL)\n\nMay also be referred to as QL (quantitation limit), sample quantitation limit, or just RL (reporting limit)​\nDetermined by the lowest point of the calibration​\nNot as specific as MDL, labs can adust​\nConcentrations at MRL can be reliably quantified​\nMRL > MDL​\nAlso laboratory, instrument, matrix, method, & analyte specific\n\n\n\nMDL & MRL Relationship\n\n\n\nOther Detection Limits\n\nPQL\n\nConsidered to be lowest concentration that can be reliably quantified by a method\nLimit of Detection (LOD); Lowest concentration that can be detected with a 1% false negative rate.\n\nGenerally 2x to 3X MDL\n\nLimit of Quantitation (LOQ); similar to MRL\n\n\n\nPCDD/F & PCB specific\n2.5 times signal to noise\n\nEQL: Estimated Quantitation Limit\nEDL: Estimated Detection Limit\nSDL: Sample Detection Limit\nEMPC\n\nEstimated Maximum Possible Concentration (EMPC)\nPeak present but not all of the identification criteria is met\nAlways greater than MDL, may be greater than MRL\nGenerally treated a non-detect in TEQ calculations\nEMPCs can present data management difficulties and need to be reviewed in QC checks"
  },
  {
    "objectID": "food.html",
    "href": "food.html",
    "title": "Food",
    "section": "",
    "text": "Typical schedule for baking sourdough bread. Timing could change based on room temperature.\n\n\n\n\n\n\n\n\nTime\nStep\nNotes\n\n\n\n\n8:00am\nFeed starter\nDiscard all but at least 2 tbsp\n\n\n12:00pm\nAutolyse\n\n\n\n1:00pm\nAdd starter & salt\n\n\n\n1:30pm\nStretch & fold\nEvery 30 min for 2-2.5 hours\n\n\n3:30pm\nBulk rise\n1-3 hours, should rise 30-50%\n\n\n8:30pm\nPre shape & bench rest\n\n\n\n9:00pm\nFinal shaping & proof\nPlace in proofing baskets, cover, place in fridge overnight\n\n\n9:00am\nBake\n500 F in dutch oven, 20 min lid on, lower to 450, 15-30 min lid off"
  },
  {
    "objectID": "food.html#greek-yogurt-smoothie",
    "href": "food.html#greek-yogurt-smoothie",
    "title": "Food",
    "section": "Greek Yogurt Smoothie",
    "text": "Greek Yogurt Smoothie\n\nGreek yogurt\nBanana\nBerries\nOats\nHoney\nIce"
  },
  {
    "objectID": "food.html#overnight-oats",
    "href": "food.html#overnight-oats",
    "title": "Food",
    "section": "Overnight Oats",
    "text": "Overnight Oats\n\n90 grams oats\n90 grams greek yogurt\n140 grams milk\n5 grams chia and/or flaxseed\n15 grams honey\n\nStir or shake to combine. Refrigerate overnight."
  },
  {
    "objectID": "food.html#buffalo-cauliflower",
    "href": "food.html#buffalo-cauliflower",
    "title": "Food",
    "section": "Buffalo Cauliflower",
    "text": "Buffalo Cauliflower\n\nPreheat oven to 450 F\nBatter:\n\n3/4 cup flour\n1 tsp paprika\n2 tsp garlic powder\n1 tsp salt\n1/2 tsp black pepper\n3/4 cup milk\n\nSauce:\n\n1/4 cup buffalo sauce\n1 tbsp Honey\n1/2 tsp black pepper\n\nSplit cauliflower into florets and mix in batter\nBake for 20 minutes, flip after 10.\nBrush sauce onto cauliflower, bake 10 minutes.\nFlip cauliflower and brush again with sauce, bake 10 minutes."
  },
  {
    "objectID": "food.html#quinoa-cucumber-salad",
    "href": "food.html#quinoa-cucumber-salad",
    "title": "Food",
    "section": "Quinoa & Cucumber Salad",
    "text": "Quinoa & Cucumber Salad\n\nSalad:\n\n2 cups cucumber, spiralized or julienned\n2 cups chopped tomatoes\n2 large avocados, diced\n1 red onion, sliced\n2 cups cooked quinoa\n1 handful chopped parsley (or cilantro)\n\nDressing - blend the following:\n\n1 ripe avocado\n1/4 cup white wine vinegar\nJuice of one lime\nSalt and fresh cracked pepper, to taste\n3/4 cup olive oil"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to geocoug’s resource toolkit. This site was built using quarto.\n\n\n\n\n\n\n\n\n\n\nCLI\n\n\nCommand Line Interface Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDBMS\n\n\nDatabase tidbits & SQL snippets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Management\n\n\nData management guidelines and resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFood\n\n\nRecipies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPython snippets and resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nR and RStudio Reference Mateiral\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources\n\n\nResource references and links\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkouts\n\n\nWorkouts and routines\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python",
    "section": "",
    "text": "Credit - Many of the Python methods below are taken directly from an RDN blog"
  },
  {
    "objectID": "python.html#cheat-sheet",
    "href": "python.html#cheat-sheet",
    "title": "Python",
    "section": "Cheat Sheet",
    "text": "Cheat Sheet\n\nMetaCharacters (Need to be escaped)\n. ^ $ * + ? { } [ ] \\ | ( )\n\n\nCharacters\n. - Any Character Except New Line \\d - Digit (0-9) \\D - Not a Digit (0-9) \\w - Word Character (a-z, A-Z, 0-9, _) \\W - Not a Word Character \\s - Whitespace (space, tab, newline) \\S - Not Whitespace (space, tab, newline)\n\n\nCharacter Classes\n[] - Matches Characters in brackets [^ ] - Matches Characters NOT in brackets [a-z] - Any lowercase character between a and z [A-Z] - Any UPPERCASE character between A and Z\n\n\nQuantifiers\n* - 0 or More + - 1 or More ? - 0 or One {3} - Exact Number {3,4} - Range of Numbers (Minimum, Maximum) {3,} - At least 3\n\n\nAnchors & Boundaries\n\\b - Word Boundary \\B - Not a Word Boundary ^ - Beginning of a String $ - End of a String\n\n\nLogic\n| - Either Or ( ) - Group \\1 - Contents of group 1\n\n\nWhite-space\n\\t - Tab \\r - Carriage return \\n - New line\n\n\n\nCode\nimport re\n\ntext_string = '''\nHello world\n\n8001234567\n800-321-7654\n900.987.6543\n\nsome.email@email.com\nmycompany@company.net\nwierd-12-address-4@somedomain.blah\n'''\n\npattern = re.compile(r'[0-9]{3}[.-]?[0-9]{3}[.-]?[0-9]{4}')\nmatches = re.finditer(pattern, text_string)\n\nfor match in matches:\n  print(match)\n  print(match.span())\n  print(text_string[match.start():match.end()])\n\n\n<re.Match object; span=(14, 24), match='8001234567'>\n(14, 24)\n8001234567\n<re.Match object; span=(25, 37), match='800-321-7654'>\n(25, 37)\n800-321-7654\n<re.Match object; span=(38, 50), match='900.987.6543'>\n(38, 50)\n900.987.6543"
  },
  {
    "objectID": "python.html#db-and-file-objects",
    "href": "python.html#db-and-file-objects",
    "title": "Python",
    "section": "DB and File Objects",
    "text": "DB and File Objects\n\n\nCode\nimport csv\n\n\nclass CsvFile(object):\n    \"\"\"CsvFile class automatically opens a file and creates a CSV reader, reads the first row containing column headers, and stores those headers so that they can be used to construct the INSERT statement.\"\"\"\n\n    def __init__(self, filename):\n        self.fn = filename\n        self.f = None\n        self.open()\n        self.rdr = csv.reader(self.f)\n        self.headers = next(self.rdr)\n\n    def open(self):\n        if self.f is None:\n            mode = \"rb\" if sys.version_info < (3,) else \"r\"\n            self.f = open(self.fn, mode)\n\n    def reader(self):\n        return self.rdr\n\n    def close(self):\n        self.rdr = None\n        self.f.close()\n        self.f = None\n\n\nclass Database(object):\n    \"\"\"The Database class and subclasses provide a database connection for each type of DBMS, and a method to construct an INSERT statement for a given CsvFile object, using that DBMS's parameter substitution string.  The conn_info argument is a dictionary containing the host name, user name, and password.\"\"\"\n\n    def __init__(self, conn_info):\n        self.paramstr = \"%s\"\n        self.conn = None\n\n    def insert_sql(self, tablename, csvfile):\n        return \"insert into %s (%s) values (%s);\" % (\n            tablename,\n            \",\".join(csvfile.headers),\n            \",\".join([self.paramstr] * len(csvfile.headers)),\n        )\n\n\nclass PgDb(Database):\n    \"\"\"Connection object for PostgreSQL\"\"\"\n\n    def __init__(self, conn_info):\n        self.db_type = \"p\"\n        import psycopg2\n\n        self.paramstr = \"%s\"\n        connstr = (\n            \"host=%(server)s dbname=%(db)s user=%(user)s password=%(pw)s\" % conn_info\n        )\n        self.conn = psycopg2.connect(connstr)\n\n\nclass MariaDb(Database):\n    \"\"\"Connection object for Maria DB\"\"\"\n\n    def __init__(self, conn_info):\n        self.db_type = \"m\"\n        import pymysql\n\n        self.paramstr = \"%s\"\n        self.conn = pymysql.connect(\n            host=conn_info[\"server\"],\n            database=conn_info[\"db\"],\n            port=3306,\n            user=conn_info[\"user\"],\n            password=conn_info[\"pw\"],\n        )"
  },
  {
    "objectID": "python.html#import-functions",
    "href": "python.html#import-functions",
    "title": "Python",
    "section": "Import Functions",
    "text": "Import Functions\n\nPostgres COPY command\n\n\nCode\ndef postgres_copy(csvfile, db):\n    curs = db.conn.cursor()\n    rf = open(csvfile.fn, \"rt\")\n    # Read and discard headers\n    hdrs = rf.readline()\n    copy_cmd = \"copy copy_test from stdin with (format csv)\"\n    curs.copy_expert(copy_cmd, rf)\n\n\n\n\nRow-by-row reading and writing\n\n\nCode\ndef simple_copy(csvfile, db):\n    ins_sql = db.insert_sql('copy_test', csvfile)\n    curs = db.conn.cursor()\n    rdr = csvfile.reader()\n    for line in rdr:\n        curs.execute(ins_sql, clean_line(line))\n        db.conn.commit()\n\n\n\n\nBuffered reading and writing\n\n\nCode\ndef buffer1_copy(csvfile, db, buflines):\n    ins_sql = db.insert_sql(\"copy_test\", csvfile)\n    curs = db.conn.cursor()\n    rdr = csvfile.reader()\n    eof = False\n    while True:\n        b = []\n        for j in range(buflines):\n            try:\n                line = next(rdr)\n            except StopIteration:\n                eof = True\n            else:\n                b.append(clean_line(line))\n        if len(b) > 0:\n            curs.executemany(ins_sql, b)\n            if eof:\n                break\n    db.conn.commit()"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Developer\n\ndevdocs.io - Searchable documentations\nss64 - CLI reference guide\nReadTheDocs - Create, host, and browse documentation\nexecsql - Run SQL with metacommands\nBootstrap - Web framework\npgAdmin - PostgreSQL sandbox\nRegex - Regular expressions\nGoogle Colab - Collaborative Python notebooks\ngeojson.io - Create, view, and share maps\nrepl.it - Collaborative in-browser IDE. 50+ languages\nIntegromat - Online scenario automation\nPostgreSQL cheatsheet - PostgreSQL cheatsheet\npython-utils - Playground for Python utilities\nSpektran - Collection of useful color tools\nObservable - JavaScript notebooks\nHTML Dog - HTML tutorials\nCrontab - Crontab scheduler\nMedia Library\nUTF-8 Character Debug - UTF-8 character debugging chart\nLaTeX Basics\nQuarto - Scientific and technical publishing system built on Pandoc\n\n\n\n\nGIS\n\nepsg.io - Spatial reference systems\nArcGIS - ArcGIS Python\n\n\n\n\nMusic\n\nSongsterr - Guitar tabs\nUltimate Guitar - Guitar tabs\nFlagrantior - Music theory\nmusictheory.net - Music theory\nTeoria - Music theory\nJustinGuitar - Guitar lessons\nHowToPlayPiano - Piano lessons\nSoundation - Broswer based music maker\n\n\n\n\ne-Books\n\nProject Gutenberg\nLibriVox\nStandard eBooks\n\n\n\n\nMiscellaneous\n\nFree Learning - List of free educational resources"
  },
  {
    "objectID": "rstudio.html#r",
    "href": "rstudio.html#r",
    "title": "R",
    "section": "R",
    "text": "R\n\n\nCode\nx <- 5 * 5\nx\n\n\n[1] 25"
  },
  {
    "objectID": "rstudio.html#sql",
    "href": "rstudio.html#sql",
    "title": "R",
    "section": "SQL",
    "text": "SQL\nThere are a couple ways to connect to a database instance. These examples show how to connect to a local PostgreSQL instance, and how to use the connection information to query a database.\n\nOption 1\nCreate connection chunk, then call connection in each subsequent chunk.\n\n\nCode\nlibrary(DBI)\n\npass <- readLines(\"../../pwd.txt\")\ndb = dbConnect(\n  RPostgres::Postgres(),\n  dbname   = \"personal\",\n  host     = \"localhost\",\n  port     = 5432,\n  user     = \"cgrant\",\n  password = pass\n)\n\n\n\n\nCode\nselect * from workouts limit 10;\n\n\n\n\nOption 2\nSet default connection in the setup chunk (pretend its the setup chunk).\n\n\nCode\nlibrary(DBI)\n\ndb = dbConnect(\n  RPostgres::Postgres(),\n  dbname   = \"personal\",\n  host     = \"localhost\",\n  port     = 5432,\n  user     = \"cgrant\",\n  password = pass\n)\nknitr::opts_chunk$set(connection = \"db\")\n\n\nNow you dont have to specify the connection for each chunk.\n\n\nCode\nselect * from workouts limit 10;"
  },
  {
    "objectID": "rstudio.html#python",
    "href": "rstudio.html#python",
    "title": "R",
    "section": "Python",
    "text": "Python\nTo use a Python engine, you need to call library(reticulate) link\n\n\nCode\n# install.packages(\"reticulate\")\n\nSys.setenv(RETICULATE_PYTHON = \"/Users/cgrant/venvs/dev/bin/python\")\nlibrary(reticulate)\npy_config()\n\n\npython:         /Users/cgrant/venvs/dev/bin/python\nlibpython:      /opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/python3.10/config-3.10-darwin/libpython3.10.dylib\npythonhome:     /Users/cgrant/venvs/dev:/Users/cgrant/venvs/dev\nversion:        3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]\nnumpy:          /Users/cgrant/venvs/dev/lib/python3.10/site-packages/numpy\nnumpy_version:  1.23.4\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\nNow you can run Python code\n\n\nCode\nx = 4 * 4\nprint(x)\n\n\n16\n\n\nYou can import libraries as normal\n\n\nCode\nimport pandas as pd\n\nvals = {\"col1\": ['a', 'b', 'c', 'd'], \"col2\": [1, 10, 100, 1000]}\ndf = pd.DataFrame(vals)\nprint(df)\n\n\n  col1  col2\n0    a     1\n1    b    10\n2    c   100\n3    d  1000\n\n\nAccess objects created within Python chunks from R using py$<var>\n\n\nCode\nlibrary(DT)\ndatatable(py$df)"
  },
  {
    "objectID": "rstudio.html#leaflet",
    "href": "rstudio.html#leaflet",
    "title": "R",
    "section": "Leaflet",
    "text": "Leaflet\nUse the leaflet map below to explore.\n\n\nCode\nlibrary(leaflet)\nlibrary(dplyr)\n\nleaflet() %>% \n  setView(lng=-122.90486, lat=47.03576, zoom=16) %>% \n  addTiles() %>% \n  addMarkers(lng=-122.90486, lat=47.03576, popup=\"WA State Capitol\")"
  },
  {
    "objectID": "rstudio.html#packages",
    "href": "rstudio.html#packages",
    "title": "R",
    "section": "Packages",
    "text": "Packages\n\nDT::datatables - options"
  },
  {
    "objectID": "rstudio.html#connection",
    "href": "rstudio.html#connection",
    "title": "R",
    "section": "Connection",
    "text": "Connection\n\n\nCode\npass <- readLines(\"../../pwd.txt\")\n\nconn <- DBI::dbConnect(\n  RPostgres::Postgres(),\n  dbname   = \"personal\",\n  host     = \"localhost\",\n  port     = 5432,\n  user     = \"cgrant\",\n  password = pass\n)"
  },
  {
    "objectID": "rstudio.html#query",
    "href": "rstudio.html#query",
    "title": "R",
    "section": "Query",
    "text": "Query\n\n\nCode\nlibrary(glue)\n\ntbls <- dbListTables(conn)\nsql <- glue(\"SELECT * FROM workouts limit 1000;\")\nres <- dbGetQuery(conn, sql)\ncols <- names(res)\ndatatable(res, options=list(scrollX=T))"
  },
  {
    "objectID": "rstudio.html#temperature",
    "href": "rstudio.html#temperature",
    "title": "R",
    "section": "Temperature",
    "text": "Temperature\nReference Source\n\n\nCode\nlibrary(leaflet)\nlibrary(dplyr)\n\nf <- \"./static/NOAA-SeaTac_cleaned.csv\"\ndt <- read.csv(f)\n\ndf <- as.data.frame(dt)\n\nupdatemenus <- list(\n  list(\n    active = 0,\n    x = -.125,\n    type= 'buttons',\n    buttons = list(\n      list(\n        label = \"Wet Bulb\",\n        method = \"update\",\n        args = list(list(visible = c(TRUE, \"legendonly\")))),\n      list(\n        label = \"Dry Bulb\",\n        method = \"update\",\n        args = list(list(visible = c(\"legendonly\", TRUE))))\n    )\n  )\n)\n\nplt <- plot_ly(data = df) %>%\n  add_markers(x=as.Date(df$Timestamp), y=df$HourlyWetBulbTemperature, name=\"Wet Bulb\") %>%\n  add_markers(x=as.Date(df$Timestamp), y=df$HourlyDryBulbTemperature, name=\"Dry Bulb\", visible=\"legendonly\") %>%\n  layout(title = \"SeaTac 2020 Temperature Data\", \n         showlegend=FALSE,\n         xaxis=list(zeroline = FALSE,title=\"Date\"),\n         yaxis=list(zeroline = FALSE,title=\"Temperature (F)\"),\n         updatemenus=updatemenus)\n\n\nError in layout(., title = \"SeaTac 2020 Temperature Data\", showlegend = FALSE, : unused arguments (title = \"SeaTac 2020 Temperature Data\", showlegend = FALSE, xaxis = list(zeroline = FALSE, title = \"Date\"), yaxis = list(zeroline = FALSE, title = \"Temperature (F)\"), updatemenus = updatemenus)\n\n\nCode\nplt\n\n\nError in eval(expr, envir, enclos): object 'plt' not found"
  },
  {
    "objectID": "workouts.html",
    "href": "workouts.html",
    "title": "Workouts",
    "section": "",
    "text": "The following regimen outlines a 4 week lift cycle. The routine is a slightly modified version of this. Non-lift days should be supplemented with active recovery workouts.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\n\n\n\nSunday\n\n\n\nMonday\n\n\n\nTuesday\n\n\n\nWednesday\n\n\n\nThursday\n\n\n\nFriday\n\n\n\nSaturday\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nBack & Biceps\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nChest & Triceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n2\n\n\n\nCardio\n\n\n\nShoulders & Traps, Run\n\n\n\nChest & Triceps\n\n\n\nBack & Biceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n3\n\n\n\nCardio\n\n\n\nBack & Biceps, Run\n\n\n\nShoulders & Traps\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n4\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nDeadlift\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nPull-Ups\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nLat Pull Downs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nRows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFace Pulls\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBarbell Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\n60 seconds\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n15\n\n\n\n\n\n\n\nSit Ups\n\n\n\n3\n\n\n\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBench Press\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nIncline Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFlys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDips\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nTricep Pushdowns\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nAb Roller\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nSitting Twists\n\n\n\n3\n\n\n\n30\n\n\n\n\n\n\n\nWipers\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBack Squat\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nFront Squat\n\n\n\n4\n\n\n\n6\n\n\n\n\n\n\n\nLunges\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nKettelbell Deadlift\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nLeg Kickbacks\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nGlute Bridges\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nMilitary Press\n\n\n\n4\n\n\n\n12, 8, 5, 3\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nRear Delt Flys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nBarbell Shrugs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nOne Arm Dumbbell Snatch\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nKettlebell Swing\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nFarmers Carry\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nScissor Kicks\n\n\n\n3\n\n\n\n30 seconds\n\n\n\n\n\n\n\nL-sit\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nToe Taps\n\n\n\n3\n\n\n\n20"
  }
]