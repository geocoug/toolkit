[
  {
    "objectID": "dbms.html",
    "href": "dbms.html",
    "title": "DBMS",
    "section": "",
    "text": "Reference\n\n\npg_dump -h [host] -d [database] -U [user] -s [schema only] -W [force password] > <file>.sql\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph inner[Inner Join]\n    direction LR\n        a1(A1)\n        a2(A2)\n        b1(B1)\n        b2(B2)\n        a1 -.- b1\n        a2 -.- b2\n    end\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    subgraph cross[Cross Join]\n    direction LR\n        a1(A1)\n        a2(A2)\n        b1(B1)\n        b2(B2)\n        a1 -.- b1\n        a1 -.- b2\n        a2 -.- b1\n        a2 -.- b2\n    end\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph left[Left Join]\n    direction RL\n        a1(A1)\n        a2(A2)\n        a3(A3)\n        b1(B1)\n        b2(B2)\n        b1 -.- a1\n        b2 -.- a2\n    end\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart TD\n    subgraph right[Right Join]\n    direction LR\n        a1(A1)\n        a2(A2)\n        b1(B1)\n        b2(B2)\n        b3(B3)\n        a1 -.- b1\n        a2 -.- b2\n    end\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nflowchart LR\n    subgraph outer[Outer<br>Join]\n    direction LR\n        subgraph a[A]\n        direction TB\n            a1(A1)\n            a2(A2)\n            a3( )\n        end\n        subgraph b[B]\n        direction TB\n            b1(B1)\n            b2( )\n            b3(B3)\n        end\n        a1 -.- b1\n    end\n    style a3 fill-opacity:0, stroke-opacity:0;\n    style b2 fill-opacity:0, stroke-opacity:0;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLENGTH(string): Returns the length of the provided string\nPOSITION(string IN substring): Returns the position of the substring within the specified string.\nCAST(expression AS datatype): Converts an expression into the specified data type.\n`NOW: Returns the current date, including time.\nCEIL(input_val): Returns the smallest integer greater than the provided number.\nFLOOR(input_val): Returns the largest integer less than the provided number.\nROUND(input_val, [round_to]): Rounds a number to a specified number of decimal places.\nTRUNC(input_value, num_decimals): Truncates a number to a number of decimals.\nREPLACE(whole_string, string_to_replace, replacement_string): Replaces one string inside the whole string with another string.\nSUBSTRING(string, [start_pos], [length]): Returns part of a value, based on a position and length.\n\n\n\n\nfunction_name ( arguments ) OVER (\n    [query_partition_clause]\n    [ORDER BY order_by_clause\n    [windowing_clause] ] \n)\n\n\n\nColumns\n\ntable_schema: PK schema name\ntable_name: PK table name\nconstraint_name: PK constraint name\nposition: index of column in table (1, 2, …). 2 or higher means key is composite (contains more than one column)\nkey_column: PK column name\n\nRows\n\nOne row represents one primary key column\nScope of rows: columns of all PK constraints in a database\nOrdered by table schema, table name, column position\n\nselect * from (\n    -- Main query. Returns all tables\n    select kcu.table_schema,\n           kcu.table_name,\n           tco.constraint_name,\n           kcu.ordinal_position as position,\n           kcu.column_name as key_column\n    from information_schema.table_constraints tco\n    join information_schema.key_column_usage kcu \n         on kcu.constraint_name = tco.constraint_name\n         and kcu.constraint_schema = tco.constraint_schema\n         and kcu.constraint_name = tco.constraint_name\n    where tco.constraint_type = 'PRIMARY KEY'\n    order by kcu.table_schema,\n             kcu.table_name,\n             position\n) main\nwhere table_name = 'd_location'\n\n\n\nColumns\n\nforeign_table: foreign table schema and name\nrel: relationship symbol implicating direction\nprimary_table: primary (rerefenced) table schema and name\nfk_columns: list of FK colum names, separated with “,”\nconstraint_name: foreign key constraint name\n\nRows\n\nOne row represents one foreign key.\nIf foreign key consists of multiple columns (composite key) it is still represented as one row.\nScope of rows: all foregin keys in a database.\nOrdered by foreign table schema name and table name.\n\n\n\n\ndrop table if exists dependencies cascade;\ncreate temporary table dependencies as\nselect \n        tc.table_name as child,\n        tu.table_name as parent\nfrom \n        information_schema.table_constraints as tc\n        inner join information_schema.constraint_table_usage as tu\n             on tu.constraint_name = tc.constraint_name\nwhere \n        tc.constraint_type = 'FOREIGN KEY'\n        and tc.table_name <> tu.table_name;\n\nwith recursive dep_depth as (\n select\n  dep.child,\n  dep.parent,\n  1 as lvl\n from\n  dependencies as dep\n union all\n select\n  dep.child,\n  dep.parent,\n  dd.lvl + 1 as lvl\n from\n  dep_depth as dd\n  inner join dependencies as dep on dep.parent = dd.child\n )\nselect\n table_name,\n table_order\nfrom (\n select\n  dd.parent as table_name,\n  max(lvl) as table_order\n from\n  dep_depth as dd\n group by\n  table_name\n union\n select\n  dd.child as table_name,\n  max(lvl) + 1 as level\n from\n  dep_depth as dd\n  left join dependencies as dp on dp.parent = dd.child\n where\n  dp.parent is null\n group by\n  dd.child\n ) as all_levels\n order by table_order;\nselect * from (\n    -- Main query. Returns FK relationships for all tables\n    select \n            kcu.table_schema as table_schema,\n            kcu.table_name as foreign_table,\n           '>-' as relationship,\n           rel_tco.table_name as primary_table,\n           string_agg(kcu.column_name, ', ') as fk_columns,\n           kcu.constraint_name\n    from information_schema.table_constraints tco\n    join information_schema.key_column_usage kcu\n              on tco.constraint_schema = kcu.constraint_schema\n              and tco.constraint_name = kcu.constraint_name\n    join information_schema.referential_constraints rco\n              on tco.constraint_schema = rco.constraint_schema\n              and tco.constraint_name = rco.constraint_name\n    join information_schema.table_constraints rel_tco\n              on rco.unique_constraint_schema = rel_tco.constraint_schema\n              and rco.unique_constraint_name = rel_tco.constraint_name\n    where tco.constraint_type = 'FOREIGN KEY'\n    group by kcu.table_schema,\n             kcu.table_name,\n             rel_tco.table_name,\n             rel_tco.table_schema,\n             kcu.constraint_name\n    order by kcu.table_schema,\n             kcu.table_name\n) main\nwhere primary_table = 'd_location'\n\n\n\ncreate database <new_db_name> owner <user_or_group> template <name_of_db_to_use_as_template>;\n-- show search_path;\nset search_path to <default_schema>,public;\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\n-- Database Creation\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncreate database <new_db_name> owner <user_or_group> template <name_of_db_to_use_as_template>;\n-- show search_path;\nset search_path to idb, public;\n\ngrant connect, temporary on database <new_db_name> to public;\ngrant all on database <new_db_name> to <user>;\ngrant all on database <new_db_name> to <group>;\n\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\ncreate schema staging;\n\n-- Add a unique constraint to e_analyte.full_name and e_analyte.cas_rn so that\n--  no full name or cas_rn can be used for more than one analyte\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nalter table e_analyte\n  add constraint uc_fullname unique(full_name),\n  add constraint uc_casrn unique(cas_rn);\n\n\n\nselect\n  a.*, b.*\nfrom\n  table1 as a\n  left join (\n    select * from dblink(\n      'dbname=<database>',\n      'select col1, col2, col3 from <table>'\n    ) as d (\n      col1 text, col2 text, col3 text\n    )\n  ) as b\n  on a.col1 = b.col2\n\n\n\nselect * from (\n    select *, row_number() over(\n        partition by\n            col1, col2, col3\n        order by col1 desc\n    ) rowid\n    from sometable\n) someid\nwhere rowid > 1;\n\n\n\nSELECT\n    n.nspname as SchemaName,\n    c.relname as RelationName,\n    CASE c.relkind\n        WHEN 'r' THEN 'table'\n        WHEN 'v' THEN 'view'\n        WHEN 'i' THEN 'index'\n        WHEN 'S' THEN 'sequence'\n        WHEN 's' THEN 'special'\n        END as RelationType,\n    pg_catalog.pg_get_userbyid(c.relowner) as RelationOwner,             \n    pg_size_pretty(pg_relation_size(n.nspname ||'.'|| c.relname)) as RelationSize\nFROM pg_catalog.pg_class c\nLEFT JOIN pg_catalog.pg_namespace AS n ON n.oid = c.relnamespace\n    WHERE  c.relkind IN ('r','s') \n    AND  (n.nspname !~ '^pg_toast' and nspname like 'pg_temp%')\nORDER BY pg_relation_size(n.nspname ||'.'|| c.relname) DESC;\n\n\n\nhttps://dataedo.com/kb/query/postgresql/list-tables-with-most-relationships\nselect * from\n(select relations.table_name as table_name, -- schema name and table name\n       count(relations.table_name) as relationships, -- number of table relationships\n       count(relations.referenced_tables) as foreign_keys, -- number of foreign keys in a table\n       count(relations.referencing_tables) as references, -- number of foreign keys that are refering to this table\n       count(distinct related_table) as related_tables, -- number of related tables\n       count(distinct relations.referenced_tables) as referenced_tables, -- number of different tables referenced with FKs (multiple FKs can refer to one table, so number of FKs might be different than number of referenced tables)\n       count(distinct relations.referencing_tables) as referencing_tables -- number of different tables that are refering to this table (similar to referenced_tables)\nfrom(\n     select pk_tco.table_schema || '.' || pk_tco.table_name as table_name,\n            fk_tco.table_schema || '.' || fk_tco.table_name as related_table,\n            fk_tco.table_name as referencing_tables,\n            null::varchar(100) as referenced_tables\n     from information_schema.referential_constraints rco\n     join information_schema.table_constraints fk_tco\n          on rco.constraint_name = fk_tco.constraint_name\n          and rco.constraint_schema = fk_tco.table_schema\n     join information_schema.table_constraints pk_tco\n          on rco.unique_constraint_name = pk_tco.constraint_name\n          and rco.unique_constraint_schema = pk_tco.table_schema\n    union all\n    select fk_tco.table_schema || '.' || fk_tco.table_name as table_name,\n           pk_tco.table_schema || '.' || pk_tco.table_name as related_table,\n           null as referencing_tables,\n           pk_tco.table_name as referenced_tables\n    from information_schema.referential_constraints rco\n    join information_schema.table_constraints fk_tco \n         on rco.constraint_name = fk_tco.constraint_name\n         and rco.constraint_schema = fk_tco.table_schema\n    join information_schema.table_constraints pk_tco\n         on rco.unique_constraint_name = pk_tco.constraint_name\n         and rco.unique_constraint_schema = pk_tco.table_schema\n) relations\ngroup by table_name\norder by relationships asc) results\n\nwhere substring(table_name, 5, 2) = 'd_'; -- substring(string, start_position, length)\n\n\n\nselect * from pg_catalog.current_database()\n\n\n\nselect * from current_role\nselect * from current_user\n\n\n\nselect * from pg_catalog.pg_backend_pid()\n\n\n\nselect \n    pg_get_userbyid(p.proowner) as owner,\n    n.nspname as function_schema,\n    p.proname as function_name,\n    l.lanname as function_language,\n    case when l.lanname = 'internal' then p.prosrc\n        else pg_get_functiondef(p.oid)\n        end as definition,\n    pg_get_function_arguments(p.oid) as function_arguments,\n    t.typname as return_type\nfrom pg_proc p\n    left join pg_namespace n on p.pronamespace = n.oid\n    left join pg_language l on p.prolang = l.oid\n    left join pg_type t on t.oid = p.prorettype \nwhere n.nspname not in ('pg_catalog', 'information_schema')\nand n.nspname = 'idb'\norder by function_schema, function_name;\n\n\n\nselect * from pg_stat_activity\nwhere usename != '' and usename != 'postgres'\norder by usename, pid\n\n\n\nhttps://www.postgresql.org/docs/9.6/catalog-pg-aggregate.html\n-- pg_proc contains data for aggregate functions as well as plain functions\nselect * from pg_proc\n-- pg_aggregate is an extension of pg_proc.\nselect * from pg_aggregate\n\n\n\nSELECT rolname FROM pg_roles;\n\n\n\nSELECT table_schema,table_name FROM information_schema.tables ORDER BY table_schema,table_name;\n\n\n\nSELECT column_name\nFROM   information_schema.columns\nWHERE  table_schema = 'schema'\nAND    table_name = 'table';\n\n\n\nUPDATE tablename\nSET columnname = someothervalue\nFROM ...\nWHERE ...\n\n\n\nReference\nCREATE MATERIALIZED VIEW view_name\nAS\nquery\nWITH [NO] DATA;\nWhen you refresh data for a materialized view, PostgreSQL locks the entire table therefore you cannot query data against it. To avoid this, you can use the CONCURRENTLY option.\nWith CONCURRENTLY option, PostgreSQL creates a temporary updated version of the materialized view, compares two versions, and performs INSERT and UPDATE only the differences.\nREFRESH MATERIALIZED VIEW CONCURRENTLY view_name;\n\n\n\nWITH myconstants (analyte_search) as (\n   values ('%Hexachlorocyclopentadiene%')\n)\n\nSELECT *\nFROM e_analyte, myconstants\nWHERE analyte ilike analyte_search\n   OR full_name ilike analyte_search\n   OR aliases ilike analyte_search;\n\n\n\nseq_key bigint NOT NULL DEFAULT nextval('seq_key'::regclass)\n\nALTER SEQUENCE seq_key RESTART WITH 3;\n\n\n\nThese could be refined further by creating a function.\n\n\nwith\nconst (param) as (\n    values ('%solid%')\n),\ndbrows as (\n   select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte\n  union\n    select * from dblink(\n        'dbname=chemcrit',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=ahtna',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=arkema_ph',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=bae_north',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=bayer_ldw',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=bcsa',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=c840_livent',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=cabotroad',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=centralia',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=centredale',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=eos',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=evraz_inwater',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=frenchtown_mill',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_columbus',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n    'dbname=gemt_meridian',\n    'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_springfield',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=reddog',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=shoreham',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=solvay',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=three_m_mb',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'     \n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n)\nselect \n    analyte, full_name, chem_class, cas_rn, aliases, \n    count(*) as num_instances, string_agg(db, '; ') as db\nfrom dbrows, const\nwhere analyte ilike param\n   or full_name ilike param\n   or aliases ilike param\ngroup by analyte, full_name, chem_class, cas_rn, aliases\norder by chem_class, analyte;\n\n\n\nwith\nconst (param) as (\n    values ('%SW8260%')\n),\ndbrows as (\n   select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod\n  union\n    select * from dblink(\n        'dbname=ahtna',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=arkema_ph',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=bae_north',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=bayer_ldw',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=bcsa',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=c840_livent',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=cabotroad',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=centralia',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=centredale',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=eos',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=evraz_inwater',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=frenchtown_mill',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_columbus',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_meridian',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_springfield',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=reddog',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=shoreham',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=solvay',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=three_m_mb',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n)\nselect \n    method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method,\n    count(*) as num_instances, string_agg(db, '; ') as db\nfrom dbrows, const\nwhere method_code ilike param\n   or description ilike param\n   or lab_prep_method ilike param\n   or lab_extraction_method ilike param\n   or lab_anal_method ilike param\n   or lab_leach_method ilike param\ngroup by method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method\norder by method_code;"
  },
  {
    "objectID": "dm.html",
    "href": "dm.html",
    "title": "Data Management",
    "section": "",
    "text": "EPA Guidance on Systematic Planning Using the Data Quality Objectives Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElement\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nOrganization\n\n\n\nIdentification and involvement of the project manager, sponsoring organization and responsible official, project personnel, stakeholders, scientific experts, etc. (e.g., all customers and suppliers).\n\n\n\n\n\n\n\nProject Goal\n\n\n\nDescription of the project goal, objectives, and study questions and issues.\n\n\n\n\n\n\n\nSchedule\n\n\n\nIdentification of project schedule, resources (including budget), milestones, and any applicable requirements (e.g., regulatory requirements, contractual requirements).\n\n\n\n\n\n\n\nData Needs\n\n\n\nIdentification of the type of data needed and how the data will be used to support the project’s objectives.\n\n\n\n\n\n\n\nCriteria\n\n\n\nDetermination of the quantity of data needed and specification of performance criteria for measuring quality.\n\n\n\n\n\n\n\nData Collection\n\n\n\nDescription of how and where the data will be obtained (including existing data) and identification of any constraints on data collection.\n\n\n\n\n\n\n\nQuality Assurance (QA)\n\n\n\nSpecification of needed QA and quality control (QC) activities to assess the quality performance criteria (e.g., QC samples for both field and laboratory, audits, technical assessments, performance evaluations, etc.).\n\n\n\n\n\n\n\nAnalysis\n\n\n\nDescription of how the acquired data will be analyzed (either in the field or the laboratory), evaluated (i.e., QA review/verification/validation), and assessed against its intended use and the quality performance criteria.\n\n\n\n\n\n\n\n\n\n\n\nWhen specifying the project goal (element #2 in Table 1), a key activity is to determine the key questions which the study will address once data and information are properly collected and analyzed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nSoundness\n\n\n\nThe extent to which the scientific and technical procedures, measures, methods or models employed to generate the information are reasonable for, and consistent with, the intended application.\n\n\n\n\n\n\n\nApplicability and Utility\n\n\n\nThe extent to which the information is relevant for the Agency’s intended use.\n\n\n\n\n\n\n\nClarity and Completeness\n\n\n\nThe degree of clarity and completeness with which the data, assumptions, methods, quality assurance, sponsoring organizations and analyses employed to generate the information are documented.\n\n\n\n\n\n\n\nUncertainty and Variability\n\n\n\nThe extent to which the variability and uncertainty (quantitative and qualitative) in the information or the procedures, measures, methods or models are evaluated and characterized.\n\n\n\n\n\n\n\nEvaluation and Review\n\n\n\nThe extent of independent verification, validation, and peer review of the information or of the procedures, measures, methods or models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchieving clarity in a project’s development becomes straightforward when using systematic planning, as almost every element of the planning process contributes to understanding how the project’s assumptions, methods, and proposed analyses will be conducted.\nPlanning for analyzing the data and information before collection clearly meets the intent of the GAFs.\nClear statements on the goals of the project developed through systematic planning leads to a better understanding of purpose and credibility of the results.\nSystematic planning leads to a clear statement of information needs and how the information will be collected, and leads to transparency in data quality.\n\n\n\n\n\n\n\n\nThe interaction amongst a multidisplinary team results in a clear understanding of the problem and the options available. Organizations that have used the DQO Process have found the structured format facilitated good communicaitons, documentation, and data collection design, all of which facilitated rapid peer review and approval.\n\n\nThe structure of the DQO Process provides a convenient way to document activities and decisions and to communicate the data collection design to others.\nThe DQO Process is an effective planning tool that can save resources by making data collection operations more resource-effective.\nThe DQO Process enables data users and technical experts to participate collectively in planning and to specify their needs prior to data collection. The DQO Process helps to focus studies by encouraging data users to clarify vague objectives and document clearly how scientific theory motivating this project is applicable to the intended use of the data.\nThe DQO Process provides a method for defining performance requirements appropriate for the intended use of the data by considering the consequences of drawing incorrect conclusions and then placing tolerable limits on them.\nThe DQO Process encourages good documentation for a model-based approach to investigate the objectives of a project, with discussion on how the key parameters were estimated or derived, and the robustness of the model to small perturbations. Upon implementing the DQO Process, your environmental programs can be strengthened"
  },
  {
    "objectID": "dm.html#resources",
    "href": "dm.html#resources",
    "title": "Data Management",
    "section": "Resources",
    "text": "Resources\n\nexecSQL\nPostgreSQL Tutorial\nPostgreSQL Tips\nDevDocs.io"
  },
  {
    "objectID": "dm.html#character-encoding",
    "href": "dm.html#character-encoding",
    "title": "Data Management",
    "section": "Character Encoding",
    "text": "Character Encoding\nWe deal with character encoding issues when importing data to databases all the time. All databases are encoded in UTF-8, and the most common non-ASCII, non-UTF-8 format is Microsoft’s custom encoding, which is CP-1252 (also known as win-1252 and a few other things). The character encoding of a file can’t necessarily be definitively determined by examination, but the Linux command-line tool file generally does a good job. If you want to change the encoding of a file rather than importing it in a known format, the Linux command-line tool iconv will do that for you. There’s a Python library on PyPI named chardet that will also diagnose file encodings.\nFor data managers, the workflow is to first guess that encoding errors on data import are due to the file being cp-1252. That covers about 90% of cases. Our import tool also automatically diagnoses instances where a file starts with a byte order mark (BOM), which covers most of the rest of the cases. For the remainders, Geany is usually the quickest way to check the file encoding.\nEverything that comes out of our databases is always in UTF-8, so I, at least, don’t ordinarily have encoding issues when importing data to R. For those who use data from other sources, it is a good idea to document a recommended workflow and set of tools."
  },
  {
    "objectID": "dm.html#summarization",
    "href": "dm.html#summarization",
    "title": "Data Management",
    "section": "Summarization",
    "text": "Summarization\nChemistry data frequently is summarized for use in analyses or for presentation using tables or maps. Summarization is ordinarily performed when there are multiple concentration values measured for a sample, or for a specific location, date, and depth. Multiple concentration values result from field or laboratory replications, from field splits created for quality control evaluations, and sometimes from sample reanalyses. Although field splits and laboratory replicates are created to support data quality assessments, all of the valid results that are produced are informative, are ordinarily stored in the project database, and are used to produce the most accurate possible estimate of the true concentration in a sample. When there are replicate results for a sample, the data will be averaged in a stepwise, or hierarchical, fashion. Because each level of the hierarchy represents a different source of variation, all the results at a single level are averaged together before results are averaged across levels. The different levels of replication, and the source of variation that each represents, are as follows:\n\nAverage across lab replicates\nAverage across multiple analyses (if they exist) for the same lab sample. This will average across multiple methods and reanalyses (different SDGs)\nAverage across multiple lab samples (if they exist) for the same sample number (split) and lab. Multiple lab samples may exist for the same sample number if the laboratory has created new laboratory sample IDs for reanalyses.\nAverage across multiple splits (if they exist) for the same lab and interpretive sample. Multiple sample numbers (splits) may exist for the same lab, or the same sample number may exist for different labs. Splits sent to different labs potentially have an additional source of variability in addition to variability between splits sent to the same lab.\nAverage across laboratories for the same main sample. This summarizes data by main sample (sample_id), which is typically the lowest level of data summarization to be used for data interpretation.\n\nBy default, data are summarized by successive averaging across these levels of replication, in the order given above. During the averaging process, data validation qualifiers and significant digits must be propagated. The rules for propagating the data validation qualifiers U (undetected), J (estimated), and R (rejected) are as follows:\n\nIf both detected and undetected data are to be averaged, then undetected data lower than the highest detected value will be taken at one-half the detection limit and averaged with the detected data, and the result will be identified as detected. Non-detects that are higher than the highest detected value will be omitted from the average.\nIf all data to be averaged are undetected, the result will be taken to be the lowest detection limit, and will be identified as undetected.\nIf J-qualified data are averaged with non-J-qualified data, the result will be J-qualified.\nIf R-qualified data are averaged with non-R-qualified data, the result will be R-qualified.\n\nSignificant digits are propagated so that the place (in the sense of one’s place, ten’s place, etc.) of the least significant digit of the average is equal to the highest place of the least significant digit of any of the values that are averaged.\nThese rules are built into custom aggregate functions in IDB that use the measurement_result data type.\nThese default data handling rules will be applied if no project-specific alternate rules are specified. The project manager, project technical staff, and data manager should evaluate, at the start of a project, whether an alternative approach is needed. Alternate data summarization rules should be summarized in the project plan or in the data management plan, if it exists. (Data managers: if not documented elsewhere, record this information in the Data Manager’s Manual.)\nNote that the handling of nondetects during hierarchical averaging and the presentation of nondetects in data summaries may be different. Regardless of whether nondetects are taken at half the detection limit or the full detection limit when averaging, the summarized result may be presented with either the full detection limit or half the detection limit. Reporting nondetects at the full detection limit should ordinarily be done when preparing data tables for reports or other deliverables. Data analyses to be conducted by Integral may be carried out using either half or full detection limits. The method of reporting nondetects should be specified when requesting data summaries."
  },
  {
    "objectID": "dm.html#chemistry",
    "href": "dm.html#chemistry",
    "title": "Data Management",
    "section": "Chemistry",
    "text": "Chemistry\n\nResources\n\nHazardous Waste Test Methods\nNational Environmental Methods Index\nSubstance Registry Service\nEIM Valid Values\nVerification and Validation\nQualifiers\nData Review\nChemical Lists\nPCBs\nWashington Water Resources Data Defs\nMeasurement Basis Conversions\nhttps://pubs.usgs.gov/wdr/WDR-WA-03-1/pdf/ADR_N.pdf\nhttp://www.eccsmobilelab.com/resources/literature/?Id=117\nConversions\n\n\n\nTEFs\n\nVan den Berg source document\nVan den Berg TEF table\n\n\n\nChemical Groups\n\nDioxin & Furans\nReference\n\n\nPCBs\nLearn about PCBs\n\nGeneral\nPCBs are a group of man-made organic chemicals consisting of carbon, hydrogen and chlorine atoms. The number of chlorine atoms and their location in a PCB molecule determine many of its physical and chemical properties. PCBs have no known taste or smell, and range in consistency from an oil to a waxy solid.\nPCBs belong to a broad family of man-made organic chemicals known as chlorinated hydrocarbons. PCBs were domestically manufactured from 1929 until manufacturing was banned in 1979. They have a range of toxicity and vary in consistency from thin, light-colored liquids to yellow or black waxy solids. Due to their non-flammability, chemical stability, high boiling point and electrical insulating properties, PCBs were used in hundreds of industrial and commercial applications including:\n\nElectrical, heat transfer and hydraulic equipment\nPlasticizers in paints, plastics and rubber products\nPigments, dyes and carbonless copy paper\nOther industrial applications\n\nCommercial Uses for PCBs\nAlthough no longer commercially produced in the United States, PCBs may be present in products and materials produced before the 1979 PCB ban. Products that may contain PCBs include:\n\nTransformers and capacitors\nElectrical equipment including voltage regulators, switches, re-closers, bushings, and electromagnets\nOil used in motors and hydraulic systems\nOld electrical devices or appliances containing PCB capacitors\nFluorescent light ballasts\nCable insulation\nThermal insulation material including fiberglass, felt, foam, and cork\nAdhesives and tapes\nOil-based paint\nCaulking\nPlastics\nCarbonless copy paper\nFloor finish\n\nThe PCBs used in these products were chemical mixtures made up of a variety of individual chlorinated biphenyl components known as congeners. Most commercial PCB mixtures are known in the United States by their industrial trade names, the most common being Arochlor.\nRelease and Exposure of PCBs\nToday, PCBs can still be released into the environment from:\n\nPoorly maintained hazardous waste sites that contain PCBs\nIllegal or improper dumping of PCB wastes\nLeaks or releases from electrical transformers containing PCBs\nDisposal of PCB-containing consumer products into municipal or other landfills not designed to handle hazardous waste\nBurning some wastes in municipal and industrial incinerators\n\nPCBs do not readily break down once in the environment. They can remain for long periods cycling between air, water and soil. PCBs can be carried long distances and have been found in snow and sea water in areas far from where they were released into the environment. As a consequence, they are found all over the world. In general, the lighter the form of PCB, the further it can be transported from the source of contamination.\nPCBs can accumulate in the leaves and above-ground parts of plants and food crops. They are also taken up into the bodies of small organisms and fish. As a result, people who ingest fish may be exposed to PCBs that have bioaccumulated in the fish they are ingesting.\nThe National Center for Health Statistics, a division of the Centers for Disease Control and Prevention, conducts the National Health and Nutrition Examination Surveys (NHANES). NHANES is a series of U.S. national surveys on the health and nutrition status of the noninstitutionalized civilian population, which includes data collection on selected chemicals. Interviews and physical examinations are conducted with approximately 10,000 people in each two-year survey cycle. PCBs are one of the chemicals where data are available from the NHANES surveys.\n\n\nPCB Congeners\nA PCB congener is any single, unique well-defined chemical compound in the PCB category. The name of a congener specifies the total number of chlorine substituents, and the position of each chlorine. For example: 4,4’-Dichlorobiphenyl is a congener comprising the biphenyl structure with two chlorine substituents - one on each of the #4 carbons of the two rings. In 1980, a numbering system was developed which assigned a sequential number to each of the 209 PCB congeners.\n\n\nPCB Homologs\nHomologs are subcategories of PCB congeners that have equal numbers of chlorine substituents. For example, the tetrachlorobiphenyls are all PCB congeners with exactly 4 chlorine substituents that can be in any arrangement.\n\n\nPCB Aroclor\nAroclor is a PCB mixture produced from approximately 1930 to 1979. It is one of the most commonly known trade names for PCB mixtures. There are many types of Aroclors and each has a distinguishing suffix number that indicates the degree of chlorination. The numbering standard for the different Aroclors is as follows:\n\nThe first two digits usually refer to the number of carbon atoms in the phenyl rings (for PCBs this is 12)\nThe second two numbers indicate the percentage of chlorine by mass in the mixture. For example, the name Aroclor 1254 means that the mixture contains approximately 54% chlorine by weight.\n\n\n\n\n\nQualifiers\n\nLabs may apply whatever flags they want to a result. Some data qualifiers are defined by EPA’s Functional Guidelines documents, which describe how data validation is to be conducted, and the use and interpretation of U, J, and R qualifiers is pretty universal (but older standards for Puget Sound data used E instead of J). Because the U, J, and R qualifiers are pretty universal and have implications for data usability, they are the only ones that are represented as Boolean fields in the meas_value column. All lab flags are put into the lab_flags column, and there is no lookup table for them, and there is no defined use for them. Similarly, the validator qualifiers (U, J, R, and possibly others) are put in the validator_flags column. If any of those three common qualifiers is in the validator_flags column, then the corresponding flags in meas_value should be set. The lab_conc_qual column is something of a relic, left over from the days when data were commonly provided in EPA’s Contract Laboratory Program (CLP) data format, which had a corresponding column. The lab_conc_qual column was meant to either contain “U” or be null. We don’t ordinarily use that column any more. Of the qualifiers you listed above, other than U, J, and R, N is commonly used to flag a tentatively identified compound, which means that the analyte code itself is uncertain. The d_labresult.tic column is meant to hold that information. The tic column is not part of the measurement_result data type because it is not used in any way during data aggregation (averaging or summing). I see that Jerry added other flags and qualifiers to the e_concqual table, but needn’t—really shouldn’t—be there. The e_concqual dictionary should have only “U” defined. It may seem odd to define a lookup table for only one value when a check constraint on the concentration qualifier columns could be used instead, but it’s easier to check relational constraints than to check check constraints programmatically.\n\n\n\nDuplicates\n\n“Duplicate” is a somewhat ambiguous term, but in practice it most commonly refers to field duplicates, which we ordinarily refer to as splits to avoid that ambiguity. Some QC data, particularly spikes, are frequently duplicated, so when we have lab QC data we may have values for spikes and spike duplicates. When we receive lab results in one big flat table that includes both analytical results for natural samples and results of lab QC samples, the word or code “DUP” in a column header or table cell could mean a couple of different things. Without seeing the original data source, I’m not sure where the “DUP” code in the “labqc_samp” column of your “d_labsample” table came from. I’m going to assume that it refers to a field duplicate, and not a spike duplicate.\n\n\nIdeally, samples are submitted to the laboratory “blind” so that the laboratory does not know which field samples are duplicates of one another. This is to prevent them from seeing that there’s a lot of variation between some pair of duplicates and deciding to re-run one or both of them. If the lab is producing highly variable data, we don’t want them to be able to hide it. Unfortunately, many field sampling programs use a suffix of “-D” or “-DUP” or something like that on the sample ID, so the lab knows which samples are field duplicates. If they know, they may pass that information back in their EDD.\n\n\nAlthough field duplicates are used as a QC check on laboratory performance, they are not lab QC samples themselves. They are just normal field samples (which have been split), and don’t need to have a laboratory QC sample ID assigned to them. Thus, field duplicates should not be listed in the “d_labqcsamp” table, so that table looks fine as it appears below. The same is true for the “d_labresult” table.\n\nThere are a couple of things to be changed about the “d_labsample” table as shown below:\n\nThe values in the “labqc_samp” column should be identifiers that appear in the “d_labqcsamp” table, not codes. The codes for the lab QC type should be in the d_labqcsamp.qc_type column, and neither “Natural” nor “DUP” should be used there.\nThe “d_labsample” table should have values in the “study_id” and “sample_no” columns, or a value in the “labqc_samp” column, but not both. There are other invalid combinations of columns also. The “d_labsample” table may have any one of the following tables as a parent: d_sampsplit, d_fldqcsplit, d_labqcsamp, d_bioaccum_samp, d_samptreatsplit, or d_bioasrepsamp. The “ck_one_sample” check constraint on the table enforces this rule. Check constraints like this are not run by the upsert scripts, so a set of staged data may pass all the checks performed by the upsert script and yet the INSERT into d_labsample will fail.\n\n\n\nMeasurement Basis\n\nR tool\nOrgMassSpecR\n\n\nGeneral\n\nData for soil and sediment are almost always reported on a dry-weight basis. If there’s anything to indicate a different basis, that deserves a closer look. Almost the only legitimate reason for a different basis for soil or sediment samples is when a leaching procedure has been applied (e.g., the Toxicity Characteristic Leaching Procedure, or TCLP); in those cases the data may be reported as the concentration in the leachate, so the basis may be “Wet” or “Whole” or “Unfiltered” – anything indicating an unfractionated liquid sample.\nData for tissues should ordinarily be reported in wet weight. Organisms’ homeostasis means that they maintain a nearly constant moisture content in their tissues, whereas the same is not true of materials like soil or sediment. If tissue data are reported in dry weight, check it carefully: labs can be sloppy about that.\nWater data are where things can be complex, because often water samples are filtered or centrifuged to remove particulates, which results in the water samples have a ‘dissolved’ basis. If the particulates are analyzed, and the results are then expressed in terms of the volume of the original sample, then the data will have a ‘particulate’ basis. Unfiltered, or whole, water, should have a basis of ‘Unfilt’, ‘Whole’, or sometimes ‘Wet’. Either of the first two of these are preferred, “Wet” is better used as a counterpart to “Dry” for soil, sediment, or tissue samples.\nThere are variations in the way things have been done in different databases. You may find that the measurement basis code for whole water samples differs from one to another, as in the third bullet above.\nIDB v.8 now has the “fraction” code, which is intended to be used to distinguish dissolved, particulate, and whole fractions of a water sample. In IDB v.8, the measurement basis for water samples will almost always be ‘Whole’. Sometimes sediment or soil samples are fractionated too, e.g., by sieving, and the fraction code should be used in those cases too, so the measurement basis will always be ‘dry’ in those cases.\nThere is an implicit association between measurement bases and units. For example, if the measurement basis is “Dry”, the units should not be “mg/L” because “…/L” implies a liquid, not a solid.\nThe measurement basis refers to the form of the sample material, which is represented in the denominator of concentration units. So codes of “Sediment” “Arsenic”, “mg/kg”, “Dry” should be read as “mg of arsenic per kg of dry sediment.”\n\n\n\nWet Weight\n\nWet weight (or as-is) basis means no calculation has been made to compensate for the moisture content of a sample. Wet weight refers to the weight of animal tissue or other substance including its contained water. (See also “Dry weight”)\n\n\n\nDry Weight\n\nDry weight basis means the lab has measured moisture content of a sample and calculated concentrations based on the percent solids present. Dry weight refers to the weight of animal tissue after it has been dried in an oven at 65°C until a constant weight is achieved. Dry weight represents total organic and inorganic matter in the tissue. (See also “Wet weight”).\n\n\n\nLipid\n\nLipid is any one of a family of compounds that are insoluble in water and that make up one of the principal components of living cells. Lipids include fats, oils, waxes, and steroids. Many environmental contaminants such as organochlorine pesticides are lipophilic.\n\n\n\n\n\nConversions\n\nWet to Dry\n\\[DryWt = \\frac{WetWt}{Percent Solids} * 100\\]\n\n\nDry to Wet\n\\[WetWt = DryWt * \\frac{PercentSolids}{100}\\]\n\n\nOrganic Carbon Normalization\n\\[OCnorm = \\frac{DryWt}{\\frac{PercentTOC}{100}}\\]\n\nDryWt & WetWt = concentration PercentSolids = percentage (no decimal)\n\n\n\nResource\n\n\n\n\nCalcs\n\n\n\n\n\n\n\nAnalytical Blanks\n\n\nTrip Blank\nThe trip blank is designed to identify levels of contamination from the exposure of the reagent or sorbent bed to the same atmospheres exposed to the analyte reagent or sorbent bed. The trip blank is prepared in the laboratory with the other reagents or adsorbents prior to shipping to the field. However, the trip blank is never exposed to the field atmospheres. It is simply sent along with the field samples to and from the site. The trip blank identified areas of exposure such as shipping temperatures and pressures, laboratory preparation of field samples and laboratory preparation of field samples for analysis.\n\nField Blank\nThe field blank is similar to the trip blank in that it is also prepared during the preparation of the field reagents or adsorbents. However, the field blank is exposed to the same atmospheres in the field as the field samples. This means that the field blank is opened during the charging of impingers or sorbents in the sample train. The field blank is also exposed during the exchanging of cartridges in SW-846, Method 0030 or when field reagents are being exchanged during a test run. In summary, field blanks consist of additional sample collection media (e.g., sorbent tubes, reagents, filters) which are transported to the monitoring site, exposed briefly at the site when the samples are exposed (but no stack gas is actually pulled through these blanks), and transported back to the laboratory for analysis, similar to a field sample. At least one field blank should be collected and analyzed for each test series.\n\n\nLaboratory Blank\nThe laboratory blank is a sample of the reagents or sorbents used during the sample train reagent preparation or recovery. The laboratory blank is a sample of the extraction solvent, the rinses used during sample recovery, or a sample from the batch of sorbent used to preparing sampling cartridges. Laboratory blanks include both method blanks and instrument blanks. Method blanks are carried through all steps of the measurement process (from extraction through analysis). A method blank is typically analyzed with each sample batch. Instrument blanks are used to demonstrate that an instrument system is free of contamination. Instrument blanks are typically analyzed prior to sample analysis and following the analysis of highly contaminated samples.\n\n\nReagent Blank\nThe reagent blank is a sample of the solvents used during recovery of the sample train after the test is completed. You recall, reagent blanks for both multi-metal and chromium +6 require that the reagent blank be the same volume as the renses used to recover the samples, from probe to impinger. This is because the blank value is substracted from the sample to obtain a final concentration.\n\n\nDiagram\n\n\n\n\n\nDetection Limits\nPresentation\n\nWhat affects detection limits?\n\nSample size\nConcentration of other constituents\nSample clean-up\nMethodology\nLab Performance\n\nExperience\nExtraction technique\nInstrument type and maintenance\n\n\ndetection_limit - the lowest possible value an instrument/method can sense a compound is present (think of it like a whisper - you can barely hear it, but know its there). This is better known as the “method detection limit”\nquantification_limit - the limit in which an instrument/method can actually start to quantify the amount of something which is present. If the result is between the detection_limit and the quantification_limit, the result is estimated, because the instrument/method cant confidently identify the amount of something until it reaches the quantification_limit.\nreporting_limit - usually project or dataset specific. this limit is used for data analysis/statistics. the reporting_limit is equal to either the detection_limit or quantification_limit. This is better known as the “reporting detection limit”.\n\n\nMethod Detection Limit (MDL)\n\nStatistically determined\nThe minimum concentration that can be measured with 99% confidence that the concentration is greater than zero\nConcentrations near MDL are estimates\nLaboratory, instrument, matrix, method, and analyte specific\nConcentrations at MDL expected to be a false positive 1% of the time, but false negatives 50% of the time\n\n\n\nMethod Reporting Limit (MRL)\n\nMay also be referred to as QL (quantitation limit), sample quantitation limit, or just RL (reporting limit)​\nDetermined by the lowest point of the calibration​\nNot as specific as MDL, labs can adust​\nConcentrations at MRL can be reliably quantified​\nMRL > MDL​\nAlso laboratory, instrument, matrix, method, & analyte specific\n\n\n\nMDL & MRL Relationship\n\n\n\nOther Detection Limits\n\nPQL\n\nConsidered to be lowest concentration that can be reliably quantified by a method\nLimit of Detection (LOD); Lowest concentration that can be detected with a 1% false negative rate.\n\nGenerally 2x to 3X MDL\n\nLimit of Quantitation (LOQ); similar to MRL\n\n\n\nPCDD/F & PCB specific\n2.5 times signal to noise\n\nEQL: Estimated Quantitation Limit\nEDL: Estimated Detection Limit\nSDL: Sample Detection Limit\nEMPC\n\nEstimated Maximum Possible Concentration (EMPC)\nPeak present but not all of the identification criteria is met\nAlways greater than MDL, may be greater than MRL\nGenerally treated a non-detect in TEQ calculations\nEMPCs can present data management difficulties and need to be reviewed in QC checks"
  },
  {
    "objectID": "food.html",
    "href": "food.html",
    "title": "Food",
    "section": "",
    "text": "Typical schedule for baking sourdough bread. Timing could change based on room temperature.\n\n\n\n\n\n\n\n\nTime\nStep\nNotes\n\n\n\n\n8:00am\nFeed starter\nDiscard all but at least 2 tbsp\n\n\n12:00pm\nAutolyse\n\n\n\n1:00pm\nAdd starter & salt\n\n\n\n1:30pm\nStretch & fold\nEvery 30 min for 2-2.5 hours\n\n\n3:30pm\nBulk rise\n1-3 hours, should rise 30-50%\n\n\n8:30pm\nPre shape & bench rest\n\n\n\n9:00pm\nFinal shaping & proof\nPlace in proofing baskets, cover, place in fridge overnight\n\n\n9:00am\nBake\n500 F in dutch oven, 20 min lid on, lower to 450, 15-30 min lid off"
  },
  {
    "objectID": "food.html#greek-yogurt-smoothie",
    "href": "food.html#greek-yogurt-smoothie",
    "title": "Food",
    "section": "Greek Yogurt Smoothie",
    "text": "Greek Yogurt Smoothie\n\nGreek yogurt\nBanana\nBerries\nOats\nHoney\nIce"
  },
  {
    "objectID": "food.html#overnight-oats",
    "href": "food.html#overnight-oats",
    "title": "Food",
    "section": "Overnight Oats",
    "text": "Overnight Oats\n\n90 grams oats\n90 grams greek yogurt\n140 grams milk\n5 grams chia and/or flaxseed\n15 grams honey\n\nStir or shake to combine. Refrigerate overnight."
  },
  {
    "objectID": "food.html#buffalo-cauliflower",
    "href": "food.html#buffalo-cauliflower",
    "title": "Food",
    "section": "Buffalo Cauliflower",
    "text": "Buffalo Cauliflower\n\nPreheat oven to 450 F\nBatter:\n\n3/4 cup flour\n1 tsp paprika\n2 tsp garlic powder\n1 tsp salt\n1/2 tsp black pepper\n3/4 cup milk\n\nSauce:\n\n1/4 cup buffalo sauce\n1 tbsp Honey\n1/2 tsp black pepper\n\nSplit cauliflower into florets and mix in batter\nBake for 20 minutes, flip after 10.\nBrush sauce onto cauliflower, bake 10 minutes.\nFlip cauliflower and brush again with sauce, bake 10 minutes."
  },
  {
    "objectID": "food.html#quinoa-cucumber-salad",
    "href": "food.html#quinoa-cucumber-salad",
    "title": "Food",
    "section": "Quinoa & Cucumber Salad",
    "text": "Quinoa & Cucumber Salad\n\nSalad:\n\n2 cups cucumber, spiralized or julienned\n2 cups chopped tomatoes\n2 large avocados, diced\n1 red onion, sliced\n2 cups cooked quinoa\n1 handful chopped parsley (or cilantro)\n\nDressing - blend the following:\n\n1 ripe avocado\n1/4 cup white wine vinegar\nJuice of one lime\nSalt and fresh cracked pepper, to taste\n3/4 cup olive oil"
  },
  {
    "objectID": "food.html#cougar-gold-mac-cheese",
    "href": "food.html#cougar-gold-mac-cheese",
    "title": "Food",
    "section": "Cougar Gold Mac & Cheese",
    "text": "Cougar Gold Mac & Cheese\n\n6 cups water\n2 1/2 cups pasta\n2 tbsp butter\n2 tbsp all-purpose flour\n1 1/3 cups milk\n1 1/3 cups heavy cream\n1 1/2 cups Cougar Gold, grated\n1 1/2 cups Crimson Fire, grated\n1/2 cup ricotta\n1/2 tbsp chicken bouillon\nground black pepper, paprika to taste\n1/2 cup fresh breadcrumbs (optional)\n\nPreheat oven to 350° F.\nBring the water, salt and pasta to a rolling boil in a medium saucepan. Cook just until tender. Drain pasta, put into prepared baking dish and set aside. Meanwhile prepare the sauce. In a saucepan over medium-low heat, melt 2 tablespoons of butter. While whisking, gradually add the flour. Whisk for about 2 minutes or until golden and bubbling. Very slowly add the milk, whisking constantly to avoid developing lumps. Simmer for 15 minutes until thickened (alfredo sauce consistency), stirring often to prevent mixture from burning. Remove from heat and stir in Cougar cheese, ricotta, chicken bouillon, paprika and black pepper to taste. Pour sauce onto cooked pasta (do not stir). Optionally top with breadcrumbs tossed in melted butter. Bake for 30 minutes until browned and bubbling."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to geocoug’s resource toolkit. This site was built using quarto.\n\n\n\n\n\n\n\n\n\n\nDBMS\n\n\nDatabase tidbits & SQL snippets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Management\n\n\nData management guidelines and resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFood\n\n\nRecipies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython\n\n\nPython snippets and resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\n\n\nR and RStudio Reference Mateiral\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources\n\n\nResource references and links\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nServer\n\n\nAdministration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTools\n\n\nDevelopment Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkouts\n\n\nWorkouts and routines\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "john.html",
    "href": "john.html",
    "title": "John’s Workouts",
    "section": "",
    "text": "Lift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nDumbbell Bench Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbell Incline Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Flys\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nTricep Push Down\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\nFailure\n\n\n\nView\n\n\n\n\n\n\n\nSit-ups\n\n\n\n3\n\n\n\n12\n\n\n\nView\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nCable Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFarmer Carry\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nKettlebell Deadlift\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\nView\n\n\n\n\n\n\n\nHalf Squat\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n3\n\n\n\n12\n\n\n\nView\n\n\n\n\n\n\n\n1 leg balance\n\n\n\n3\n\n\n\n30 sec\n\n\n\nNaN\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nBird Dog\n\n\n\n3\n\n\n\n10\n\n\n\nView"
  },
  {
    "objectID": "python.html",
    "href": "python.html",
    "title": "Python",
    "section": "",
    "text": "Credit - Many of the Python methods below are taken directly from an RDN blog"
  },
  {
    "objectID": "python.html#cheat-sheet",
    "href": "python.html#cheat-sheet",
    "title": "Python",
    "section": "Cheat Sheet",
    "text": "Cheat Sheet\n\nMetaCharacters (Need to be escaped)\n. ^ $ * + ? { } [ ] \\ | ( )\n\n\nCharacters\n. - Any Character Except New Line \\d - Digit (0-9) \\D - Not a Digit (0-9) \\w - Word Character (a-z, A-Z, 0-9, _) \\W - Not a Word Character \\s - Whitespace (space, tab, newline) \\S - Not Whitespace (space, tab, newline)\n\n\nCharacter Classes\n[] - Matches Characters in brackets [^ ] - Matches Characters NOT in brackets [a-z] - Any lowercase character between a and z [A-Z] - Any UPPERCASE character between A and Z\n\n\nQuantifiers\n* - 0 or More + - 1 or More ? - 0 or One {3} - Exact Number {3,4} - Range of Numbers (Minimum, Maximum) {3,} - At least 3\n\n\nAnchors & Boundaries\n\\b - Word Boundary \\B - Not a Word Boundary ^ - Beginning of a String $ - End of a String\n\n\nLogic\n| - Either Or ( ) - Group \\1 - Contents of group 1\n\n\nWhite-space\n\\t - Tab \\r - Carriage return \\n - New line\n\n\n\nCode\nimport re\n\ntext_string = \"\"\"\nHello world\n\n8001234567\n800-321-7654\n900.987.6543\n\nsome.email@email.com\nmycompany@company.net\nwierd-12-address-4@somedomain.blah\n\"\"\"\n\npattern = re.compile(r\"[0-9]{3}[.-]?[0-9]{3}[.-]?[0-9]{4}\")\nmatches = re.finditer(pattern, text_string)\n\nfor match in matches:\n    print(match)\n    print(match.span())\n    print(text_string[match.start() : match.end()])\n\n\n<re.Match object; span=(14, 24), match='8001234567'>\n(14, 24)\n8001234567\n<re.Match object; span=(25, 37), match='800-321-7654'>\n(25, 37)\n800-321-7654\n<re.Match object; span=(38, 50), match='900.987.6543'>\n(38, 50)\n900.987.6543"
  },
  {
    "objectID": "r.html#r",
    "href": "r.html#r",
    "title": "R",
    "section": "R",
    "text": "R\n\n\nCode\nx <- 5 * 5\nx\n\n\n[1] 25"
  },
  {
    "objectID": "r.html#sql",
    "href": "r.html#sql",
    "title": "R",
    "section": "SQL",
    "text": "SQL\nThere are a couple ways to connect to a database instance. These examples show how to connect to a local PostgreSQL instance, and how to use the connection information to query a database.\n\nOption 1\nCreate connection chunk, then call connection in each subsequent chunk.\n\n\nCode\nlibrary(DBI)\n\npass <- readLines(\"../../pwd.txt\")\ndb = dbConnect(\n  RPostgres::Postgres(),\n  dbname   = \"personal\",\n  host     = \"localhost\",\n  port     = 5432,\n  user     = \"cgrant\",\n  password = pass\n)\n\n\n\n\nCode\nselect * from workouts limit 10;\n\n\n\n\nOption 2\nSet default connection in the setup chunk (pretend its the setup chunk).\n\n\nCode\nlibrary(DBI)\n\ndb = dbConnect(\n  RPostgres::Postgres(),\n  dbname   = \"personal\",\n  host     = \"localhost\",\n  port     = 5432,\n  user     = \"cgrant\",\n  password = pass\n)\nknitr::opts_chunk$set(connection = \"db\")\n\n\nNow you dont have to specify the connection for each chunk.\n\n\nCode\nselect * from workouts limit 10;"
  },
  {
    "objectID": "r.html#python",
    "href": "r.html#python",
    "title": "R",
    "section": "Python",
    "text": "Python\nTo use a Python engine, you need to call library(reticulate) link\n\n\nCode\n# install.packages(\"reticulate\")\n\nSys.setenv(RETICULATE_PYTHON = \"/Users/cgrant/venvs/dev/bin/python\")\nlibrary(reticulate)\npy_config()\n\n\npython:         /Users/cgrant/venvs/dev/bin/python\nlibpython:      /opt/homebrew/opt/python@3.10/Frameworks/Python.framework/Versions/3.10/lib/python3.10/config-3.10-darwin/libpython3.10.dylib\npythonhome:     /Users/cgrant/venvs/dev:/Users/cgrant/venvs/dev\nversion:        3.10.10 (main, Feb 16 2023, 02:49:39) [Clang 14.0.0 (clang-1400.0.29.202)]\nnumpy:          /Users/cgrant/venvs/dev/lib/python3.10/site-packages/numpy\nnumpy_version:  1.23.4\n\nNOTE: Python version was forced by RETICULATE_PYTHON\n\n\nNow you can run Python code\n\n\nCode\nx = 4 * 4\nprint(x)\n\n\n16\n\n\nYou can import libraries as normal\n\n\nCode\nimport pandas as pd\n\nvals = {\"col1\": ['a', 'b', 'c', 'd'], \"col2\": [1, 10, 100, 1000]}\ndf = pd.DataFrame(vals)\nprint(df)\n\n\n  col1  col2\n0    a     1\n1    b    10\n2    c   100\n3    d  1000\n\n\nAccess objects created within Python chunks from R using py$<var>\n\n\nCode\nlibrary(DT)\ndatatable(py$df)"
  },
  {
    "objectID": "r.html#leaflet",
    "href": "r.html#leaflet",
    "title": "R",
    "section": "Leaflet",
    "text": "Leaflet\nUse the leaflet map below to explore.\n\n\nCode\nlibrary(leaflet)\nlibrary(dplyr)\n\nleaflet() %>% \n  setView(lng=-122.90486, lat=47.03576, zoom=16) %>% \n  addTiles() %>% \n  addMarkers(lng=-122.90486, lat=47.03576, popup=\"WA State Capitol\")"
  },
  {
    "objectID": "r.html#packages",
    "href": "r.html#packages",
    "title": "R",
    "section": "Packages",
    "text": "Packages\n\nDT::datatables - options"
  },
  {
    "objectID": "r.html#connection",
    "href": "r.html#connection",
    "title": "R",
    "section": "Connection",
    "text": "Connection\n\n\nCode\npass <- readLines(\"../../pwd.txt\")\n\nconn <- DBI::dbConnect(\n  RPostgres::Postgres(),\n  dbname   = \"personal\",\n  host     = \"localhost\",\n  port     = 5432,\n  user     = \"cgrant\",\n  password = pass\n)"
  },
  {
    "objectID": "r.html#query",
    "href": "r.html#query",
    "title": "R",
    "section": "Query",
    "text": "Query\n\n\nCode\nlibrary(glue)\n\ntbls <- dbListTables(conn)\nsql <- glue(\"SELECT * FROM workouts limit 1000;\")\nres <- dbGetQuery(conn, sql)\ncols <- names(res)\ndatatable(res, options=list(scrollX=T))"
  },
  {
    "objectID": "r.html#temperature",
    "href": "r.html#temperature",
    "title": "R",
    "section": "Temperature",
    "text": "Temperature\nReference Source\n\n\nCode\nlibrary(leaflet)\nlibrary(dplyr)\n\nf <- \"./static/NOAA-SeaTac_cleaned.csv\"\ndt <- read.csv(f)\n\ndf <- as.data.frame(dt)\n\nupdatemenus <- list(\n  list(\n    active = 0,\n    x = -.125,\n    type= 'buttons',\n    buttons = list(\n      list(\n        label = \"Wet Bulb\",\n        method = \"update\",\n        args = list(list(visible = c(TRUE, \"legendonly\")))),\n      list(\n        label = \"Dry Bulb\",\n        method = \"update\",\n        args = list(list(visible = c(\"legendonly\", TRUE))))\n    )\n  )\n)\n\nplt <- plot_ly(data = df) %>%\n  add_markers(x=as.Date(df$Timestamp), y=df$HourlyWetBulbTemperature, name=\"Wet Bulb\") %>%\n  add_markers(x=as.Date(df$Timestamp), y=df$HourlyDryBulbTemperature, name=\"Dry Bulb\", visible=\"legendonly\") %>%\n  layout(title = \"SeaTac 2020 Temperature Data\", \n         showlegend=FALSE,\n         xaxis=list(zeroline = FALSE,title=\"Date\"),\n         yaxis=list(zeroline = FALSE,title=\"Temperature (F)\"),\n         updatemenus=updatemenus)\n\n\nError in layout(., title = \"SeaTac 2020 Temperature Data\", showlegend = FALSE, : unused arguments (title = \"SeaTac 2020 Temperature Data\", showlegend = FALSE, xaxis = list(zeroline = FALSE, title = \"Date\"), yaxis = list(zeroline = FALSE, title = \"Temperature (F)\"), updatemenus = updatemenus)\n\n\nCode\nplt\n\n\nError in eval(expr, envir, enclos): object 'plt' not found"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Developer\n\ndevdocs.io - Searchable documentations\nss64 - CLI reference guide\nReadTheDocs - Create, host, and browse documentation\nexecsql - Run SQL with metacommands\nBootstrap - Web framework\npgAdmin - PostgreSQL sandbox\nRegex - Regular expressions\nGoogle Colab - Collaborative Python notebooks\ngeojson.io - Create, view, and share maps\nrepl.it - Collaborative in-browser IDE. 50+ languages\nIntegromat - Online scenario automation\nPostgreSQL cheatsheet - PostgreSQL cheatsheet\npython-utils - Playground for Python utilities\nSpektran - Collection of useful color tools\nObservable - JavaScript notebooks\nHTML Dog - HTML tutorials\nCrontab - Crontab scheduler\nMedia Library\nUTF-8 Character Debug - UTF-8 character debugging chart\nLaTeX Basics\nQuarto - Scientific and technical publishing system built on Pandoc\n\n\n\n\nGIS\n\nepsg.io - Spatial reference systems\nArcGIS - ArcGIS Python\n\n\n\n\nMusic\n\nSongsterr - Guitar tabs\nUltimate Guitar - Guitar tabs\nFlagrantior - Music theory\nmusictheory.net - Music theory\nTeoria - Music theory\nJustinGuitar - Guitar lessons\nHowToPlayPiano - Piano lessons\nSoundation - Broswer based music maker\n\n\n\n\ne-Books\n\nProject Gutenberg\nLibriVox\nStandard eBooks\n\n\n\n\nMiscellaneous\n\nFree Learning - List of free educational resources"
  },
  {
    "objectID": "server.html",
    "href": "server.html",
    "title": "Server",
    "section": "",
    "text": "General: _rwxrwxrwx 1 owner group\n_ | rwx | rwx | rwx = Special | Owner | Group | All Users\n\n\n\n4 | r = Read\n2 | w = Write\n1 | x = Execute\n\n\n\n\n\n0 = ---\n1 = --x\n2 = -w-\n3 = -wx\n4 = r-\n5 = r-x\n6 = rw-\n7 = rwx\n\n\n\n\n\nchgrp = Change group\nExample: sudo chgrp -R <group> <folder>\nchown = Change ownership\nExample: sudo chown -R <user>:<group> <file/folder>\nchmod = Change permissions\nExample: sudo chmod -R 774 <file/folder>\nMake new files inherit the group: sudo chmod g+s <folder>\n\n\n\nCreate a shared directory for a group.\n\nCreate a shared directory for users to access: /share\nAssign users to a common group (staff): sudo usermod -a -G staff <user>\nVerify user groups: groups <user>\nCreate shared directory and assign permissions:\nsudo mkdir /share && \\\nsudo chgrp -R staff /share && \\  # assign group\nsudo chmod -R g+w /share && \\  # permissions\nsudo chmod -R +s /share  # inherit permissions for newly created files/folders"
  },
  {
    "objectID": "server.html#server-initialization",
    "href": "server.html#server-initialization",
    "title": "Server",
    "section": "Server Initialization",
    "text": "Server Initialization\nInitial Linux server setup steps:\n\nLogin to server as root: ssh root@<ip>\nCreate new admin user: sudo adduser <user>\nAdd user to sudo group: sudo usermod -aG sudo <user>\nDisable root login\n\nOpen SSH configuration: sudo vi /etc/ssh/sshd_config\nSet PermitRootLogin to no\n\nConfigure Uncomplicated Firewall (UFW)\n\nAllow SSH connections: sudo ufw allow OpenSSH\nOpen port 80 and 443 for Apache: sudo ufw allow 'Apache Full'\nTurn on firewall: sudo ufw enable\nCheck: sudo ufw status verbose\n\nReboot the server: sudo reboot\nLogin as new user: ssh <user>@<ip>\nCheck for package updates apt list --upgradable 1, Upgrade packages sudo apt upgrade\nShow system info: uname -a\nEven though the kernel was updated, need to reboot to take effect: sudo reboot\nJust to be sure: sudo apt update && sudo apt upgrade\nShutdown for backup/snapshot sudo shutdown -h now\nAfter snapshot, navigate to provider website and restart machine"
  },
  {
    "objectID": "server.html#python-web-app",
    "href": "server.html#python-web-app",
    "title": "Server",
    "section": "Python Web App",
    "text": "Python Web App\n\nFlask App Configuration\n\nClone repository:\nsudo mkdir /etc/local/webs && \\\ncd /etc/local/webs && \\\nsudo git clone https://github.com/app.git && \\\nsudo chown -R <user>:<user> app && \\\ncd app\nCreate python virtual environment & install dependencies:\npython -m venv .venv && \\\nsource .venv/bin/activate && \\\npython -m pip install -r requirements.txt\nConfigure WSGI - /usr/local/webs/app/app.wsgi:\n#!/usr/bin/python\nimport sys\nimport logging\nimport os\n\nAPP_DIR = '/usr/local/webs/app'\nos.environ[\"APP_DIR\"] = APP_DIR\n\nlogging.basicConfig(stream=sys.stderr)\nsys.path.insert(0, APP_DIR)\n\nfrom app import app as application\nUpdate owner:group\ncd /usr/local/web && \\\nsudo chown -R www-data:www-data cmat\n\n\n\nApache Configuration\n\nCreate custom Apache log directory:\nsudo mkdir /usr/local/webs/apache-logs && \\\nsudo touch /usr/local/webs/apache-logs/app/error.log && \\\nsudo touch /usr/local/webs/apache-logs/app/access.log && \\\nsudo chown -R www-data:www-data /usr/local/webs/apache-logs\nCreate configuration file:\ncd /etc/apache2/sites-available && \\\nsudo vi app.conf\nConfiguration - app.conf:\n<VirtualHost *:80>\n    ServerName {DNS}\n\n    ServerSignature Off\n\n    RewriteEngine On\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n</VirtualHost>\n\n<VirtualHost *:443>\n    ServerAdmin webmaster@localhost\n    ServerName {DNS}\n\n    DocumentRoot /usr/local/webs/app\n\n    WSGIDaemonProcess web-app threads=5 python-home=/usr/local/webs/app/.venv\n    WSGIProcessGroup web-app\n    WSGIScriptAlias / /usr/local/webs/app/app.wsgi\n    WSGIPassAuthorization On\n    <Directory /usr/local/webs/app>\n            Order allow,deny\n            Allow from all\n    </Directory>\n\n    <Location />\n            Require all granted\n    </Location>\n\n    ErrorLog /usr/local/webs/apache-logs/app/error.log\n    CustomLog /usr/local/webs/apache-logs/app/access.log combined\n\n    SSLEngine on\n    SSLCertificateFile /etc/letsencrypt/live/{DNS}/fullchain.pem\n    SSLCertificateKeyFile /etc/letsencrypt/live/{DNS}/privkey.pem\n    Include /etc/letsencrypt/options-ssl-apache.conf\n</VirtualHost>\nTest configuration:\nsudo apache2ctl configtest\nEnable the site\nsudo a2ensite app.conf\nRestart Apache service\nsudo systemctl restart apache2"
  },
  {
    "objectID": "server.html#jupyter-server",
    "href": "server.html#jupyter-server",
    "title": "Server",
    "section": "Jupyter Server",
    "text": "Jupyter Server\n\nCreate jupyter user\nsudo adduser jupyter && \\\nsudo usermod -a -G staff jupyter\nsudo su jupyter && \\\nInstall Jupyter Lab\nsource /home/jupyter/.venv/bin/activate && \\\npython -m pip install jupyterlab && \\\njupyter-lab --generate-config\nConfigure Jupyter\nc.NotebookApp.ip = '*'\nc.NotebookApp.notebook_dir = '/home/jupyter/notebooks/'\nc.NotebookApp.open_browser = False\nc.NotebookApp.password = '' # hashed password\nc.NotebookApp.port = 9999\nConfigure Apache:\n<VirtualHost *:80>\n    ServerName <DNS ENTRY>\n    ServerSignature Off\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n\n    ProxyPreserveHost On\n    ProxyPass \"/\" \"http://localhost:9999/\"\n    ProxyPassReverse \"/\" \"http://localhost:9999/\"\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =<DNS ENTRY>\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n</VirtualHost>\n\n<IfModule mod_ssl.c>\n    <VirtualHost *:443>\n        ServerName <DNS ENTRY>\n        ServerSignature Off\n\n        ErrorLog /var/log/apache2/redirect.error.log\n        LogLevel warn\n\n        ProxyPreserveHost On\n        ProxyPass \"/\" \"http://localhost:9999/\"\n        ProxyPassReverse \"/\" \"http://localhost:9999/\"\n\n    <Location \"/api/kernels/\">\n        ProxyPass        ws://localhost:9999/api/kernels/\n            ProxyPassReverse ws://localhost:9999/api/kernels/\n    </Location>\n\n        SSLCertificateFile /etc/letsencrypt/live/<DNS ENTRY>/fullchain.pem\n        SSLCertificateKeyFile /etc/letsencrypt/<DNS ENTRY>/privkey.pem\n        Include /etc/letsencrypt/options-ssl-apache.conf\n    </VirtualHost>\n</IfModule>\nEnable Apache modules\nsudo a2enmod proxy\nsudo a2enmod proxy_http\nsudo a2enmod proxy_wstunnel\nGenerate SSL certs\nsudo certbot --apache certonly\nEnable the site\nsudo a2ensite jupyter.conf && \\\nsudo systemctl reload apache2 && \\\nsudo systemctl status apache2\nCreate the Jupyter service: /lib/systemd/system/jupyter.service\n# service name:     jupyter.service\n# path:             /lib/systemd/system/jupyter.service\n\n[Unit]\nDescription=Jupyter Notebook Server\n\n[Service]\nType=simple\nPIDFile=/run/jupyter.pid\nExecStart=/bin/bash -c \"/home/jupyter/.venv/bin/jupyter lab --no-browser\"\nUser=jupyter\nGroup=staff\nWorkingDirectory=/home/jupyter/notebooks\nRestart=always\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target\nEnable the service\nsudo systemctl daemon-reload && \\\nsudo systemctl start jupyter.service && \\\nsudo service jupyter status"
  },
  {
    "objectID": "server.html#rstudio-server",
    "href": "server.html#rstudio-server",
    "title": "Server",
    "section": "Rstudio Server",
    "text": "Rstudio Server\n\nInstall:\nwget https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2022.07.2-576-amd64.deb && \\\nsudo gdebi rstudio-server-2022.07.2-576-amd64.deb && \\\nrm rstudio-server-2022.07.2-576-amd64.deb && \\\nsudo adduser rstudio\nApache config\n<VirtualHost *:80>\n    ServerName <DNS ENTRY>\n    ServerSignature Off\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n\n    ProxyPreserveHost On\n    ProxyPass \"/\" \"http://localhost:8787/\"\n    ProxyPassReverse \"/\" \"http://localhost:8787/\"\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =<DNS ENTRY>\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n</VirtualHost>\n\n<IfModule mod_ssl.c>\n    <VirtualHost *:443>\n        ServerName <DNS ENTRY>\n        ServerSignature Off\n\n        ErrorLog /var/log/apache2/redirect.error.log\n        LogLevel warn\n\n        ProxyPreserveHost On\n        ProxyPass \"/\" \"http://localhost:8787/\"\n        ProxyPassReverse \"/\" \"http://localhost:8787/\"\n\n        SSLCertificateFile /etc/letsencrypt/live/<DNS ENTRY>/fullchain.pem\n        SSLCertificateKeyFile /etc/letsencrypt/live/<DNS ENTRY>/privkey.pem\n        Include /etc/letsencrypt/options-ssl-apache.conf\n    </VirtualHost>\n</IfModule>\nGenerate SSL certs\nsudo certbot --apache certonly\nEnable the site\nsudo a2ensite rstudio.conf && \\\nsudo systemctl reload apache2 && \\\nsudo systemctl status apache2"
  },
  {
    "objectID": "server.html#docker",
    "href": "server.html#docker",
    "title": "Server",
    "section": "Docker",
    "text": "Docker\nCheck out additional Docker examples here.\n\nInstallation (Ubuntu)\nSteps are pulled from a Digital Ocean tutorial.\n\nUpdate list of packages and install prerequisites.\nsudo apt update && \\\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\nAdd the GPG key for the official Docker repository.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nAdd the Docker repository to APT sources and update package list from the new repo.\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"\nMake sure install is pulling from Docker repo instead of default Ubuntu.\napt-cache policy docker-ce\nInstall Docker.\nsudo apt install docker-ce\nCheck that Docker is running.\nsudo systemctl status docker\nExecute Docker commands without sudo.\nsudo usermod -aG docker ${USER} && \\\nsu - ${USER}\nVerify user is now aded to the docker group.\ngroups\n\n\n\nWeb Servers\n\nCaddy\nDocs\n\nCreate a docker-compose.yml\n---\nversion: \"3.8\"\n\nservices:\n\ncaddy:\n    image: caddy:latest\n    restart: always\n    ports:\n    - \"80:80\"\n    - \"443:443\"\n    volumes:\n    - ./Caddyfile:/etc/caddy/Caddyfile\n    - ./docs:/srv  # Serve static files (ie. Quarto)\n    - caddy_data:/data\n    - caddy_config:/config\n\nvolumes:\ncaddy_data:\ncaddy_config:\nBuild a Caddyfile\n\nDevelopment static site on localhost\nhttps:// {\n    tls internal {\n        on_demand\n    }\n    root * /srv\n    file_server\n}\nStatic site\ndomain.com {\n    root * /srv\n    file_server\n}\nReverse proxy\ndomain.com {\n    reverse_proxy dockerservice:port\n}"
  },
  {
    "objectID": "tools.html#movie-gif",
    "href": "tools.html#movie-gif",
    "title": "Tools",
    "section": "Movie > Gif",
    "text": "Movie > Gif\nffmpeg -i file.mov -s 600x400 -pix_fmt rgb24 -r 20 -f gif - | gifsicle --optimize=3 --delay=3 > file.gif"
  },
  {
    "objectID": "tools.html#resources",
    "href": "tools.html#resources",
    "title": "Tools",
    "section": "Resources",
    "text": "Resources\n\nCheatSheet\nConvert Docker command to docker-compose.yml"
  },
  {
    "objectID": "tools.html#image-vs.-container",
    "href": "tools.html#image-vs.-container",
    "title": "Tools",
    "section": "Image vs. Container",
    "text": "Image vs. Container\nImage - Application we want to run\nContainer - Instance of that image running as a process"
  },
  {
    "objectID": "tools.html#docker-basics",
    "href": "tools.html#docker-basics",
    "title": "Tools",
    "section": "Docker Basics",
    "text": "Docker Basics\n\nCreate an Nginx container\ndocker run -p 80:80 -d --name webhost nginx\n\nDownloads Nginx from Docker Hub\nStarts new container from that image\nOpened port 80 on host IP\nRoutes port 80 traffic to the container IP, port 80\nView container at http://localhost:80\n\n\n\nOther examples\ndocker run -p 80:80 -d --name nginx nginx\ndocker run -p 8080:80 -d --name httpd httpd\ndocker run -p 3306:3306 --platform linux/amd64 -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql\nCreate a JupyterLab instance and attach your current directory as a volume: docker run -it --rm -p 8888:8888 -v $(PWD):/home/jovyan jupyter/pyspark-notebook\n\n\nProcesses and configurations\nCheck processes running inside a container: docker top <container>\nContainer configuration: docker <container> inspect\nCheck container stats (memory, cpu, network): docker stats <container>\n\n\nGetting a shell inside containers\nStart a new container interactively: docker run -it <container>\nRun commands in existing container: docker exec -it <container>\n\nExample: Start a container interactively and launch bash within it\n\nStart container and launch bash: docker run -it --name ubuntu ubuntu bash\nRun some bash command: apt-get install -y curl\nExit the container: exit\nStart and re-enter the container: docker start -ai ubuntu\n\n\n\nExample: Launch shell in running container\ndocker exec -it <container> bash\n\n\n\nPull an image from docker hub\ndocker pull <imagename>"
  },
  {
    "objectID": "tools.html#docker-networks",
    "href": "tools.html#docker-networks",
    "title": "Tools",
    "section": "Docker Networks",
    "text": "Docker Networks\n\nEach container is connected to a private virtual network (called “bridge”).\nEach virtual network routes through NAT firewall on host IP.\nAll containers on a virtual network can talk to each other without -p\nBest practice: Create a new virtual network for each app.\nYou can skip virtual networks and use the host IP (--net=host).\n\nGet container IP: docker inspect --format '{{ .NetworkSettings.IPAddress }}' <container>\n\nPublishing (#:#)\nexample: 8080:80\nleft number: published/host port\nright number: listening/container port\nTraffic passing through port 8080 on the HOST will be directed to port 80 on the container.\n\n\nDNS\nDocker uses container names as host names.\nDont rely on IPs for inter-communication.\nBest Practice Always use custom networks.\n\nAssignment\nCheck different curl versions within current versions of Ubuntu and CentOS.\nRun “curl –version” on both operating systems.\n\nSteps\nubuntu: apt-get update && apt-get install curl\ncentos: yum update curl\nThen…\ncurl --version\nAlso:\nCheck out command docker --rm"
  },
  {
    "objectID": "tools.html#dockerfiles",
    "href": "tools.html#dockerfiles",
    "title": "Tools",
    "section": "Dockerfiles",
    "text": "Dockerfiles\nRecipe for creating images\nEach Dockerfile stanza such as “RUN”, “CMD”, etc. are stored as a single image layer. Docker caches each layer by giving it a unique SHA (hash), so whenever the image is (re)built, it can check to see if a layer has changed, and if not, it will use the cached layer.\nDocker builds images top down, so it is best practice to structure the Dockerfile in such a way that lines which will change the most are at the bottom, and lines that will change the least are at the top. If a line is changed (ie. source code changes) Docker will rebuild that line, and thus each line after that will also need to be rebuilt."
  },
  {
    "objectID": "tools.html#keeping-the-docker-system-clean",
    "href": "tools.html#keeping-the-docker-system-clean",
    "title": "Tools",
    "section": "Keeping the Docker system clean",
    "text": "Keeping the Docker system clean\ndocker system prune - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache"
  },
  {
    "objectID": "tools.html#volumes-an-bind-mounts",
    "href": "tools.html#volumes-an-bind-mounts",
    "title": "Tools",
    "section": "Volumes an Bind Mounts",
    "text": "Volumes an Bind Mounts\nVolumes - Special location outside of container UFS\nBind Mounts - Link container path to host path\nBuild an image and named volume (persistent): docker run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql:/var/lib/mysql --platform linux/amd64 mysql"
  },
  {
    "objectID": "tools.html#initialize",
    "href": "tools.html#initialize",
    "title": "Tools",
    "section": "Initialize",
    "text": "Initialize\n\nLaunch Git Bash\nNavigate to project directory\ninitialize git repository in the folder root: git init\ncreate new file in directory: touch filename.extension\nlist files in root: ls\ncheck which files git recognizes: git status"
  },
  {
    "objectID": "tools.html#staging",
    "href": "tools.html#staging",
    "title": "Tools",
    "section": "Staging",
    "text": "Staging\nA commit is a record of what files you have changed since the last time you made a commit. Essentially, you make changes to your repo (for example, adding a file or modifying one) and then tell git to put those files into a commit. Commits make up the essence of your project and allow you to go back to the state of a project at any point.\nSo, how do you tell git which files to put into a commit? This is where the staging environment or index come in. When you make changes to your repo, git notices that a file has changed but won’t do anything with it (like adding it in a commit).\nTo add a file to a commit, you first need to add it to the staging environment. To do this, you can use the git add <filename> command.\nOnce you’ve used the git add command to add all the files you want to the staging environment, you can then tell git to package them into a commit using the git commit command. Note: The staging environment, also called ‘staging’, is the new preferred term for this, but you can also see it referred to as the ‘index’.\n\nAdd files to the staging environment: git add filename.extension\nCheck staging environment for new files: git status"
  },
  {
    "objectID": "tools.html#commit-locally",
    "href": "tools.html#commit-locally",
    "title": "Tools",
    "section": "Commit Locally",
    "text": "Commit Locally\ngit commit -m \"Your message about the commit\""
  },
  {
    "objectID": "tools.html#branches",
    "href": "tools.html#branches",
    "title": "Tools",
    "section": "Branches",
    "text": "Branches\nSay you want to make a new feature but are worried about making changes to the main project while developing the feature. This is where git branches come in.\nBranches allow you to move back and forth between ‘states’ of a project. For instance, if you want to add a new page to your website you can create a new branch just for that page without affecting the main part of the project. Once you’re done with the page, you can merge your changes from your branch into the master branch. When you create a new branch, Git keeps track of which commit your branch ‘branched’ off of, so it knows the history behind all the files.\n\ngit checkout -b <my branch name>\nShow list of branches: git branch"
  },
  {
    "objectID": "tools.html#commit-to-github",
    "href": "tools.html#commit-to-github",
    "title": "Tools",
    "section": "Commit to Github",
    "text": "Commit to Github\n\nCreate new repo on GitHub\ngit remote add origin <url produced on github for new repo>\ngit push -u origin [master/main]"
  },
  {
    "objectID": "tools.html#push-a-branch-to-github",
    "href": "tools.html#push-a-branch-to-github",
    "title": "Tools",
    "section": "Push a Branch to Github",
    "text": "Push a Branch to Github\ngit push origin <my-new-branch>\nYou might be wondering what that “origin” word means in the command above. What happens is that when you clone a remote repository to your local machine, git creates an alias for you. In nearly all cases this alias is called “origin.” It’s essentially shorthand for the remote repository’s URL. So, to push your changes to the remote repository, you could’ve used either the command: git push git@github.com:git/git.git yourbranchname or git push origin yourbranchname"
  },
  {
    "objectID": "tools.html#pull-request",
    "href": "tools.html#pull-request",
    "title": "Tools",
    "section": "Pull Request",
    "text": "Pull Request\nA pull request (or PR) is a way to alert a repo’s owners that you want to make some changes to their code. It allows them to review the code and make sure it looks good before putting your changes on the master branch."
  },
  {
    "objectID": "tools.html#get-changes-on-github",
    "href": "tools.html#get-changes-on-github",
    "title": "Tools",
    "section": "Get Changes on Github",
    "text": "Get Changes on Github\ngit pull origin master\ncheck all new commits: git log"
  },
  {
    "objectID": "tools.html#view-differences",
    "href": "tools.html#view-differences",
    "title": "Tools",
    "section": "View Differences",
    "text": "View Differences\n\nrun: git diff"
  },
  {
    "objectID": "tools.html#remove-a-branch",
    "href": "tools.html#remove-a-branch",
    "title": "Tools",
    "section": "Remove a Branch",
    "text": "Remove a Branch\n\nLocally\ngit branch -d <branch_name>\n\n\nRemote\ngit push <remote_name> --delete <branch_name>"
  },
  {
    "objectID": "tools.html#remove-tracked-filedirectory",
    "href": "tools.html#remove-tracked-filedirectory",
    "title": "Tools",
    "section": "Remove tracked file/directory",
    "text": "Remove tracked file/directory\n\nFile\ngit rm --cached <file>\n\n\nDirectory\ngit rm --cahced -r dir/"
  },
  {
    "objectID": "tools.html#pre-commit",
    "href": "tools.html#pre-commit",
    "title": "Tools",
    "section": "pre-commit",
    "text": "pre-commit\nPlease make sure to install our pre-commit hooks into your Git workflow. Pre-commit will help keep our code clean and make sure we are following best practices.\n\nInstall pre-commit hooks: python -m pre_commit install --install-hooks\nRun hooks on the entire codebase: python -m pre_commit run --all-files\n\nHooks will run on the current commit snapshot when executing a git commit. Pre-commit hooks allow us to check for potential issues and make sure we are applying standards to our code before pushing to GitHub.\nSee an example .pre-commit-config.yml"
  },
  {
    "objectID": "tools.html#export-table-to-csv",
    "href": "tools.html#export-table-to-csv",
    "title": "Tools",
    "section": "Export table to CSV",
    "text": "Export table to CSV\n\n\\copy table TO '<path>' CSV\n\\copy table(col1,col1) TO '<path>' CSV\n\\copy (SELECT...) TO '<path>' CSV"
  },
  {
    "objectID": "tools.html#backup",
    "href": "tools.html#backup",
    "title": "Tools",
    "section": "Backup",
    "text": "Backup\nUse pg_dumpall to backup all databases\n$ pg_dumpall -U postgres > all.sql\nUse pg_dump to backup a database\n$ pg_dump -d mydb -f mydb_backup.sql\n\n  -a   Dump only the data, not the schema\n  -s   Dump only the schema, no data\n  -c   Drop database before recreating\n  -C   Create database before restoring\n  -t   Dump the named table(s) only\n  -F   Format (c: custom, d: directory, t: tar)\n\nUse pg_dump -? to get the full list of options"
  },
  {
    "objectID": "tools.html#restore",
    "href": "tools.html#restore",
    "title": "Tools",
    "section": "Restore",
    "text": "Restore\n\npsql\n$ psql -U user mydb < mydb_backup.sql\n\n\npg_restore\n$ pg_restore -d mydb mydb_backup.sql -c\n\n-U   Specify a database user\n-c   Drop database before recreating\n-C   Create database before restoring\n-e   Exit if an error has encountered\n-F   Format (c: custom, d: directory, t: tar, p: plain text sql(default))\n\nUse pg_restore -? to get the full list of options"
  },
  {
    "objectID": "workouts.html",
    "href": "workouts.html",
    "title": "Workouts",
    "section": "",
    "text": "The following regimen outlines a 4 week lift cycle. The routine is a slightly modified version of this. Non-lift days should be supplemented with active recovery workouts.\n\n\n\n\n\n\n\n\n\n\n\n\nWeek\n\n\n\nSunday\n\n\n\nMonday\n\n\n\nTuesday\n\n\n\nWednesday\n\n\n\nThursday\n\n\n\nFriday\n\n\n\nSaturday\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nBack & Biceps\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nChest & Triceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n2\n\n\n\nCardio\n\n\n\nShoulders & Traps, Run\n\n\n\nChest & Triceps\n\n\n\nBack & Biceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n3\n\n\n\nCardio\n\n\n\nBack & Biceps, Run\n\n\n\nShoulders & Traps\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n4\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nDeadlift\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nPull-Ups\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nLat Pull Downs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nRows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFace Pulls\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBarbell Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\n60 seconds\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n15\n\n\n\n\n\n\n\nSit Ups\n\n\n\n3\n\n\n\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBench Press\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nIncline Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFlys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDips\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nTricep Pushdowns\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nAb Roller\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nSitting Twists\n\n\n\n3\n\n\n\n30\n\n\n\n\n\n\n\nWipers\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBack Squat\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nFront Squat\n\n\n\n4\n\n\n\n6\n\n\n\n\n\n\n\nLunges\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nKettelbell Deadlift\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nLeg Kickbacks\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nGlute Bridges\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nMilitary Press\n\n\n\n4\n\n\n\n12, 8, 5, 3\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nRear Delt Flys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nBarbell Shrugs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nOne Arm Dumbbell Snatch\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nKettlebell Swing\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nFarmers Carry\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nScissor Kicks\n\n\n\n3\n\n\n\n30 seconds\n\n\n\n\n\n\n\nL-sit\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nToe Taps\n\n\n\n3\n\n\n\n20"
  }
]