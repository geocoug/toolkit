{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: PySpark Practice\n",
    "description: Introduction to Apache Spark | PySpark\n",
    "image: \"/static/development/pyspark.jpeg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imK1M524ln3O"
   },
   "source": [
    "This is intended to run on Google Colab\n",
    "\n",
    "Install everything necessary to make spark work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pk1n6zuIe3aC"
   },
   "outputs": [],
   "source": [
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
    "!tar xf spark-3.0.3-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WWenlsQvltBW"
   },
   "source": [
    "Set the paths to the installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JCV3jcRJfSGo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Dl8rQrIlwwM"
   },
   "source": [
    "Find the spark installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "_RUoWAC_lOG1"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Edhpm-Db5CMn"
   },
   "source": [
    "Start doing fancy pyspark stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "9MwBiKLJmgMD"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFDpjbdAktQk"
   },
   "source": [
    "Request Orders.json from google drive via command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "0H__7fMZg8LW"
   },
   "outputs": [],
   "source": [
    "!wget -q --no-check-certificate 'https://drive.google.com/uc?export=download&id=1I6VuRILNtyhnWMUml61Dv58YOP2dqlvx' -O 'Orders.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZ_SH_QYk0BP"
   },
   "source": [
    "Or, do it the Python way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SnaPo4Qfk4ac"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# INPUT_FILE = '/content/drive/MyDrive/Colab Notebooks/Starbucks/1_basic_exercise/resources/Order.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIahcZY3mhXN"
   },
   "source": [
    "Set input/output variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tP3yVJDn2eLE"
   },
   "outputs": [],
   "source": [
    "INPUT_FILE = '/content/Orders.json'  # TODO: Change this based on actual location for your environment setup\n",
    "OUTPUT_CSV_FILE = './output/files/output.csv'\n",
    "OUTPUT_DELTA_PATH = './output/delta/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFWgvMg5nm81"
   },
   "source": [
    "JSON data summary:\n",
    "\n",
    "- Each list element is an event\n",
    "- Events contain a message about an order\n",
    "- Orders contain a list of items that contain attributes about the item\n",
    "- Items can also contain a lists of items (childItems, discounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_C_uemPjmcdS"
   },
   "source": [
    "Create spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "UCiQ_Dpzmtc1"
   },
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"programming\")\n",
    "        .master(\"local\")\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\")\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "        .config('spark.ui.port', '4050')\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0qO4SIUfmqrB"
   },
   "outputs": [],
   "source": [
    "from traitlets.traitlets import default\n",
    "def read_json(file_path: str, schema: StructType) -> DataFrame:\n",
    "    \"\"\"\n",
    "    The goal of this method is to parse the input json data using the schema from another method.\n",
    "\n",
    "    We are only interested in data starting at orderPaid attribute.\n",
    "\n",
    "    :param file_path: Order.json will be provided\n",
    "    :param schema: schema that needs to be passed to this method\n",
    "    :return: Dataframe containing records from Order.json\n",
    "    \"\"\"\n",
    "    # Only interested in data starting at orderPaid\n",
    "    with open(file_path) as f:\n",
    "      js = json.load(f)[0]['data']['message']['orderPaid']\n",
    "    \n",
    "    # Create Dataframe\n",
    "    #   - Use spark JSON method for reading object\n",
    "    #   - Use parallelize on array object, which contains json structured data\n",
    "    #   - Use custom schema so data type/structure is defined instead of inferred\n",
    "    df = spark.read.json(spark.sparkContext.parallelize([js]), \n",
    "                         schema=schema['order_paid_type'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPvm3pDfqp6V"
   },
   "source": [
    "The schema outlined below represents a \"one-to-many\" relationship and is defined in a bottom-up fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uPii-b3-mqxa"
   },
   "outputs": [],
   "source": [
    "def get_struct_type() -> StructType:\n",
    "    \"\"\"\n",
    "    Build a schema based on the the file Order.json\n",
    "\n",
    "    :return: Structype of equivalent JSON schema\n",
    "    \"\"\"\n",
    "    discount_type = StructType([StructField(\"amount\", IntegerType(), True),\n",
    "                                StructField(\"description\", StringType(), True)\n",
    "                                ])\n",
    "\n",
    "    child_item_type = StructType([StructField(\"lineItemNumber\", StringType(), True),\n",
    "                                  StructField(\"itemLabel\", StringType(), True),\n",
    "                                  StructField(\"quantity\", DoubleType(), True),\n",
    "                                  StructField(\"price\", IntegerType(), True),\n",
    "                                  StructField(\"discounts\", ArrayType(discount_type), True), # Changed \"TODO --> ArrayType(discount_type). Will inherit discout_type attributes.\n",
    "                                  ])\n",
    "\n",
    "    item_type = StructType([StructField(\"lineItemNumber\", StringType(), True),\n",
    "                            StructField(\"itemLabel\", StringType(), True),\n",
    "                            StructField(\"quantity\", DoubleType(), True),\n",
    "                            StructField(\"price\", IntegerType(), True),\n",
    "                            StructField(\"discounts\", ArrayType(discount_type), True), # Changed \"TODO\" --> ArrayType(discount_type). Will inherit discount_type attributes.\n",
    "                            StructField(\"childItems\", ArrayType(child_item_type), True), # Changed \"TODO\" --> ArrayType(child_item_type). Will inherit chile_item_type attributes.\n",
    "                            ])\n",
    "\n",
    "    order_paid_type = StructType([StructField(\"orderToken\", StringType(), True),\n",
    "                                  StructField(\"preparation\", StringType(), True),\n",
    "                                  StructField(\"items\", ArrayType(item_type), True), # Changed \"TODO\" --> ArrayType(item_type). Will inherit item_type attributes.\n",
    "                                  ])\n",
    "\n",
    "    message_type = StructType([StructField(\"orderPaid\", order_paid_type, True)]) # Changed \"TODO\" --> order_paid_type. Will inherit order_paid_type attributes.\n",
    "\n",
    "    data_type = StructType([StructField(\"message\", message_type, True)]) # Changed \"TODO\" --> message_type. Will inherit message_type attributes.\n",
    "\n",
    "    body_type = StructType([StructField(\"id\", StringType(), True),\n",
    "                            StructField(\"subject\", StringType(), True),\n",
    "                            StructField(\"data\", data_type, True), # Changed \"TODO\" --> data_type. Will inherit data_type attributes.\n",
    "                            StructField(\"eventTime\", StringType(), True),\n",
    "                            ])\n",
    "    return {'body_type': body_type,\n",
    "            'data_type': data_type,\n",
    "            'message_type': message_type,\n",
    "            'order_paid_type': order_paid_type,\n",
    "            'item_type': item_type,\n",
    "            'child_item_type': child_item_type,\n",
    "            'discount_type': discount_type}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wes57SHSmqza"
   },
   "outputs": [],
   "source": [
    "def get_rows_from_array(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Input data frame contains columns of type array. Identify those columns and convert them to rows.\n",
    "\n",
    "    :param df: Contains column with data type of type array.\n",
    "    :return: The dataframe should not contain any columns of type array\n",
    "    \"\"\"\n",
    "    # explode will create a new row for each element in an array\n",
    "    from pyspark.sql.functions import explode\n",
    "\n",
    "    # Iterate over field names\n",
    "    for i, f in enumerate(df.schema.fields):\n",
    "      # Check datatype of field\n",
    "      if isinstance(f.dataType, ArrayType):\n",
    "        arrayCol = f.name\n",
    "\n",
    "    # Overwrite dataframe object\n",
    "    # Create a new row for every element in \"arrayCol\".\n",
    "    # Each new row will contain the same values from \n",
    "    #   columns that are not \"arrayCol\".\n",
    "    # Use \"withColumn()\" to transform dataframe. \n",
    "    #   First argument - What column will be transformed (will overwrite because already exists)\n",
    "    #   Second argument - Expression to create/modify values for the column\n",
    "    df = df.withColumn(arrayCol, explode(arrayCol))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SEj15kUbmq1q"
   },
   "outputs": [],
   "source": [
    "def get_unwrapped_nested_structure(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Convert columns that contain multiple attributes to columns of their own\n",
    "\n",
    "    :param df: Contains columns that have multiple attributes\n",
    "    :return: Dataframe should not contain any nested structures\n",
    "    \"\"\"\n",
    "\n",
    "    def parse_struct(df):\n",
    "      \"\"\"Create an array of columns to be selected.\n",
    "         If column type = StructType, select all attributes\n",
    "         in that StructType as additional columns.\n",
    "         Returns list of columns.\"\"\"\n",
    "      cols = []\n",
    "      for i, d in enumerate(df.schema.fields):\n",
    "        if isinstance(d.dataType, StructType):\n",
    "          cols.append(f\"{d.name}.*\")\n",
    "        else:\n",
    "          cols.append(d.name)\n",
    "      return cols\n",
    "\n",
    "    df = df.select(parse_struct(df))\n",
    "\n",
    "    # Check for columns of type Array.\n",
    "    # If type Array, transform elements to rows\n",
    "    arrayCols = [c.name for c in df.schema.fields if isinstance(c.dataType, ArrayType)]\n",
    "    if len(arrayCols) > 0:\n",
    "      for col in arrayCols:\n",
    "        df = get_rows_from_array(df)\n",
    "\n",
    "    # Could have multiple instances of key names \n",
    "    # Will have to add columns manually\n",
    "    # If unique names, could probably reuse \"parse_struct()\"\n",
    "    df = df.withColumn(\"discountAmount\", df.discounts.amount) \\\n",
    "      .withColumn(\"discountDescription\", df.discounts.description)\n",
    "    df = df.drop(\"discounts\")\n",
    "\n",
    "    df = df.withColumn(\"childItemLineNumber\", df.childItems.lineItemNumber) \\\n",
    "      .withColumn(\"childItemLabel\", df.childItems.itemLabel) \\\n",
    "      .withColumn(\"childItemQuantity\", df.childItems.quantity) \\\n",
    "      .withColumn(\"childItemPrice\", df.childItems.price) \\\n",
    "      .withColumn(\"childItemDiscounts\", df.childItems.discounts)\n",
    "    df = df.drop(\"childItems\")\n",
    "\n",
    "    df = get_rows_from_array(df)\n",
    "    df = df.withColumn(\"childItemDiscountAmount\", df.childItemDiscounts.amount) \\\n",
    "      .withColumn(\"childItemDiscountDescription\", df.childItemDiscounts.description)\n",
    "    df = df.drop(\"childItemDiscounts\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gT-dpY1ymq32"
   },
   "outputs": [],
   "source": [
    "def write_df_as_csv(df: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Write the data frame to a local destination of your choice with headers\n",
    "\n",
    "    :param df: Contains flattened order data\n",
    "    \"\"\"\n",
    "    df.write.format(\"csv\") \\\n",
    "      .mode('overwrite') \\\n",
    "      .option(\"header\", \"true\") \\\n",
    "      .save(OUTPUT_CSV_FILE)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iDbq9SHdmq52"
   },
   "outputs": [],
   "source": [
    "def create_delta_table(spark: SparkSession) -> None:\n",
    "    spark.sql('CREATE DATABASE IF NOT EXISTS EXERCISE')\n",
    "\n",
    "    spark.sql('''\n",
    "    CREATE TABLE IF NOT EXISTS EXERCISE.ORDERS(\n",
    "        OrderToken String,\n",
    "        Preparation  String,\n",
    "        ItemLineNumber String,\n",
    "        ItemLabel String,\n",
    "        ItemQuantity Double,\n",
    "        ItemPrice Integer,\n",
    "        ItemDiscountAmount Integer,\n",
    "        ItemDiscountDescription String,\n",
    "        ChildItemLineNumber String, \n",
    "        ChildItemLabel String,\n",
    "        ChildItemQuantity Double,\n",
    "        ChildItemPrice Integer,\n",
    "        ChildItemDiscountAmount Integer,\n",
    "        ChildItemDiscountDescription String\n",
    "    ) USING DELTA\n",
    "    LOCATION \"{0}\"\n",
    "    '''.format(OUTPUT_DELTA_PATH))\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyO1onsK9wEI"
   },
   "source": [
    "Haven't used Delta before. Reference material: https://docs.microsoft.com/en-us/azure/databricks/delta/quick-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2e09P2hdmq7-"
   },
   "outputs": [],
   "source": [
    "def write_df_as_delta(df: DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Write the dataframe output to the table created, overwrite mode can be used\n",
    "\n",
    "    :param df: flattened data\n",
    "    :return: Data from the orders table\n",
    "    \"\"\"\n",
    "    # Rename columns to match delta table schema\n",
    "    df = df.withColumnRenamed(\"orderToken\", \"OrderToken\") \\\n",
    "      .withColumnRenamed(\"preparation\", \"Rreparation\") \\\n",
    "      .withColumnRenamed(\"lineItemNumber\", \"LineItemNumber\") \\\n",
    "      .withColumnRenamed(\"itemLabel\", \"ItemLabel\") \\\n",
    "      .withColumnRenamed(\"quantity\", \"Quantity\") \\\n",
    "      .withColumnRenamed(\"price\", \"Price\") \\\n",
    "      .withColumnRenamed(\"discountAmount\", \"DiscountAmount\") \\\n",
    "      .withColumnRenamed(\"discountDescription\", \"DiscountDescription\") \\\n",
    "      .withColumnRenamed(\"childItemLineNumber\", \"ChildItemLineNumber\") \\\n",
    "      .withColumnRenamed(\"childItemLabel\", \"ChildItemLabel\") \\\n",
    "      .withColumnRenamed(\"childItemQuantity\", \"ChildItemQuantity\") \\\n",
    "      .withColumnRenamed(\"childItemPrice\", \"ChildItemPrice\") \\\n",
    "      .withColumnRenamed(\"childItemDiscountAmount\", \"ChildItemDiscountAmount\") \\\n",
    "      .withColumnRenamed(\"childItemDiscountDescription\", \"ChildItemDiscountDescription\") \n",
    "\n",
    "    df.write.insertInto(\"EXERCISE.ORDERS\", overwrite = True)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ZdMR7P0bm_yw"
   },
   "outputs": [],
   "source": [
    "def read_data_delta(spark: SparkSession) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read data from the table created\n",
    "    \n",
    "    :param spark:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return spark.sql(\"select * from exercise.orders;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZFrQ_ConMzP",
    "outputId": "6c3032c0-ad9f-4b45-e408-44cea0aff62e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+------------------+--------------+---------+------------+---------+------------------+-----------------------+-------------------+--------------+-----------------+--------------+-----------------------+----------------------------+\n",
      "|OrderToken             |Preparation       |ItemLineNumber|ItemLabel|ItemQuantity|ItemPrice|ItemDiscountAmount|ItemDiscountDescription|ChildItemLineNumber|ChildItemLabel|ChildItemQuantity|ChildItemPrice|ChildItemDiscountAmount|ChildItemDiscountDescription|\n",
      "+-----------------------+------------------+--------------+---------+------------+---------+------------------+-----------------------+-------------------+--------------+-----------------+--------------+-----------------------+----------------------------+\n",
      "|97331549875122744335422|Magic happens here|1             |COFFEE   |1.0         |345      |495               |Item 1, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 1, Discount 1    |\n",
      "|97331549875122744335422|Magic happens here|2             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n",
      "|97331549875122744335422|Magic happens here|3             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n",
      "|97331549875122744335422|Magic happens here|4             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n",
      "|97331549875122744335422|Magic happens here|5             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n",
      "|97331549875122744335422|Magic happens here|6             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n",
      "|97331549875122744335422|Magic happens here|7             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n",
      "|97331549875122744335422|Magic happens here|8             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n",
      "|97331549875122744335422|Magic happens here|9             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n",
      "|97331549875122744335422|Magic happens here|10            |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n",
      "+-----------------------+------------------+--------------+---------+------------+---------+------------------+-----------------------+-------------------+--------------+-----------------+--------------+-----------------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    input_schema = get_struct_type()\n",
    "\n",
    "    input_df = read_json(INPUT_FILE, input_schema)\n",
    "\n",
    "    arrays_to_rows_df = get_rows_from_array(input_df)\n",
    "\n",
    "    unwrap_struct_df = get_unwrapped_nested_structure(arrays_to_rows_df)\n",
    "\n",
    "    write_df_as_csv(unwrap_struct_df)\n",
    "\n",
    "    create_delta_table(spark)\n",
    "    write_df_as_delta(unwrap_struct_df)\n",
    "\n",
    "    result_df = read_data_delta(spark)\n",
    "    result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84um1nE47pBA"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "basic.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
