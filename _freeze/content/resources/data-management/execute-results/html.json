{
  "hash": "ca16cf24ef33739ce07a8eb9284eb610",
  "result": {
    "markdown": "---\ntitle: \"Data Management\"\ndescription: \"Data management guidelines and resources\"\nimage: /static/resources/datamanagement.png\n---\n\n## Data Quality\n\n[EPA Guidance on Systematic Planning Using the Data Quality Objectives Process](https://www.epa.gov/sites/default/files/2015-06/documents/g4-final.pdf)\n\n### The Data Quality Objective (DQO) Process\n\n![](../../static/resources/DQO_Process.png)\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\n\ndqo = \"../../static/resources/DQO.xlsx\"\nelements = pd.read_excel(dqo, sheet_name=0)\nfactors = pd.read_excel(dqo, sheet_name=1)\n```\n:::\n\n\n### Table 1. Elements of Systematic Planning\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nelements.to_html(index=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th>Element</th>\\n      <th>Description</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Organization</td>\\n      <td>Identification and involvement of the project manager, sponsoring organization and responsible official, project personnel, stakeholders, scientific experts, etc. (e.g., all customers and suppliers).</td>\\n    </tr>\\n    <tr>\\n      <td>Project Goal</td>\\n      <td>Description of the project goal, objectives, and study questions and issues.</td>\\n    </tr>\\n    <tr>\\n      <td>Schedule</td>\\n      <td>Identification of project schedule, resources (including budget), milestones, and any applicable requirements (e.g., regulatory requirements, contractual requirements).</td>\\n    </tr>\\n    <tr>\\n      <td>Data Needs</td>\\n      <td>Identification of the type of data needed and how the data will be used to support the project’s objectives.</td>\\n    </tr>\\n    <tr>\\n      <td>Criteria</td>\\n      <td>Determination of the quantity of data needed and specification of performance criteria for measuring quality.</td>\\n    </tr>\\n    <tr>\\n      <td>Data Collection</td>\\n      <td>Description of how and where the data will be obtained (including existing data) and identification of any constraints on data collection.</td>\\n    </tr>\\n    <tr>\\n      <td>Quality Assurance (QA)</td>\\n      <td>Specification of needed QA and quality control (QC) activities to assess the quality performance criteria (e.g., QC samples for both field and laboratory, audits, technical assessments, performance evaluations, etc.).</td>\\n    </tr>\\n    <tr>\\n      <td>Analysis</td>\\n      <td>Description of how the acquired data will be analyzed (either in the field or the laboratory), evaluated (i.e., QA review/verification/validation), and assessed against its intended use and the quality performance criteria.</td>\\n    </tr>\\n  </tbody>\\n</table>\n:::\n:::\n\n\n> When specifying the project goal (element #2 in Table 1), a key activity is to determine the key questions which the study will address once data and information are properly collected and analyzed.\n\n### Table 2. EPA General Assessment Factors\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nfactors.to_html(index=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th>Factor</th>\\n      <th>Description</th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <td>Soundness</td>\\n      <td>The extent to which the scientific and technical procedures, measures, methods or models employed to generate the information are reasonable for, and consistent with, the intended application.</td>\\n    </tr>\\n    <tr>\\n      <td>Applicability and Utility</td>\\n      <td>The extent to which the information is relevant for the Agency’s intended use.</td>\\n    </tr>\\n    <tr>\\n      <td>Clarity and Completeness</td>\\n      <td>The degree of clarity and completeness with which the data, assumptions, methods, quality assurance, sponsoring organizations and analyses employed to generate the information are documented.</td>\\n    </tr>\\n    <tr>\\n      <td>Uncertainty and Variability</td>\\n      <td>The extent to which the variability and uncertainty (quantitative and qualitative) in the information or the procedures, measures, methods or models are evaluated and characterized.</td>\\n    </tr>\\n    <tr>\\n      <td>Evaluation and Review</td>\\n      <td>The extent of independent verification, validation, and peer review of the information or of the procedures, measures, methods or models.</td>\\n    </tr>\\n  </tbody>\\n</table>\n:::\n:::\n\n\n### Systematic Planning & GAF Commonalities\n\n- Achieving clarity in a project’s development becomes straightforward when using systematic planning, as almost every element of the planning process contributes to understanding how the project’s assumptions, methods, and proposed analyses will be conducted.\n- Planning for analyzing the data and information before collection clearly meets the intent of the GAFs.\n- Clear statements on the goals of the project developed through systematic planning leads to a better understanding of purpose and credibility of the results.\n- Systematic planning leads to a clear statement of information needs and how the information will be collected, and leads to transparency in data quality.\n\n![](../../static/resources/Commonalities.png)\n\n![](../../static/resources/Project_Life_Cycle.png)\n\n## Benefits of Using the DQO Process\n\n> The interaction amongst a multidisplinary team results in a clear understanding of the problem and the options available. Organizations that have used the DQO Process have found the structured format facilitated good communicaitons, documentation, and data collection design, all of which facilitated rapid peer review and approval.\n\n- The structure of the DQO Process provides a convenient way to document activities and decisions and to communicate the data collection design to others.\n- The DQO Process is an effective planning tool that can save resources by making data collection operations more resource-effective.\n- The DQO Process enables data users and technical experts to participate collectively in planning and to specify their needs prior to data collection. The DQO Process helps to focus studies by encouraging data users to clarify vague objectives and document clearly how scientific theory motivating this project is applicable to the intended use of the data.\n- The DQO Process provides a method for defining performance requirements appropriate for the intended use of the data by considering the consequences of drawing incorrect conclusions and then placing tolerable limits on them.\n- The DQO Process encourages good documentation for a model-based approach to investigate the objectives of a project, with discussion on how the key parameters were estimated or derived, and the robustness of the model to small perturbations. Upon implementing the DQO Process, your environmental programs can be strengthened\n\n## Data Handling\n\n- [execSQL](https://execsql.osdn.io/)\n- [PostgreSQL Tutorial](https://www.postgresqltutorial.com/)\n- [PostgreSQL Tips](https://pgdash.io/blog/postgres-tips-and-tricks.html?)\n- [DevDocs.io](https://devdocs.io/)\n\n### Character Encoding\n\nWe deal with character encoding issues when importing data to databases all the time.  All databases are encoded in UTF-8, and the most common non-ASCII, non-UTF-8 format is Microsoft's custom encoding, which is CP-1252 (also known as win-1252 and a few other things).  The character encoding of a file can't necessarily be definitively determined by examination, but the Linux command-line tool file generally does a good job.  If you want to change the encoding of a file rather than importing it in a known format, the Linux command-line tool iconv will do that for you.  There's a Python library on PyPI named chardet that will also diagnose file encodings.\n\nFor data managers, the workflow is to first guess that encoding errors on data import are due to the file being cp-1252.  That covers about 90% of cases.  Our import tool also automatically diagnoses instances where a file starts with a byte order mark (BOM), which covers most of the rest of the cases.  For the remainders, Geany is usually the quickest way to check the file encoding.\n\nEverything that comes out of our databases is always in UTF-8, so I, at least, don't ordinarily have encoding issues when importing data to R.  For those who use data from other sources, it is a good idea to document a recommended workflow and set of tools.\n\n## Analytical Chemistry\n\n### Summarization\n\nChemistry data frequently is summarized for use in analyses or for presentation using tables or maps. Summarization is ordinarily performed when there are multiple concentration values measured for a sample, or for a specific location, date, and depth. Multiple concentration values result from field or laboratory replications, from field splits created for quality control evaluations, and sometimes from sample reanalyses. Although field splits and laboratory replicates are created to support data quality assessments, all of the valid results that are produced are informative, are ordinarily stored in the project database, and are used to produce the most accurate possible estimate of the true concentration in a sample. When there are replicate results for a sample, the data will be averaged in a stepwise, or hierarchical, fashion. Because each level of the hierarchy represents a different source of variation, all the results at a single level are averaged together before results are averaged across levels. The different levels of replication, and the source of variation that each represents, are as follows:\n\n1. Average across lab replicates\n2. Average across multiple analyses (if they exist) for the same lab sample. This will average across multiple methods and reanalyses (different SDGs)\n3. Average across multiple lab samples (if they exist) for the same sample number (split) and lab.  Multiple lab samples may exist for the same sample number if the laboratory has created new laboratory sample IDs for reanalyses.\n4. Average across multiple splits (if they exist) for the same lab and interpretive sample. Multiple sample numbers (splits) may exist for the same lab, or the same sample  number may exist for different labs. Splits sent to different labs potentially have an additional source of variability in addition to variability between splits sent to the same lab.\n5. Average across laboratories for the same main sample. This summarizes data by main sample (sample_id), which is typically the lowest level of data summarization to be used for data interpretation.\n\nBy default, data are summarized by successive averaging across these levels of replication, in the order given above. During the averaging process, data validation qualifiers and significant digits must be propagated. The rules for propagating the data validation qualifiers U (undetected), J (estimated), and R (rejected) are as follows:\n\n- If both detected and undetected data are to be averaged, then undetected data lower than the highest detected value will be taken at one-half the detection limit and averaged with the detected data, and the result will be identified as detected. Non-detects that are higher than the highest detected value will be omitted from the average.\n- If all data to be averaged are undetected, the result will be taken to be the lowest detection limit, and will be identified as undetected.\n- If J-qualified data are averaged with non-J-qualified data, the result will be J-qualified.\n- If R-qualified data are averaged with non-R-qualified data, the result will be R-qualified.\n\nSignificant digits are propagated so that the place (in the sense of one's place, ten's place, etc.) of the least significant digit of the average is equal to the highest place of the least significant digit of any of the values that are averaged.\n\nThese rules are built into custom aggregate functions in IDB that use the measurement_result data type.\n\nThese default data handling rules will be applied if no project-specific alternate rules are specified. The project manager, project technical staff, and data manager should evaluate, at the start of a project, whether an alternative approach is needed. Alternate data summarization rules should be summarized in the project plan or in the data management plan, if it exists.  (Data managers: if not documented elsewhere, record this information in the Data Manager's Manual.)\n\nNote that the handling of nondetects during hierarchical averaging and the presentation of nondetects in data summaries may be different.  Regardless of whether nondetects are taken at half the detection limit or the full detection limit when averaging, the summarized result may be presented with either the full detection limit or half the detection limit.  Reporting nondetects at the full detection limit should ordinarily be done when preparing data tables for reports or other deliverables.  Data analyses to be conducted by Integral may be carried out using either half or full detection limits.  The method of reporting nondetects should be specified when requesting data summaries.\n\n## Chemistry\n\n### Resources\n\n- [Hazardous Waste Test Methods](https://www.epa.gov/hw-sw846/sw-846-compendium)\n- [National Environmental Methods Index](https://www.nemi.gov/home/)\n- [Substance Registry Service](https://iaspub.epa.gov/sor_internet/registry/substreg/LandingPage.do)\n- [EIM Valid Values](https://apps.ecology.wa.gov/eim/help/ValidValues)\n- [Verification and Validation](https://www.epa.gov/sites/production/files/2015-06/documents/g8-final.pdf)\n- [Qualifiers](https://apps.ecology.wa.gov/eim/help/ValidValues/DataQualifiers)\n- [Data Review](https://www.epa.gov/clp/superfund-clp-national-functional-guidelines-data-review)\n- [Chemical Lists](../../static/resources/Chemical_Lists.xls)\n- [PCBs](https://www.epa.gov/pcbs/learn-about-polychlorinated-biphenyls-pcbs)\n- [Washington Water Resources Data Defs](https://pubs.usgs.gov/wdr/WDR-WA-03-1/pdf/ADR_N.pdf)\n- [Measurement Basis Conversions](../../static/resources/EFH-REFS_CHPT10.pdf)\n- [https://pubs.usgs.gov/wdr/WDR-WA-03-1/pdf/ADR_N.pdf](https://pubs.usgs.gov/wdr/WDR-WA-03-1/pdf/ADR_N.pdf)\n- [http://www.eccsmobilelab.com/resources/literature/?Id=117](http://www.eccsmobilelab.com/resources/literature/?Id=117)\n- [Conversions](../../static/resources/EFH-REFS_CHPT10.pdf)\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport pandas as pd\n\nwb = \"../../static/resources/Chemical_Lists.xls\"\n\nchem_lists = pd.read_excel(wb, sheet_name=0)\ncwa = pd.read_excel(wb, sheet_name=1)\npfas = pd.read_excel(wb, sheet_name=2)\npesticides = pd.read_excel(wb, sheet_name=3)\npcb = pd.read_excel(wb, sheet_name=4)\npah = pd.read_excel(wb, sheet_name=5)\nfertilizers = pd.read_excel(wb, sheet_name=6)\ndioxfuran = pd.read_excel(wb, sheet_name=7)\npbde = pd.read_excel(wb, sheet_name=8)\n```\n:::\n\n\n### TEFs\n\n- [Van den Berg source document](../../static/resources/TEFs Van den Berg.pdf)\n- [Van den Berg TEF table](../../static/resources/TEFs_VanDenBerg.xlsx)\n\n### Chemical Groups\n\n#### Dioxin & Furans\n\n[Reference](https://www.eurofins.com/media/312559/dioxine_ts_eng.pdf)\n\n#### PCBs\n\n[Learn about PCBs](https://www.epa.gov/pcbs/learn-about-polychlorinated-biphenyls-pcbs)\n\n##### General\n\nPCBs are a group of man-made organic chemicals consisting of carbon, hydrogen and chlorine atoms. The number of chlorine atoms and their location in a PCB molecule determine many of its physical and chemical properties. PCBs have no known taste or smell, and range in consistency from an oil to a waxy solid.\n\nPCBs belong to a broad family of man-made organic chemicals known as chlorinated hydrocarbons. PCBs were domestically manufactured from 1929 until manufacturing was banned in 1979. They have a range of toxicity and vary in consistency from thin, light-colored liquids to yellow or black waxy solids. Due to their non-flammability, chemical stability, high boiling point and electrical insulating properties, PCBs were used in hundreds of industrial and commercial applications including:\n\n- Electrical, heat transfer and hydraulic equipment\n- Plasticizers in paints, plastics and rubber products\n- Pigments, dyes and carbonless copy paper\n- Other industrial applications\n\n**Commercial Uses for PCBs**\n\nAlthough no longer commercially produced in the United States, PCBs may be present in products and materials produced before the 1979 PCB ban. Products that may contain PCBs include:\n\n- Transformers and capacitors\n- Electrical equipment including voltage regulators, switches, re-closers, bushings, and electromagnets\n- Oil used in motors and hydraulic systems\n- Old electrical devices or appliances containing PCB capacitors\n- Fluorescent light ballasts\n- Cable insulation\n- Thermal insulation material including fiberglass, felt, foam, and cork\n- Adhesives and tapes\n- Oil-based paint\n- Caulking\n- Plastics\n- Carbonless copy paper\n- Floor finish\n\nThe PCBs used in these products were chemical mixtures made up of a variety of individual chlorinated biphenyl components known as congeners. Most commercial PCB mixtures are known in the United States by their industrial trade names, the most common being Arochlor.\n\n**Release and Exposure of PCBs**\n\nToday, PCBs can still be released into the environment from:\n\n- Poorly maintained hazardous waste sites that contain PCBs\n- Illegal or improper dumping of PCB wastes\n- Leaks or releases from electrical transformers containing PCBs\n- Disposal of PCB-containing consumer products into municipal or other landfills not designed to handle hazardous waste\n- Burning some wastes in municipal and industrial incinerators\n\nPCBs do not readily break down once in the environment. They can remain for long periods cycling between air, water and soil. PCBs can be carried long distances and have been found in snow and sea water in areas far from where they were released into the environment. As a consequence, they are found all over the world. In general, the lighter the form of PCB, the further it can be transported from the source of contamination.\n\nPCBs can accumulate in the leaves and above-ground parts of plants and food crops. They are also taken up into the bodies of small organisms and fish. As a result, people who ingest fish may be exposed to PCBs that have bioaccumulated in the fish they are ingesting.\n\nThe National Center for Health Statistics, a division of the Centers for Disease Control and Prevention, conducts the National Health and Nutrition Examination Surveys (NHANES). NHANES is a series of U.S. national surveys on the health and nutrition status of the noninstitutionalized civilian population, which includes data collection on selected chemicals. Interviews and physical examinations are conducted with approximately 10,000 people in each two-year survey cycle. PCBs are one of the chemicals where data are available from the NHANES surveys.\n\n##### PCB Congeners\n\nA PCB congener is any single, unique well-defined chemical compound in the PCB category. The name of a congener specifies the total number of chlorine substituents, and the position of each chlorine. For example: 4,4'-Dichlorobiphenyl is a congener comprising the biphenyl structure with two chlorine substituents - one on each of the #4 carbons of the two rings. In 1980, a numbering system was developed which assigned a sequential number to each of the 209 PCB congeners.\n\n##### PCB Homologs\n\nHomologs are subcategories of PCB congeners that have equal numbers of chlorine substituents. For example, the tetrachlorobiphenyls are all PCB congeners with exactly 4 chlorine substituents that can be in any arrangement.\n\n##### PCB Aroclor\n\nAroclor is a PCB mixture produced from approximately 1930 to 1979. It is one of the most commonly known trade names for PCB mixtures. There are many types of Aroclors and each has a distinguishing suffix number that indicates the degree of chlorination. The numbering standard for the different Aroclors is as follows:\n\n- The first two digits usually refer to the number of carbon atoms in the phenyl rings (for PCBs this is 12)\n- The second two numbers indicate the percentage of chlorine by mass in the mixture. For example, the name Aroclor 1254 means that the mixture contains approximately 54% chlorine by weight.\n\n### Qualifiers\n\n> Labs may apply whatever flags they want to a result.  Some data qualifiers are defined by EPA’s Functional Guidelines documents, which describe how data validation is to be conducted, and the use and interpretation of U, J, and R qualifiers is pretty universal (but older standards for Puget Sound data used E instead of J).  Because the U, J, and R qualifiers are pretty universal and have implications for data usability, they are the only ones that are represented as Boolean fields in the meas_value column.  All lab flags are put into the lab_flags column, and there is no lookup table for them, and there is no defined use for them.  Similarly, the validator qualifiers (U, J, R, and possibly others) are put in the validator_flags column.  If any of those three common qualifiers is in the validator_flags column, then the corresponding flags in meas_value should be set.  The lab_conc_qual column is something of a relic, left over from the days when data were commonly provided in EPA’s Contract Laboratory Program (CLP) data format, which had a corresponding column.  The lab_conc_qual column was meant to either contain “U” or be null.  We don’t ordinarily use that column any more.  Of the qualifiers you listed above, other than U, J, and R, N is commonly used to flag a tentatively identified compound, which means that the analyte code itself is uncertain.  The d_labresult.tic column is meant to hold that information.  The tic column is not part of the measurement_result data type because it is not used in any way during data aggregation (averaging or summing).  I see that Jerry added other flags and qualifiers to the e_concqual table, but needn’t—really shouldn’t—be there.  The e_concqual dictionary should have only “U” defined.  It may seem odd to define a lookup table for only one value when a check constraint on the concentration qualifier columns could be used instead, but it’s easier to check relational constraints than to check check constraints programmatically.\n\n### Duplicates\n\n“Duplicate” is a somewhat ambiguous term, but in practice it most commonly refers to field duplicates, which we ordinarily refer to as splits to avoid that ambiguity.  Some QC data, particularly spikes, are frequently duplicated, so when we have lab QC data we may have values for spikes and spike duplicates.  When we receive lab results in one big flat table that includes both analytical results for natural samples and results of lab QC samples, the word or code “DUP” in a column header or table cell could mean a couple of different things.  Without seeing the original data source, I’m not sure where the “DUP” code in the “labqc_samp” column of your “d_labsample” table came from.  I’m going to assume that it refers to a field duplicate, and not a spike duplicate.\n\nIdeally, samples are submitted to the laboratory “blind” so that the laboratory does not know which field samples are duplicates of one another.  This is to prevent them from seeing that there’s a lot of variation between some pair of duplicates and deciding to re-run one or both of them.  If the lab is producing highly variable data, we don’t want them to be able to hide it.  Unfortunately, many field sampling programs use a suffix of “-D” or “-DUP” or something like that on the sample ID, so the lab knows which samples are field duplicates.  If they know, they may pass that information back in their EDD.\n\nAlthough field duplicates are used as a QC check on laboratory performance, they are not lab QC samples themselves.  They are just normal field samples (which have been split), and don’t need to have a laboratory QC sample ID assigned to them.  Thus, field duplicates should not be listed in the “d_labqcsamp” table, so that table looks fine as it appears below.  The same is true for the “d_labresult” table.\n\nThere are a couple of things to be changed about the “d_labsample” table as shown below:\n\n- The values in the “labqc_samp” column should be identifiers that appear in the “d_labqcsamp” table, not codes.  The codes for the lab QC type should be in the d_labqcsamp.qc_type column, and neither “Natural” nor “DUP” should be used there.\n- The “d_labsample” table should have values in the “study_id” and “sample_no” columns, or a value in the “labqc_samp” column, but not both.  There are other invalid combinations of columns also.  The “d_labsample” table may have any one of the following tables as a parent: d_sampsplit, d_fldqcsplit, d_labqcsamp, d_bioaccum_samp, d_samptreatsplit, or d_bioasrepsamp.  The “ck_one_sample” check constraint on the table enforces this rule.  Check constraints like this are not run by the upsert scripts, so a set of staged data may pass all the checks performed by the upsert script and yet the INSERT into d_labsample will fail.\n\n### Measurement Basis\n\n[OrgMassSpecR](https://www.rdocumentation.org/packages/OrgMassSpecR/versions/0.5-3/topics/ConvertConcentration)\n\n- Data for soil and sediment are almost always reported on a dry-weight basis.  If there’s anything to indicate a different basis, that deserves a closer look.  Almost the only legitimate reason for a different basis for soil or sediment samples is when a leaching procedure has been applied (e.g., the Toxicity Characteristic Leaching Procedure, or TCLP); in those cases the data may be reported as the concentration in the leachate, so the basis may be “Wet” or “Whole” or “Unfiltered” – anything indicating an unfractionated liquid sample.\n- Data for tissues should ordinarily be reported in wet weight.  Organisms’ homeostasis means that they maintain a nearly constant moisture content in their tissues, whereas the same is not true of materials like soil or sediment.  If tissue data are reported in dry weight, check it carefully: labs can be sloppy about that.\n- Water data are where things can be complex, because often water samples are filtered or centrifuged to remove particulates, which results in the water samples have a ‘dissolved’ basis.  If the particulates are analyzed, and the results are then expressed in terms of the volume of the original sample, then the data will have a ‘particulate’ basis.  Unfiltered, or whole, water, should have a basis of ‘Unfilt’, ‘Whole’, or sometimes ‘Wet’.  Either of the first two of these are preferred, “Wet” is better used as a counterpart to “Dry” for soil, sediment, or tissue samples.\n- There are variations in the way things have been done in different databases.  You may find that the measurement basis code for whole water samples differs from one to another, as in the third bullet above.\n- IDB v.8 now has the “fraction” code, which is intended to be used to distinguish dissolved, particulate, and whole fractions of a water sample.  In IDB v.8, the measurement basis for water samples will almost always be ‘Whole’.  Sometimes sediment or soil samples are fractionated too, e.g., by sieving, and the fraction code should be used in those cases too, so the measurement basis will always be ‘dry’ in those cases.\n- There is an implicit association between measurement bases and units.  For example, if the measurement basis is “Dry”, the units should not be “mg/L” because “…/L” implies a liquid, not a solid.\n- The measurement basis refers to the form of the sample material, which is represented in the denominator of concentration units.  So codes of “Sediment” “Arsenic”, “mg/kg”, “Dry” should be read as “mg of arsenic per kg of dry sediment.”\n\n#### Wet Weight\n\n> Wet weight (or as-is) basis means no calculation has been made to compensate for the moisture content of a sample. Wet weight refers to the weight of animal tissue or other substance including its contained water. (See also “Dry weight”)\n\n#### Dry Weight\n\n> Dry weight basis means the lab has measured moisture content of a sample and calculated concentrations based on the percent solids present. Dry weight refers to the weight of animal tissue after it has been dried in an oven at 65°C until a constant weight is achieved. Dry weight represents total organic and inorganic matter in the tissue. (See also “Wet weight”).\n\n#### Lipid\n\n> Lipid is any one of a family of compounds that are insoluble in water and that make up one of the principal components of living cells. Lipids include fats, oils, waxes, and steroids. Many environmental contaminants such as organochlorine pesticides are lipophilic.\n\n---\n\n### Conversions\n\n#### Wet to Dry\n\n$$DryWt = \\frac{WetWt}{Percent Solids} * 100$$\n\n#### Dry to Wet\n\n$$WetWt = DryWt * \\frac{PercentSolids}{100}$$\n\n#### Organic Carbon Normalization\n\n$$OCnorm = \\frac{DryWt}{\\frac{PercentTOC}{100}}$$\n\n**DryWt & WetWt** = concentration\n\n**PercentSolids** = percentage (no decimal)\n\n#### Resource\n\n![Calcs](../../static/resources/mass_conv.png){fig-align=\"center\"}\n\n---\n\n### Analytical Blanks\n\n### Trip Blank\n\nThe trip blank is designed to identify levels of contamination from the exposure of the reagent or sorbent bed to the same atmospheres exposed to the analyte reagent or sorbent bed.  The trip blank is prepared in the laboratory with the other reagents or adsorbents prior to shipping to the field.  However, the trip blank is never exposed to the field atmospheres.  It is simply sent along with the field samples to and from the site.  The trip blank identified areas of exposure such as shipping temperatures and pressures, laboratory preparation of field samples and laboratory preparation of field samples for analysis.\n\n#### Field Blank\n\nThe field blank is similar to the trip blank in that it is also prepared during the preparation of the field reagents or adsorbents.  However, the field blank is exposed to the same atmospheres in the field as the field samples.  This means that the field blank is opened during the charging of impingers or sorbents in the sample train.  The field blank is also exposed during the exchanging of cartridges in SW-846, Method 0030 or when field reagents are being exchanged during a test run.  In summary, field blanks consist of additional sample collection media (e.g., sorbent tubes, reagents, filters) which are transported to the monitoring site, exposed briefly at the site when the samples are exposed (but no stack gas is actually pulled through these blanks), and transported back to the laboratory for analysis, similar to a field sample.  At least one field blank should be collected and analyzed for each test series.\n\n#### Laboratory Blank\n\nThe laboratory blank is a sample of the reagents or sorbents used during the sample train reagent preparation or recovery.  The laboratory blank is a sample of the extraction solvent, the rinses used during sample recovery, or a sample from the batch of sorbent used to preparing sampling cartridges.  Laboratory blanks include both method blanks and instrument blanks.  Method blanks are carried through all steps of the measurement process (from extraction through analysis).  A method blank is typically analyzed with each sample batch.  Instrument blanks are used to demonstrate that an instrument system is free of contamination.  Instrument blanks are typically analyzed prior to sample analysis and following the analysis of highly contaminated samples.\n\n#### Reagent Blank\n\nThe reagent blank is a sample of the solvents used during recovery of the sample train after the test is completed.  You recall, reagent blanks for both multi-metal and chromium +6 require that the reagent blank be the same volume as the renses used to recover the samples, from probe to impinger.  This is because the blank value is substracted from the sample to obtain a final concentration.\n\n#### Diagram\n\n![](../../static/resources/blanks.PNG)\n\n### Detection Limits\n\n[Presentation](../../static/resources/DLs_Demystified.pptx)\n\n#### What affects detection limits?\n\n- Sample size\n- Concentration of other constituents\n- Sample clean-up\n- Methodology\n- Lab Performance\n  - Experience\n  - Extraction technique\n  - Instrument type and maintenance\n\n**detection_limit** - the lowest possible value an instrument/method can sense a compound is present (think of it like a whisper - you can barely hear it, but know its there). This is better known as the \"method detection limit\"\n\n**quantification_limit** - the limit in which an instrument/method can actually start to quantify the amount of something which is present. If the result is between the detection_limit and the quantification_limit, the result is estimated, because the instrument/method cant confidently identify the amount of something until it reaches the quantification_limit.\n\n**reporting_limit** - usually project or dataset specific. this limit is used for data analysis/statistics. the reporting_limit is equal to either the detection_limit or quantification_limit. This is better known as the \"reporting detection limit\".\n\n#### Method Detection Limit (MDL)\n\n- Statistically determined\n- The minimum concentration that can be measured with 99% confidence that the concentration is greater than zero\n- Concentrations near MDL are estimates\n- Laboratory, instrument, matrix, method, and analyte specific\n- Concentrations at MDL expected to be a false positive 1% of the time, but false negatives 50% of the time\n\n#### Method Reporting Limit (MRL)\n\n- May also be referred to as QL (quantitation limit), sample quantitation limit, or just RL (reporting limit)​\n- Determined by the lowest point of the calibration​\n- Not as specific as MDL, labs can adust​\n- Concentrations at MRL can be reliably quantified​\n- MRL > MDL​\n- Also laboratory, instrument, matrix, method, & analyte specific\n\n#### MDL & MRL Relationship\n\n![](../../static/resources/MDL_v_MRL.png)\n\n#### Other Detection Limits\n\n##### PQL\n\n- Considered to be lowest concentration that can be reliably quantified by a method\n- Limit of Detection (LOD); Lowest concentration that can be detected with a 1% false negative rate.\n  - Generally 2x to 3X MDL\n- Limit of Quantitation (LOQ); similar to MRL\n\n##### PCDD/F & PCB specific\n\n2.5 times signal to noise\n\n- **EQL**: Estimated Quantitation Limit\n- **EDL**: Estimated Detection Limit\n- **SDL**: Sample Detection Limit\n- **EMPC**\n  - Estimated Maximum Possible Concentration (EMPC)\n  - Peak present but not all of the identification criteria is met\n  - Always greater than MDL, may be greater than MRL\n  - Generally treated a non-detect in TEQ calculations\n  - EMPCs can present data management difficulties and need to be reviewed in QC checks\n\n",
    "supporting": [
      "data-management_files"
    ],
    "filters": [],
    "includes": {}
  }
}