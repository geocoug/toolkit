{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b7a8fdba-78ec-4371-864d-d1aa6cbdb6ba",
   "metadata": {},
   "source": [
    "---\n",
    "title: Web Scraping\n",
    "description: Download .grb2 files from WAVEWATCH III web server\n",
    "image: \"/static/development/webscrape.svg\"\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e0f7354-593a-4017-a965-e3b45802ea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import os\n",
    "import sys\n",
    "import requests\n",
    "import datetime\n",
    "import urllib.request\n",
    "\n",
    "# Third party packages\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504b9ae9-99b6-403a-82eb-4f962de6fb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAVEWATCH III web server base URL\n",
    "base_url = \"https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cbbbdc-fe37-4fb5-ada9-82d7a7fe55b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove logs and data files at termination of this script.\n",
    "cleanup = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "507e64fc-7b21-4ce3-8c22-5c4f4c997a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send an HTTP request\n",
    "def page_request(url):\n",
    "    page = requests.get(url)\n",
    "    if not page.ok:\n",
    "        print(\"Error reaching URL: {}\".format(url))\n",
    "        print(\"Page returned status code <{}>\".format(page.status_code))\n",
    "    else:\n",
    "        return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bd72946-ff5e-4aa3-935c-ba5a2c2c8e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_cleanup():\n",
    "    if os.path.exists(logfile):\n",
    "        os.remove(logfile)\n",
    "\n",
    "    import shutil\n",
    "    if os.path.exists('./data/'):\n",
    "        shutil.rmtree('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347a7a2c-9312-4011-b898-22a7d7a6b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the requested page content to a Beautiful Soup object\n",
    "soup = BeautifulSoup(page_request(base_url).content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e533a14a-9092-4abc-8331-baa70ea76625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all the anchor tags with an HREF. Each tag represents a data subdirectory. Discard the first and last anchor tag [0 = parent dir, -1 = reference].\n",
    "links = soup.find_all('a', href=True)[1:-1]\n",
    "grib_dirs = dict(map(lambda x: (x['href'], base_url + x['href'] + '/gribs'), links))\n",
    "\n",
    "# Only want the latest years data (for now)\n",
    "grib_dirs = dict(map(lambda x: x, list(grib_dirs.items())[-12:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3142988-91bc-4c81-b8a5-14e4dc6e5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "gribs = {}\n",
    "for grib_dir in grib_dirs:\n",
    "    grib_tags = BeautifulSoup(page_request(grib_dirs[grib_dir]).content, 'html.parser').find_all('a', href=True)[1:]\n",
    "    grib_links = dict(map(lambda x: (x['href'], grib_dirs[grib_dir] + '/' + x['href']), grib_tags))\n",
    "    gribs.update({grib_dir: grib_links})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2ab4a78-8940-4d02-91ea-2e33934f5fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = \"logfile_{}.log\".format(datetime.datetime.now().strftime(\"%Y%m%d\"))\n",
    "with open(logfile, \"w\") as log:\n",
    "    log.write(\"Date;Last_Updated;Content_Size_MB;URL;Data_Dir;Filename\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48804845-6f34-4704-8396-889933611a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.dp.200901.grb2\n",
      "Retrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.hs.200901.grb2\n",
      "Retrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.tp.200901.grb2\n",
      "Retrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.wind.200901.grb2\n"
     ]
    }
   ],
   "source": [
    "# Loop through data directories separated by month\n",
    "for month in gribs:\n",
    "    # Loop through each file in the data directory\n",
    "    i = 0\n",
    "    for grib in gribs[month]:\n",
    "        # Only interested in two gribs for now\n",
    "        if not ('glo_30m' in grib or 'ecg_10m' in grib):\n",
    "            continue\n",
    "        print('Retrieving file: {}'.format(gribs[month][grib]))\n",
    "        # Path to save grib file\n",
    "        data_dir = os.path.join(os.getcwd(), 'data', month)\n",
    "        # Make directory to save file\n",
    "        if not os.path.exists(data_dir):\n",
    "            os.makedirs(data_dir)\n",
    "        # Get header info from URL endpoint\n",
    "        meta = urllib.request.urlopen(gribs[month][grib]).info()\n",
    "        # Write some information to a log file\n",
    "        with open(logfile, \"a\") as log:\n",
    "            log.write(\"{};{};{};{};{};{}\\n\".format(meta['Date'], meta['Last-Modified'], int(meta['Content-Length']) / 1000000, gribs[month][grib], data_dir, grib))\n",
    "        # Send a request to the url and save the response file\n",
    "        urllib.request.urlretrieve(gribs[month][grib], os.path.join(data_dir, grib))\n",
    "        \n",
    "        # Only download a couple files for development purposes. Remove in production\n",
    "        if i >= 3:\n",
    "            break;\n",
    "        else:\n",
    "            i += 1\n",
    "    break # Remove if in production\n",
    "\n",
    "if cleanup:\n",
    "    do_cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
