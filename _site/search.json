[
  {
    "objectID": "content/development/code/notebooks/web-scraping.html",
    "href": "content/development/code/notebooks/web-scraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "# Standard libraries\nimport os\nimport sys\nimport requests\nimport datetime\nimport urllib.request\n\n# Third party packages\nfrom bs4 import BeautifulSoup\n\n\n# WAVEWATCH III web server base URL\nbase_url = \"https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/\"\n\n\n# Remove logs and data files at termination of this script.\ncleanup = True\n\n\n# Send an HTTP request\ndef page_request(url):\n    page = requests.get(url)\n    if not page.ok:\n        print(\"Error reaching URL: {}\".format(url))\n        print(\"Page returned status code &lt;{}&gt;\".format(page.status_code))\n    else:\n        return page\n\n\ndef do_cleanup():\n    if os.path.exists(logfile):\n        os.remove(logfile)\n\n    import shutil\n\n    if os.path.exists(\"./data/\"):\n        shutil.rmtree(\"./data/\")\n\n\n# Convert the requested page content to a Beautiful Soup object\nsoup = BeautifulSoup(page_request(base_url).content, \"html.parser\")\n\n\n# Search for all the anchor tags with an HREF. Each tag represents a data subdirectory. Discard the first and last anchor tag [0 = parent dir, -1 = reference].\nlinks = soup.find_all(\"a\", href=True)[1:-1]\ngrib_dirs = dict(map(lambda x: (x[\"href\"], base_url + x[\"href\"] + \"/gribs\"), links))\n\n# Only want the latest years data (for now)\ngrib_dirs = dict(map(lambda x: x, list(grib_dirs.items())[-12:]))\n\n\ngribs = {}\nfor grib_dir in grib_dirs:\n    grib_tags = BeautifulSoup(\n        page_request(grib_dirs[grib_dir]).content, \"html.parser\"\n    ).find_all(\"a\", href=True)[1:]\n    grib_links = dict(\n        map(lambda x: (x[\"href\"], grib_dirs[grib_dir] + \"/\" + x[\"href\"]), grib_tags)\n    )\n    gribs.update({grib_dir: grib_links})\n\n\nlogfile = \"logfile_{}.log\".format(datetime.datetime.now().strftime(\"%Y%m%d\"))\nwith open(logfile, \"w\") as log:\n    log.write(\"Date;Last_Updated;Content_Size_MB;URL;Data_Dir;Filename\\n\")\n\n\n# Loop through data directories separated by month\nfor month in gribs:\n    # Loop through each file in the data directory\n    i = 0\n    for grib in gribs[month]:\n        # Only interested in two gribs for now\n        if not (\"glo_30m\" in grib or \"ecg_10m\" in grib):\n            continue\n        print(\"Retrieving file: {}\".format(gribs[month][grib]))\n        # Path to save grib file\n        data_dir = os.path.join(os.getcwd(), \"data\", month)\n        # Make directory to save file\n        if not os.path.exists(data_dir):\n            os.makedirs(data_dir)\n        # Get header info from URL endpoint\n        meta = urllib.request.urlopen(gribs[month][grib]).info()\n        # Write some information to a log file\n        with open(logfile, \"a\") as log:\n            log.write(\n                \"{};{};{};{};{};{}\\n\".format(\n                    meta[\"Date\"],\n                    meta[\"Last-Modified\"],\n                    int(meta[\"Content-Length\"]) / 1000000,\n                    gribs[month][grib],\n                    data_dir,\n                    grib,\n                )\n            )\n        # Send a request to the url and save the response file\n        urllib.request.urlretrieve(gribs[month][grib], os.path.join(data_dir, grib))\n\n        # Only download a couple files for development purposes. Remove in production\n        if i &gt;= 3:\n            break\n        else:\n            i += 1\n    break  # Remove if in production\n\nif cleanup:\n    do_cleanup()\n\nRetrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.dp.200901.grb2\nRetrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.hs.200901.grb2\nRetrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.tp.200901.grb2\nRetrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.wind.200901.grb2"
  },
  {
    "objectID": "content/development/code/notebooks/sleep-data.html",
    "href": "content/development/code/notebooks/sleep-data.html",
    "title": "Sleep Data",
    "section": "",
    "text": "Sleep Cycle\nimport os\nimport re\nimport pandas as pd\nimport psycopg2\nfrom sqlalchemy import create_engine\nsleepdata = \"./data/sleepdata.csv\""
  },
  {
    "objectID": "content/development/code/notebooks/sleep-data.html#database-credentials",
    "href": "content/development/code/notebooks/sleep-data.html#database-credentials",
    "title": "Sleep Data",
    "section": "Database credentials",
    "text": "Database credentials\n\nwith open(\"../../postgres.txt\", \"r\") as f:\n    user, pwd = [s.strip() for s in f.readlines()]"
  },
  {
    "objectID": "content/development/code/notebooks/sleep-data.html#create-dataframe",
    "href": "content/development/code/notebooks/sleep-data.html#create-dataframe",
    "title": "Sleep Data",
    "section": "Create DataFrame",
    "text": "Create DataFrame\n\ndf = pd.read_csv(sleepdata, delimiter=\";\")\ndf.dtypes\n\nStart                           object\nEnd                             object\nSleep Quality                   object\nRegularity                      object\nMood                           float64\nHeart rate (bpm)                 int64\nSteps                            int64\nAlarm mode                      object\nAir Pressure (Pa)              float64\nCity                            object\nMovements per hour             float64\nTime in bed (seconds)          float64\nTime asleep (seconds)          float64\nTime before sleep (seconds)    float64\nWindow start                    object\nWindow stop                     object\nDid snore                         bool\nSnore time                     float64\nWeather temperature (°F)       float64\nWeather type                    object\nNotes                           object\ndtype: object"
  },
  {
    "objectID": "content/development/code/notebooks/sleep-data.html#create-db-safe-column-names",
    "href": "content/development/code/notebooks/sleep-data.html#create-db-safe-column-names",
    "title": "Sleep Data",
    "section": "Create DB Safe Column Names",
    "text": "Create DB Safe Column Names\n\nnonword_pattern = re.compile(r\"[^\\w]\")\nspacing_pattern = re.compile(r\"[_]{2,}\")\nending_pattern = re.compile(r\"_$\")\ndf.columns = [\n    re.sub(\n        ending_pattern,\n        \"\",\n        re.sub(spacing_pattern, \"_\", re.sub(nonword_pattern, \"_\", col)),\n    ).lower()\n    for col in df.columns\n]\ndf.columns\n\nIndex(['start', 'end', 'sleep_quality', 'regularity', 'mood', 'heart_rate_bpm',\n       'steps', 'alarm_mode', 'air_pressure_pa', 'city', 'movements_per_hour',\n       'time_in_bed_seconds', 'time_asleep_seconds',\n       'time_before_sleep_seconds', 'window_start', 'window_stop', 'did_snore',\n       'snore_time', 'weather_temperature_f', 'weather_type', 'notes'],\n      dtype='object')\n\n\n\ndb_host = \"localhost\"\ndb_table = \"sleepdata\"\ndb_name = \"personal\"\n\nengine = create_engine(f\"postgresql://{user}:{pwd}@{db_host}/{db_name}\")\n\ndf.to_sql(f\"{db_table}\", engine, if_exists=\"replace\", index=False)\n\n509"
  },
  {
    "objectID": "content/development/code/notebooks/pyspark-practice.html",
    "href": "content/development/code/notebooks/pyspark-practice.html",
    "title": "PySpark Practice",
    "section": "",
    "text": "This is intended to run on Google Colab\nInstall everything necessary to make spark work.\n\n!apt-get install openjdk-11-jdk-headless -qq &gt; /dev/null\n!wget -q https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n!tar xf spark-3.0.3-bin-hadoop2.7.tgz\n!pip install -q findspark\n\nSet the paths to the installs\n\nimport os\n\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n\nFind the spark installation\n\nimport findspark\n\nfindspark.init()\n\nStart doing fancy pyspark stuff\n\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    StringType,\n    DoubleType,\n    IntegerType,\n    ArrayType,\n)\nfrom pyspark.sql import DataFrame, SparkSession\n\nimport json\n\nRequest Orders.json from google drive via command line.\n\n!wget -q --no-check-certificate 'https://drive.google.com/uc?export=download&id=1I6VuRILNtyhnWMUml61Dv58YOP2dqlvx' -O 'Orders.json'\n\nOr, do it the Python way.\n\n# from google.colab import drive\n# drive.mount('/content/drive')\n# INPUT_FILE = '/content/drive/MyDrive/Colab Notebooks/Starbucks/1_basic_exercise/resources/Order.json'\n\nSet input/output variables\n\nINPUT_FILE = \"/content/Orders.json\"  # TODO: Change this based on actual location for your environment setup\nOUTPUT_CSV_FILE = \"./output/files/output.csv\"\nOUTPUT_DELTA_PATH = \"./output/delta/\"\n\nJSON data summary:\n\nEach list element is an event\nEvents contain a message about an order\nOrders contain a list of items that contain attributes about the item\nItems can also contain a lists of items (childItems, discounts)\n\nCreate spark session\n\nspark = (\n    SparkSession.builder.appName(\"programming\")\n    .master(\"local\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n    )\n    .config(\"spark.ui.port\", \"4050\")\n    .getOrCreate()\n)\n\n\nfrom traitlets.traitlets import default\n\n\ndef read_json(file_path: str, schema: StructType) -&gt; DataFrame:\n    \"\"\"\n    The goal of this method is to parse the input json data using the schema from another method.\n\n    We are only interested in data starting at orderPaid attribute.\n\n    :param file_path: Order.json will be provided\n    :param schema: schema that needs to be passed to this method\n    :return: Dataframe containing records from Order.json\n    \"\"\"\n    # Only interested in data starting at orderPaid\n    with open(file_path) as f:\n        js = json.load(f)[0][\"data\"][\"message\"][\"orderPaid\"]\n\n    # Create Dataframe\n    #   - Use spark JSON method for reading object\n    #   - Use parallelize on array object, which contains json structured data\n    #   - Use custom schema so data type/structure is defined instead of inferred\n    df = spark.read.json(\n        spark.sparkContext.parallelize([js]), schema=schema[\"order_paid_type\"]\n    )\n    return df\n\nThe schema outlined below represents a “one-to-many” relationship and is defined in a bottom-up fashion.\n\ndef get_struct_type() -&gt; StructType:\n    \"\"\"\n    Build a schema based on the the file Order.json\n\n    :return: Structype of equivalent JSON schema\n    \"\"\"\n    discount_type = StructType(\n        [\n            StructField(\"amount\", IntegerType(), True),\n            StructField(\"description\", StringType(), True),\n        ]\n    )\n\n    child_item_type = StructType(\n        [\n            StructField(\"lineItemNumber\", StringType(), True),\n            StructField(\"itemLabel\", StringType(), True),\n            StructField(\"quantity\", DoubleType(), True),\n            StructField(\"price\", IntegerType(), True),\n            StructField(\n                \"discounts\", ArrayType(discount_type), True\n            ),  # Changed \"TODO --&gt; ArrayType(discount_type). Will inherit discout_type attributes.\n        ]\n    )\n\n    item_type = StructType(\n        [\n            StructField(\"lineItemNumber\", StringType(), True),\n            StructField(\"itemLabel\", StringType(), True),\n            StructField(\"quantity\", DoubleType(), True),\n            StructField(\"price\", IntegerType(), True),\n            StructField(\n                \"discounts\", ArrayType(discount_type), True\n            ),  # Changed \"TODO\" --&gt; ArrayType(discount_type). Will inherit discount_type attributes.\n            StructField(\n                \"childItems\", ArrayType(child_item_type), True\n            ),  # Changed \"TODO\" --&gt; ArrayType(child_item_type). Will inherit chile_item_type attributes.\n        ]\n    )\n\n    order_paid_type = StructType(\n        [\n            StructField(\"orderToken\", StringType(), True),\n            StructField(\"preparation\", StringType(), True),\n            StructField(\n                \"items\", ArrayType(item_type), True\n            ),  # Changed \"TODO\" --&gt; ArrayType(item_type). Will inherit item_type attributes.\n        ]\n    )\n\n    message_type = StructType(\n        [StructField(\"orderPaid\", order_paid_type, True)]\n    )  # Changed \"TODO\" --&gt; order_paid_type. Will inherit order_paid_type attributes.\n\n    data_type = StructType(\n        [StructField(\"message\", message_type, True)]\n    )  # Changed \"TODO\" --&gt; message_type. Will inherit message_type attributes.\n\n    body_type = StructType(\n        [\n            StructField(\"id\", StringType(), True),\n            StructField(\"subject\", StringType(), True),\n            StructField(\n                \"data\", data_type, True\n            ),  # Changed \"TODO\" --&gt; data_type. Will inherit data_type attributes.\n            StructField(\"eventTime\", StringType(), True),\n        ]\n    )\n    return {\n        \"body_type\": body_type,\n        \"data_type\": data_type,\n        \"message_type\": message_type,\n        \"order_paid_type\": order_paid_type,\n        \"item_type\": item_type,\n        \"child_item_type\": child_item_type,\n        \"discount_type\": discount_type,\n    }\n\n\ndef get_rows_from_array(df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Input data frame contains columns of type array. Identify those columns and convert them to rows.\n\n    :param df: Contains column with data type of type array.\n    :return: The dataframe should not contain any columns of type array\n    \"\"\"\n    # explode will create a new row for each element in an array\n    from pyspark.sql.functions import explode\n\n    # Iterate over field names\n    for i, f in enumerate(df.schema.fields):\n        # Check datatype of field\n        if isinstance(f.dataType, ArrayType):\n            arrayCol = f.name\n\n    # Overwrite dataframe object\n    # Create a new row for every element in \"arrayCol\".\n    # Each new row will contain the same values from\n    #   columns that are not \"arrayCol\".\n    # Use \"withColumn()\" to transform dataframe.\n    #   First argument - What column will be transformed (will overwrite because already exists)\n    #   Second argument - Expression to create/modify values for the column\n    df = df.withColumn(arrayCol, explode(arrayCol))\n    return df\n\n\ndef get_unwrapped_nested_structure(df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Convert columns that contain multiple attributes to columns of their own\n\n    :param df: Contains columns that have multiple attributes\n    :return: Dataframe should not contain any nested structures\n    \"\"\"\n\n    def parse_struct(df):\n        \"\"\"Create an array of columns to be selected.\n        If column type = StructType, select all attributes\n        in that StructType as additional columns.\n        Returns list of columns.\"\"\"\n        cols = []\n        for i, d in enumerate(df.schema.fields):\n            if isinstance(d.dataType, StructType):\n                cols.append(f\"{d.name}.*\")\n            else:\n                cols.append(d.name)\n        return cols\n\n    df = df.select(parse_struct(df))\n\n    # Check for columns of type Array.\n    # If type Array, transform elements to rows\n    arrayCols = [c.name for c in df.schema.fields if isinstance(c.dataType, ArrayType)]\n    if len(arrayCols) &gt; 0:\n        for col in arrayCols:\n            df = get_rows_from_array(df)\n\n    # Could have multiple instances of key names\n    # Will have to add columns manually\n    # If unique names, could probably reuse \"parse_struct()\"\n    df = df.withColumn(\"discountAmount\", df.discounts.amount).withColumn(\n        \"discountDescription\", df.discounts.description\n    )\n    df = df.drop(\"discounts\")\n\n    df = (\n        df.withColumn(\"childItemLineNumber\", df.childItems.lineItemNumber)\n        .withColumn(\"childItemLabel\", df.childItems.itemLabel)\n        .withColumn(\"childItemQuantity\", df.childItems.quantity)\n        .withColumn(\"childItemPrice\", df.childItems.price)\n        .withColumn(\"childItemDiscounts\", df.childItems.discounts)\n    )\n    df = df.drop(\"childItems\")\n\n    df = get_rows_from_array(df)\n    df = df.withColumn(\n        \"childItemDiscountAmount\", df.childItemDiscounts.amount\n    ).withColumn(\"childItemDiscountDescription\", df.childItemDiscounts.description)\n    df = df.drop(\"childItemDiscounts\")\n\n    return df\n\n\ndef write_df_as_csv(df: DataFrame) -&gt; None:\n    \"\"\"\n    Write the data frame to a local destination of your choice with headers\n\n    :param df: Contains flattened order data\n    \"\"\"\n    df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\n        OUTPUT_CSV_FILE\n    )\n    return None\n\n\ndef create_delta_table(spark: SparkSession) -&gt; None:\n    spark.sql(\"CREATE DATABASE IF NOT EXISTS EXERCISE\")\n\n    spark.sql(\n        \"\"\"\n    CREATE TABLE IF NOT EXISTS EXERCISE.ORDERS(\n        OrderToken String,\n        Preparation  String,\n        ItemLineNumber String,\n        ItemLabel String,\n        ItemQuantity Double,\n        ItemPrice Integer,\n        ItemDiscountAmount Integer,\n        ItemDiscountDescription String,\n        ChildItemLineNumber String, \n        ChildItemLabel String,\n        ChildItemQuantity Double,\n        ChildItemPrice Integer,\n        ChildItemDiscountAmount Integer,\n        ChildItemDiscountDescription String\n    ) USING DELTA\n    LOCATION \"{0}\"\n    \"\"\".format(\n            OUTPUT_DELTA_PATH\n        )\n    )\n\n    return None\n\nHaven’t used Delta before. Reference material: https://docs.microsoft.com/en-us/azure/databricks/delta/quick-start\n\ndef write_df_as_delta(df: DataFrame) -&gt; None:\n    \"\"\"\n    Write the dataframe output to the table created, overwrite mode can be used\n\n    :param df: flattened data\n    :return: Data from the orders table\n    \"\"\"\n    # Rename columns to match delta table schema\n    df = (\n        df.withColumnRenamed(\"orderToken\", \"OrderToken\")\n        .withColumnRenamed(\"preparation\", \"Rreparation\")\n        .withColumnRenamed(\"lineItemNumber\", \"LineItemNumber\")\n        .withColumnRenamed(\"itemLabel\", \"ItemLabel\")\n        .withColumnRenamed(\"quantity\", \"Quantity\")\n        .withColumnRenamed(\"price\", \"Price\")\n        .withColumnRenamed(\"discountAmount\", \"DiscountAmount\")\n        .withColumnRenamed(\"discountDescription\", \"DiscountDescription\")\n        .withColumnRenamed(\"childItemLineNumber\", \"ChildItemLineNumber\")\n        .withColumnRenamed(\"childItemLabel\", \"ChildItemLabel\")\n        .withColumnRenamed(\"childItemQuantity\", \"ChildItemQuantity\")\n        .withColumnRenamed(\"childItemPrice\", \"ChildItemPrice\")\n        .withColumnRenamed(\"childItemDiscountAmount\", \"ChildItemDiscountAmount\")\n        .withColumnRenamed(\n            \"childItemDiscountDescription\", \"ChildItemDiscountDescription\"\n        )\n    )\n\n    df.write.insertInto(\"EXERCISE.ORDERS\", overwrite=True)\n\n    return None\n\n\ndef read_data_delta(spark: SparkSession) -&gt; DataFrame:\n    \"\"\"\n    Read data from the table created\n\n    :param spark:\n    :return:\n    \"\"\"\n    return spark.sql(\"select * from exercise.orders;\")\n\n\nif __name__ == \"__main__\":\n    input_schema = get_struct_type()\n\n    input_df = read_json(INPUT_FILE, input_schema)\n\n    arrays_to_rows_df = get_rows_from_array(input_df)\n\n    unwrap_struct_df = get_unwrapped_nested_structure(arrays_to_rows_df)\n\n    write_df_as_csv(unwrap_struct_df)\n\n    create_delta_table(spark)\n    write_df_as_delta(unwrap_struct_df)\n\n    result_df = read_data_delta(spark)\n    result_df.show(truncate=False)\n\n+-----------------------+------------------+--------------+---------+------------+---------+------------------+-----------------------+-------------------+--------------+-----------------+--------------+-----------------------+----------------------------+\n|OrderToken             |Preparation       |ItemLineNumber|ItemLabel|ItemQuantity|ItemPrice|ItemDiscountAmount|ItemDiscountDescription|ChildItemLineNumber|ChildItemLabel|ChildItemQuantity|ChildItemPrice|ChildItemDiscountAmount|ChildItemDiscountDescription|\n+-----------------------+------------------+--------------+---------+------------+---------+------------------+-----------------------+-------------------+--------------+-----------------+--------------+-----------------------+----------------------------+\n|97331549875122744335422|Magic happens here|1             |COFFEE   |1.0         |345      |495               |Item 1, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 1, Discount 1    |\n|97331549875122744335422|Magic happens here|2             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|3             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|4             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|5             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|6             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|7             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|8             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|9             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|10            |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n+-----------------------+------------------+--------------+---------+------------+---------+------------------+-----------------------+-------------------+--------------+-----------------+--------------+-----------------------+----------------------------+"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html",
    "href": "content/development/code/notebooks/ml-notes.html",
    "title": "ML Notes",
    "section": "",
    "text": "Use of algorithms and statistical models to perform tasks without explicit instructions, instead using paterns and inference.\nExamples:\nimport numpy as np\nimport matplotlib.pyplot as plt\nMatplotlib:"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#practical-example---plotting",
    "href": "content/development/code/notebooks/ml-notes.html#practical-example---plotting",
    "title": "ML Notes",
    "section": "Practical Example - Plotting",
    "text": "Practical Example - Plotting\n\n# Effect of time spent walking (hours) on the distance travelled (miles)\ntime_spent_walking = [1, 2, 3, 4, 5]  # (independent variable)\ndistance = [2, 4, 6, 8, 10]  # (dependent variable)\n\nplt.plot(time_spent_walking, distance)\nplt.xlabel(\"Time Spent Walking (hours)\")\nplt.ylabel(\"Distance (miles)\")\nplt.title(\"Effect of time spent walking on distance travelled\")\nplt.show()\n\n\n\n\n\n# Effect of car age (years) on price ($)\ncar_age = [1, 2, 5, 10, 30]\nprice = [30000, 25000, 18000, 10000, 4000]\n\nplt.plot(car_age, price)\nplt.xlabel(\"Car Age (years)\")\nplt.ylabel(\"Price ($)\")\nplt.title(\"Effect of car age on price\")\nplt.show()\n\n\n\n\n\n# Effect of amount of time spent studying (hours) on test score results (%)\nstudy_time = [1, 5, 10, 20, 50, 100]\ntest_score_results = [40, 60, 70, 75, 88, 93]\n\nplt.plot(study_time, test_score_results)\nplt.xlabel(\"Study Time (hours)\")\nplt.ylabel(\"Test Score (%)\")\nplt.title(\"Effect of study time on test scores\")\nplt.show()"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#linear-regression-y-mx-b",
    "href": "content/development/code/notebooks/ml-notes.html#linear-regression-y-mx-b",
    "title": "ML Notes",
    "section": "Linear Regression: y = mx + b",
    "text": "Linear Regression: y = mx + b\n\nx: x axis\ny: y axis\nm: gradient of line\nb: value of y when x = 0\n\n\nExample 1\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 6, 8, 10]\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n# y = 2x + 2\nfor i in x:\n    y = 2 * i\n    print(y)\n\n2\n4\n6\n8\n10\n\n\n\n\nExample 2\n\nx = [1, 2, 3, 4, 5]\ny = [6, 9, 12, 15, 18]\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n# y = mx + b\nfor i in x:\n    y = (3 * i) + 3\n    print(y)\n\n6\n9\n12\n15\n18\n\n\n\n\nExample 3\n\nx = [0, 1, 2, 3, 4]\ny = [6, 9, 12, 15, 18]\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n# y = mx + b\n# y = 3x + 6\n\nfor i in x:\n    y = (3 * i) + 6\n    print(i, y)\n\n0 6\n1 9\n2 12\n3 15\n4 18"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#practical-examples---linear-regression-y-mx-b",
    "href": "content/development/code/notebooks/ml-notes.html#practical-examples---linear-regression-y-mx-b",
    "title": "ML Notes",
    "section": "Practical Examples - Linear Regression (y = mx + b)",
    "text": "Practical Examples - Linear Regression (y = mx + b)\n\nQuestion 1\n\n# Effect of amount of water provided (L) per day on size of trees (m)\nwater = [0, 1, 2, 3, 4, 5, 6, 7, 8]\ntree_size = [4, 5, 6, 7, 8, 9, 10, 11, 12]\n\nplt.plot(water, tree_size)\nplt.show()\n\n\n\n\n\n\nSolution\n\n# y = mx + b\nfor x in water:\n    y = (1 * x) + 4\n    print(x, y)\n    # y = x + 4\n\n0 4\n1 5\n2 6\n3 7\n4 8\n5 9\n6 10\n7 11\n8 12\n\n\n\n\nQuestion 2\n\n# Effect of wingspan (cm) on flying speed (km/h)\nwingspan = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nflying_speed = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\nplt.plot(wingspan, flying_speed)\nplt.show()\n\n\n\n\n\n\nSolution\n\n# y = mx + b\nfor x in wingspan:\n    y = 0.5 * x\n    print(x, y)\n    # y = 0.5x + 0\n\n0 0.0\n10 5.0\n20 10.0\n30 15.0\n40 20.0\n50 25.0\n60 30.0\n70 35.0\n80 40.0\n90 45.0\n100 50.0\n\n\n\n\nQuestion 3\n\n# Effect of number of gifts given to employees each year, on staff statisfaction levels (100%)\nnum_of_gifts = [0, 1, 2, 3, 4, 5]\nsatisfaction = [50, 55, 60, 65, 70, 75]\nplt.plot(num_of_gifts, satisfaction)\nplt.show()\n\n\n\n\n\n\nSolution\n\n# y = mx + b\nfor x in num_of_gifts:\n    y = (5 * x) + 50\n    print(x, y)\n    # y = 5x + 50\n\n0 50\n1 55\n2 60\n3 65\n4 70\n5 75"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#line-of-best-fit",
    "href": "content/development/code/notebooks/ml-notes.html#line-of-best-fit",
    "title": "ML Notes",
    "section": "Line of Best Fit",
    "text": "Line of Best Fit\nCost: distance of each point from best fit line\nLoss: Sum of all distances between best fit line and data points (sum of costs)\n\n\n\nimage.png"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#mean-squared-error-mse",
    "href": "content/development/code/notebooks/ml-notes.html#mean-squared-error-mse",
    "title": "ML Notes",
    "section": "Mean Squared Error (MSE)",
    "text": "Mean Squared Error (MSE)\n\n\n\nimage.png\n\n\nMean Squared Error = sum (Y @ prediction - y at datapoint)^2 / number of datapoints\n\nExample\n\n# Data\nx_data = [1, 5, 8, 10]\ny_data = [4, 8, 9, 7]\n\n# Data plot\nplt.plot(x_data, y_data)\nplt.show()\n\n\n\n\nBest fit line (y = mx + b)\n\n# Calculate m, b\nm, b = np.polyfit(x_data, y_data, 1)\n\n# Best fit line equation:\nfor x in x_data:\n    y = (m * x) + b\n    print(x, y)\n\n1 5.04347826086957\n5 6.608695652173917\n8 7.7826086956521765\n10 8.56521739130435\n\n\nCost & Loss - Mean Squared Error\n\n\n\nimage.png\n\n\n\n\n\nvariable\n\n\n\n\n\n\n\n\nx\n1\n5\n8\n10\n\n\ny\n4\n8\n9\n7\n\n\ny[hat]\n5\n6\n7\n8\n\n\ncost\n1\n4\n4\n1\n\n\n\n\n# cost = (y[hat] - y) ^ 2 / 4\n\nmse = 10 / 4\nmse\n\n2.5"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#logistic-regression",
    "href": "content/development/code/notebooks/ml-notes.html#logistic-regression",
    "title": "ML Notes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\n\nimage.png"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#overfitting",
    "href": "content/development/code/notebooks/ml-notes.html#overfitting",
    "title": "ML Notes",
    "section": "Overfitting",
    "text": "Overfitting\nBest fit too accuratly defines traning data and minimizes the loss… but thats only training data and new data will keep the model humble.\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nRandom Forests\n\nRandom forests takes an average or majority decision from numerous decision trees and creates an output from this"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html",
    "href": "content/development/code/notebooks/grib.html",
    "title": "GRIB",
    "section": "",
    "text": "#! /usr/bin/env python3\n#\n#\n#   Purpose:\n#       Test manipulation of .grb2 files from NOAA WAVEWATCH III hindcast repo\n#\n#   Notes:\n#       1. pygrib is not compatible with Windows.\n#       2. GRIB = General Regularly distributed Information in Binary form\n#           GRIB is a binary format of gridded data.\n#       3. WAVEWATCH III: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2.php\n#       3. pygrib docs: https://jswhit.github.io/pygrib/api.html#example-usage\n#\n#   Author(s):\n#       Caleb Grant\n#\n#   Revision History\n#   Date        Reason\n#   ----------------------------\n#   2021-09-23  Created. CG.\n# ===========================================\n# Standard libraries\nimport os\nimport datetime\n\n# Third party packages\nimport pygrib\nimport numpy as np\n\n# Make output of plotting commands display inline with notebook\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\n\nModuleNotFoundError: No module named 'pygrib'"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#object-methods",
    "href": "content/development/code/notebooks/grib.html#object-methods",
    "title": "GRIB",
    "section": "Object methods",
    "text": "Object methods\n\nprint(dir(dp_grb_obj))\n\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__ne__', '__new__', '__next__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '_advance', 'close', 'closed', 'has_multi_field_msgs', 'message', 'messagenumber', 'messages', 'name', 'read', 'readline', 'rewind', 'seek', 'select', 'tell']"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#grib-object-keys",
    "href": "content/development/code/notebooks/grib.html#grib-object-keys",
    "title": "GRIB",
    "section": "GRIB Object keys",
    "text": "GRIB Object keys\n\nprint(sorted(dp_grb_obj[1].keys()))\n\n['GRIBEditionNumber', 'NV', 'Ni', 'Nj', 'PLPresent', 'PVPresent', 'alternativeRowScanning', 'analDate', 'angleDivisor', 'angleMultiplier', 'angleSubdivisions', 'average', 'backgroundProcess', 'basicAngleOfTheInitialProductionDomain', 'binaryScaleFactor', 'bitMapIndicator', 'bitmapPresent', 'bitsPerValue', 'bottomLevel', 'centre', 'centreDescription', 'cfName', 'cfNameECMF', 'cfVarName', 'cfVarNameECMF', 'changeDecimalPrecision', 'codedValues', 'dataDate', 'dataRepresentationTemplateNumber', 'dataTime', 'day', 'decimalPrecision', 'decimalScaleFactor', 'deleteCalendarId', 'deleteLocalDefinition', 'deletePV', 'discipline', 'distinctLatitudes', 'distinctLongitudes', 'editionNumber', 'endStep', 'forecastTime', 'g2grid', 'genVertHeightCoords', 'generatingProcessIdentifier', 'getNumberOfValues', 'globalDomain', 'grib2LocalSectionPresent', 'grib2divider', 'gridDefinitionDescription', 'gridDefinitionTemplateNumber', 'gridDescriptionSectionPresent', 'gridType', 'hour', 'hoursAfterDataCutoff', 'iDirectionIncrement', 'iDirectionIncrementGiven', 'iDirectionIncrementInDegrees', 'iScansNegatively', 'iScansPositively', 'identifier', 'ieeeFloats', 'ifsParam', 'ijDirectionIncrementGiven', 'indicatorOfUnitOfTimeRange', 'interpretationOfNumberOfPoints', 'isConstant', 'isHindcast', 'is_aerosol', 'is_aerosol_optical', 'is_chemical', 'is_chemical_distfn', 'is_efas', 'is_uerra', 'jDirectionIncrement', 'jDirectionIncrementGiven', 'jDirectionIncrementInDegrees', 'jPointsAreConsecutive', 'jScansPositively', 'julianDay', 'kurtosis', 'latLonValues', 'latitudeOfFirstGridPoint', 'latitudeOfFirstGridPointInDegrees', 'latitudeOfLastGridPoint', 'latitudeOfLastGridPointInDegrees', 'latitudes', 'lengthOfHeaders', 'level', 'localTablesVersion', 'longitudeOfFirstGridPoint', 'longitudeOfFirstGridPointInDegrees', 'longitudeOfLastGridPoint', 'longitudeOfLastGridPointInDegrees', 'longitudes', 'mAngleMultiplier', 'mBasicAngle', 'masterDir', 'maximum', 'md5Headers', 'md5Section1', 'md5Section3', 'md5Section4', 'md5Section5', 'md5Section6', 'md5Section7', 'minimum', 'minute', 'minutesAfterDataCutoff', 'missingValue', 'modelName', 'month', 'name', 'nameECMF', 'nameOfFirstFixedSurface', 'nameOfSecondFixedSurface', 'neitherPresent', 'numberOfDataPoints', 'numberOfMissing', 'numberOfOctectsForNumberOfPoints', 'numberOfSection', 'numberOfSection', 'numberOfSection', 'numberOfSection', 'numberOfSection', 'numberOfSection', 'numberOfValues', 'offsetValuesBy', 'optimizeScaleFactor', 'packingType', 'paramId', 'paramIdECMF', 'parameterCategory', 'parameterName', 'parameterNumber', 'parameterUnits', 'pressureUnits', 'productDefinitionTemplateNumber', 'productType', 'productionStatusOfProcessedData', 'radius', 'referenceValue', 'referenceValueError', 'resolutionAndComponentFlags', 'resolutionAndComponentFlags1', 'resolutionAndComponentFlags2', 'resolutionAndComponentFlags6', 'resolutionAndComponentFlags7', 'resolutionAndComponentFlags8', 'scaleFactorOfEarthMajorAxis', 'scaleFactorOfEarthMinorAxis', 'scaleFactorOfFirstFixedSurface', 'scaleFactorOfRadiusOfSphericalEarth', 'scaleFactorOfSecondFixedSurface', 'scaleValuesBy', 'scaledValueOfEarthMajorAxis', 'scaledValueOfEarthMinorAxis', 'scaledValueOfFirstFixedSurface', 'scaledValueOfRadiusOfSphericalEarth', 'scaledValueOfSecondFixedSurface', 'scanningMode', 'scanningMode5', 'scanningMode6', 'scanningMode7', 'scanningMode8', 'second', 'section0Length', 'section1Length', 'section3Length', 'section4Length', 'section5Length', 'section6Length', 'section7Length', 'section8Length', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'selectStepTemplateInstant', 'selectStepTemplateInterval', 'setBitsPerValue', 'setCalendarId', 'shapeOfTheEarth', 'shortName', 'shortNameECMF', 'significanceOfReferenceTime', 'skewness', 'sourceOfGridDefinition', 'standardDeviation', 'startStep', 'stepRange', 'stepType', 'stepTypeInternal', 'stepUnits', 'subCentre', 'subdivisionsOfBasicAngle', 'tablesVersion', 'tablesVersionLatest', 'targetCompressionRatio', 'tempPressureUnits', 'topLevel', 'totalLength', 'typeOfCompressionUsed', 'typeOfFirstFixedSurface', 'typeOfGeneratingProcess', 'typeOfLevel', 'typeOfOriginalFieldValues', 'typeOfProcessedData', 'typeOfSecondFixedSurface', 'units', 'unitsECMF', 'unitsOfFirstFixedSurface', 'unitsOfSecondFixedSurface', 'uvRelativeToGrid', 'validDate', 'validityDate', 'validityTime', 'values', 'year']"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#test-value-arrays",
    "href": "content/development/code/notebooks/grib.html#test-value-arrays",
    "title": "GRIB",
    "section": "Test value arrays",
    "text": "Test value arrays\n\nvalues = np.array([grb.values for grb in hs_grb_obj])\nprint(values.shape, values.min(), values.max())\n\n(249, 331, 301) 0.0 9999.0\n\n\n\nprint(dir(values))\n\n['T', '__abs__', '__add__', '__and__', '__array__', '__array_finalize__', '__array_function__', '__array_interface__', '__array_prepare__', '__array_priority__', '__array_struct__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__xor__', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n\n\n\n# sets, rows per set, columns per row\nprint(values.shape)\n\n# would equate to\nsets = len(values)\nrows = len(values[0])\ncols = len(values[0][0])\nprint(sets, rows, cols)\n\n(249, 331, 301)\n249 331 301"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#testing-latlongs",
    "href": "content/development/code/notebooks/grib.html#testing-latlongs",
    "title": "GRIB",
    "section": "Testing lat/longs",
    "text": "Testing lat/longs\n\nfor grb in dp_grb_obj[1:6]:\n    lats, lons = grb.latlons()\n    print(\"min/max lat ang lon: \", lats.min(), lats.max(), lons.min(), lons.max())\n\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\n\n\n\nwind_grb_file = \"multi_reanal.ecg_10m.wind.200901.grb2\"\nwind_grb_obj = pygrib.open(os.path.join(data_dir, wind_grb_file))\n\n# Reqind the iterator\nwind_grb_obj.rewind()\n\n# values = np.array([grb.values for grb in wind_grb_obj])\n# np.savetxt(\"test_wind.csv\", values, delimiter=\",\")\n\ndata_dict = {}\n# Each iteration captures point in time across entire grid\nfor grb in wind_grb_obj:\n    data_dict.update({grb.validDate: {grb.parameterName: np.array(grb.values)}})\n\n\nprint(data_dict[datetime.datetime(2009, 1, 1, 0, 0)])\n\n{'v-component of wind': array([[9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       ...,\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.]])}\n\n\n\nhs_grb_obj.rewind()\n\nprint(hs_grb_obj[1].parameterName)\n\ndate_selected = datetime.datetime(2009, 1, 1, 0, 0)\n\nvalues = []\n\nfor grb in hs_grb_obj:\n    if (\n        grb.validDate == date_selected\n        and grb.parameterName == \"Significant height of combined wind waves and swell\"\n    ):\n        values.append(grb.values)\nvalues = np.array(values)\nprint(values.shape, values.min(), values.max())\n\nlats, longs = grb.latlons()\nprint(lats.min(), lats.max(), longs.min(), longs.max())\n\nSignificant height of combined wind waves and swell\n(1, 331, 301) 0.0 9999.0\n-0.00010999999961602835 55.0 260.0 310.0000000000057\n\n\n\n\"\"\"\nfig = plt.figure(figsize=(16,35))\n# https://matplotlib.org/basemap/users/ortho.html\nm = Basemap(projection='lcc', lon_0=-80, lat_0=30, resolution='l', width=5.e6, height=5.e6)\nx, y = m(longs, lats)\n\n# for vals in range(1, 2):\n#     print(values[vals])\n\nax = plt.plot(values[0])\nm.drawcoastlines()\ncs = m.contourf(x, y, values[0], np.linspace(230, 300, 41), cmap=plt.cm.jet, extend='both')\nt = plt.title('test')\n\"\"\"\n\n\"\\nfig = plt.figure(figsize=(16,35))\\n# https://matplotlib.org/basemap/users/ortho.html\\nm = Basemap(projection='lcc', lon_0=-80, lat_0=30, resolution='l', width=5.e6, height=5.e6)\\nx, y = m(longs, lats)\\n\\n# for vals in range(1, 2):\\n#     print(values[vals])\\n\\nax = plt.plot(values[0])\\nm.drawcoastlines()\\ncs = m.contourf(x, y, values[0], np.linspace(230, 300, 41), cmap=plt.cm.jet, extend='both')\\nt = plt.title('test')\\n\"\n\n\n\nhs_grb_obj.rewind()\n\nfor grb in hs_grb_obj[1:2]:\n    print(grb.validDate, grb.parameterName, grb.latlons())\n\n2009-01-01 00:00:00 Significant height of combined wind waves and swell (array([[ 5.5000000e+01,  5.5000000e+01,  5.5000000e+01, ...,\n         5.5000000e+01,  5.5000000e+01,  5.5000000e+01],\n       [ 5.4833333e+01,  5.4833333e+01,  5.4833333e+01, ...,\n         5.4833333e+01,  5.4833333e+01,  5.4833333e+01],\n       [ 5.4666666e+01,  5.4666666e+01,  5.4666666e+01, ...,\n         5.4666666e+01,  5.4666666e+01,  5.4666666e+01],\n       ...,\n       [ 3.3322400e-01,  3.3322400e-01,  3.3322400e-01, ...,\n         3.3322400e-01,  3.3322400e-01,  3.3322400e-01],\n       [ 1.6655700e-01,  1.6655700e-01,  1.6655700e-01, ...,\n         1.6655700e-01,  1.6655700e-01,  1.6655700e-01],\n       [-1.1000000e-04, -1.1000000e-04, -1.1000000e-04, ...,\n        -1.1000000e-04, -1.1000000e-04, -1.1000000e-04]]), array([[260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       ...,\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ]]))\n\n\n\nprint(\n    (datetime.datetime(2009, 1, 1, 0, 0) + datetime.timedelta(hours=744)).strftime(\n        \"%Y-%m-%d\"\n    )\n)\n\n2009-02-01\n\n\n\nBetter iteration method:\n\n# hs_grb_obj.rewind()\nhs_grb_obj.seek(0)\nprint(hs_grb_obj.tell())\nprint(hs_grb_obj.read(1))\n\n0\n[1:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 0 hrs:from 200901010000]\n\n\n\nhs_grb_obj.seek(0)\nprint(hs_grb_obj.tell())\n\nfor grb in hs_grb_obj:\n    grb\n\nprint(hs_grb_obj.tell())\n\n0\n249\n\n\n\nhs_grb_obj.seek(0)\ngrb = hs_grb_obj.read(1)[0]\nprint(\"1\")\nprint(grb.values)\n\n\nprint(\"2\")\ngrb = hs_grb_obj.select(name=\"Significant height of combined wind waves and swell\")[0]\nprint(grb.values)\n\n1\n[[-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n ...\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]]\n2\n[[-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n ...\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]]\n\n\n\nhs_grb_obj.select(name=\"Significant height of combined wind waves and swell\")[1:6]\n\n[2:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 3 hrs:from 200901010000,\n 3:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 6 hrs:from 200901010000,\n 4:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 9 hrs:from 200901010000,\n 5:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 12 hrs:from 200901010000,\n 6:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 15 hrs:from 200901010000]\n\n\n\nFind the first grib message with a matching name:\n\ngrb = hs_grb_obj.select(name=\"Significant height of combined wind waves and swell\")[0]\n\nExtract the data values using the values key (grb.keys() will return a list of the available keys):\n\n# The data is returned as a numpy array, or if missing values or a bitmap\n# are present, a numpy masked array.  Reduced lat/lon or gaussian grid\n# data is automatically expanded to a regular grid. Details of the internal\n# representation of the grib data (such as the scanning mode) are handled\n# automatically.\n\ngrb.values\n\nmasked_array(\n  data=[[--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        ...,\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --]],\n  mask=[[ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n  fill_value=9999.0)\n\n\n\ngrb.values.shape, grb.values.min(), grb.values.max()\n\n((331, 301), 0.0, 6.5600000000000005)\n\n\nGet the latitudes and longitudes of the grid:\n\nlats, lons = grb.latlons()\nlats.shape, lats.min(), lats.max(), lons.shape, lons.min(), lons.max()\n\n((331, 301),\n -0.00010999999961602835,\n 55.0,\n (331, 301),\n 260.0,\n 310.0000000000057)\n\n\nExtract data and get lat/lon values for a subset over North America:\n\ndata, lats, lons = grb.data(lat1=20, lat2=70, lon1=220, lon2=320)\n\ndata.shape, lats.min(), lats.max(), lons.min(), lons.max()\n\n((210, 301), 20.166597000000415, 55.0, 260.0, 310.0000000000057)\n\n\nClose the grib file\n\nhs_grb_obj.close()\n\n\n\n\nTesting docstrings\n\ngrbs = wind_grb_obj\n\ngrbs.seek(0)\n\nu_grb = grbs.select(name=\"U component of wind\")\nv_grb = grbs.select(name=\"V component of wind\")\n\nprint(len(u_grb), len(v_grb))\n\n249 249\n\n\n\nprint(u_grb[0].values.shape, v_grb[0].values.shape)\n\n(331, 301) (331, 301)\n\n\n\ngrbs = wind_grb_obj\ngrbs.seek(0)\ngrb = grbs.select(name=\"U component of wind\")[0]\n\n\ngrb.has_key(\"gaulats\")\n\nFalse\n\n\n\n\npygrib.gribmessage object\n\nprint(\"grb.messagenumber: \", grb.messagenumber)\nprint(\"grb.fcstimeunits: \", grb.fcstimeunits)\nprint(\"grb.analDate: \", grb.analDate)\nprint(\"grb.validDate: \", grb.validDate)\n\ngrb.messagenumber:  1\ngrb.fcstimeunits:  hrs\ngrb.analDate:  2009-01-01 00:00:00\ngrb.validDate:  2009-01-01 00:00:00\n\n\n\n# If the default values of lat1,lat2,lon1,lon2 are None, which means the entire grid is returned\ngrb.data(lat1=None, lat2=None, lon1=None, lon2=None)\n\n(masked_array(\n   data=[[--, --, --, ..., --, --, --],\n         [--, --, --, ..., --, --, --],\n         [--, --, --, ..., --, --, --],\n         ...,\n         [--, --, --, ..., --, --, --],\n         [--, --, --, ..., --, --, --],\n         [--, --, --, ..., --, --, --]],\n   mask=[[ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ...,  True,  True,  True],\n         ...,\n         [ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ...,  True,  True,  True]],\n   fill_value=9999.0),\n array([[ 5.5000000e+01,  5.5000000e+01,  5.5000000e+01, ...,\n          5.5000000e+01,  5.5000000e+01,  5.5000000e+01],\n        [ 5.4833333e+01,  5.4833333e+01,  5.4833333e+01, ...,\n          5.4833333e+01,  5.4833333e+01,  5.4833333e+01],\n        [ 5.4666666e+01,  5.4666666e+01,  5.4666666e+01, ...,\n          5.4666666e+01,  5.4666666e+01,  5.4666666e+01],\n        ...,\n        [ 3.3322400e-01,  3.3322400e-01,  3.3322400e-01, ...,\n          3.3322400e-01,  3.3322400e-01,  3.3322400e-01],\n        [ 1.6655700e-01,  1.6655700e-01,  1.6655700e-01, ...,\n          1.6655700e-01,  1.6655700e-01,  1.6655700e-01],\n        [-1.1000000e-04, -1.1000000e-04, -1.1000000e-04, ...,\n         -1.1000000e-04, -1.1000000e-04, -1.1000000e-04]]),\n array([[260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        ...,\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ]]))\n\n\n\ngrbs = wind_grb_obj\n\ngrbs.seek(0)\n\nu_grb = grbs.select(name=\"U component of wind\")\nv_grb = grbs.select(name=\"V component of wind\")\n\ngrbs = u_grb\n\n\nfor grb in grbs:\n    print(\"grb.messagenumber: \", grb.messagenumber)\n    print(\"grb.fcstimeunits: \", grb.fcstimeunits)\n    print(\"grb.validDate: \", grb.validDate)\n    print(\"type(grb.values): \", type(grb.values))\n    print(grb.values)\n    break\n\ngrb.messagenumber:  1\ngrb.fcstimeunits:  hrs\ngrb.validDate:  2009-01-01 00:00:00\ntype(grb.values):  &lt;class 'numpy.ma.core.MaskedArray'&gt;\n[[-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n ...\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]]"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#masked-arrays",
    "href": "content/development/code/notebooks/grib.html#masked-arrays",
    "title": "GRIB",
    "section": "Masked arrays",
    "text": "Masked arrays\n\n# Masked arrays are arrays that may have missing or invalid entries\n# https://numpy.org/doc/stable/reference/maskedarray.generic.html\n\n# When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked.\n# When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid).\n# The package ensures that masked entries are not used in computations.\n\nimport numpy.ma as ma\n\n\nMasked array example\n\nx = np.array([1, 2, 3, -1, 5])\nprint(x.mean())\n\n# Mark the fourth element as invalid\nmx = ma.masked_array(x, mask=[0, 0, 0, 1, 0])\nprint(mx.mean())\n\n2.0\n2.75\n\n\n\n\nUsage\n\ngrbs = wind_grb_obj\n\ngrbs.seek(0)\n\nu_grb = grbs.select(name=\"U component of wind\")\nv_grb = grbs.select(name=\"V component of wind\")\n\ngrb = u_grb[0]\n\n\n# Test if input is instance of masked array\nma.isMaskedArray(grb.values)\n\nTrue\n\n\n\n# Check if all elements evaluate to True\n# If false, invalid values exist\nma.all(grb.values)\n\nFalse\n\n\n\nprint(ma.count(grb.values))  # non-maked elements\nprint(ma.count_masked(grb.values))  # masked elements\n\n30096\n69535\n\n\n\nma.getdata(grb.values)\n\narray([[9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       ...,\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.]])\n\n\n\nma.ravel(grb.values)  # Returns a 1D version of self, as a view.\n\nmasked_array(data=[--, --, --, ..., --, --, --],\n             mask=[ True,  True,  True, ...,  True,  True,  True],\n       fill_value=9999.0)\n\n\n\nma.ravel(grb.values).fill_value\n\n9999.0\n\n\n\nma.filled(grb.values)\n\narray([[9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       ...,\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.]])\n\n\n\nma.MaskedArray.torecords(grb.values)\n\narray([[(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       ...,\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)]],\n      dtype=[('_data', '&lt;f8'), ('_mask', '?')])\n\n\n\n\nwind_grb_obj.name  # name of object\nwind_grb_obj.messages  # count of messages\nwind_grb_obj.messagenumber  # Current message index\n\n0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This site is designed to be my personal curated toolkit. It is where I store useful information, resources, tools, and code snippets I use on a regular basis."
  },
  {
    "objectID": "content/workouts/index.html",
    "href": "content/workouts/index.html",
    "title": "Workouts",
    "section": "",
    "text": "The following regimen can be used for any 5 day/week workout schedule. I typically workout Monday-Friday, where each lift is executed once Monday-Thursday, and any targeted muscle group is performed again on Friday.\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\nRowerg\n\n\n\n1\n\n\n\n2-5 minutes\n\n\n\nWarmup\n\n\n\n\n\n\n\nDeadlift\n\n\n\n5\n\n\n\n10, 8, 6, 8, 10\n\n\n\nPyramid intensity\n\n\n\n\n\n\n\nPull-Ups\n\n\n\n3\n\n\n\n10\n\n\n\nFailure on last set\n\n\n\n\n\n\n\nLat Pull Downs\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nOne Arm Seated Cable Rows\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nFace Pulls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nBent Over Rows\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nShoulder Superset\n\n\n\n3\n\n\n\n12\n\n\n\nFailure\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nBarbell Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\n60 seconds\n\n\n\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n15\n\n\n\n\n\n\n\n\n\n\nSit Ups\n\n\n\n3\n\n\n\n25\n\n\n\n\n\n\n\n\n\n\nBike\n\n\n\n1\n\n\n\n20-30 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\nBench Press\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\nPyramid intensity\n\n\n\n\n\n\n\nIncline Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nFlys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nDips\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\n\n\n\nTricep Pushdowns\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nDumbbell Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nAb Roller\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nSitting Twists\n\n\n\n3\n\n\n\n30\n\n\n\n\n\n\n\n\n\n\nWipers\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nRun\n\n\n\n1\n\n\n\n6 miles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\nRowerg\n\n\n\n1\n\n\n\n10 minutes\n\n\n\nWarmup\n\n\n\n\n\n\n\nBack Squat\n\n\n\n5\n\n\n\n10, 8, 6, 8, 10\n\n\n\nPyramid intensity\n\n\n\n\n\n\n\nLunges\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nGood Mornings\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nKettelbell Deadlift\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nMixed Cable\n\n\n\n3\n\n\n\n12\n\n\n\nLeg Curl, Side Step, Knee Raise\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n3\n\n\n\n12\n\n\n\nHold for 3 seconds at full extension\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nOne Leg Balance\n\n\n\n3\n\n\n\n45 seconds\n\n\n\nOn Incline\n\n\n\n\n\n\n\nGlute Bridges\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nRowerg\n\n\n\n1\n\n\n\n5 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n\nRowerg\n\n\n\n1.0\n\n\n\n10 minutes\n\n\n\nWarmup\n\n\n\n\n\n\n\nMilitary Press\n\n\n\n4.0\n\n\n\n12, 8, 5, 3\n\n\n\nFailure On Last Set\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n3.0\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nRear Delt Flys\n\n\n\n3.0\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nDumbell Superset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarbell Shrugs\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nOne Arm Dumbbell Snatch\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nKettlebell Swing\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nFarmers Carry\n\n\n\n3.0\n\n\n\nFailure\n\n\n\n\n\n\n\n\n\n\nScissor Kicks\n\n\n\n3.0\n\n\n\n30 seconds\n\n\n\n\n\n\n\n\n\n\nL-sit\n\n\n\n3.0\n\n\n\nFailure\n\n\n\n\n\n\n\n\n\n\nToe Taps\n\n\n\n3.0\n\n\n\n20"
  },
  {
    "objectID": "content/workouts/index.html#intermediate-routines",
    "href": "content/workouts/index.html#intermediate-routines",
    "title": "Workouts",
    "section": "",
    "text": "The following regimen can be used for any 5 day/week workout schedule. I typically workout Monday-Friday, where each lift is executed once Monday-Thursday, and any targeted muscle group is performed again on Friday.\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\nRowerg\n\n\n\n1\n\n\n\n2-5 minutes\n\n\n\nWarmup\n\n\n\n\n\n\n\nDeadlift\n\n\n\n5\n\n\n\n10, 8, 6, 8, 10\n\n\n\nPyramid intensity\n\n\n\n\n\n\n\nPull-Ups\n\n\n\n3\n\n\n\n10\n\n\n\nFailure on last set\n\n\n\n\n\n\n\nLat Pull Downs\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nOne Arm Seated Cable Rows\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nFace Pulls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nBent Over Rows\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nShoulder Superset\n\n\n\n3\n\n\n\n12\n\n\n\nFailure\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nBarbell Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\n60 seconds\n\n\n\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n15\n\n\n\n\n\n\n\n\n\n\nSit Ups\n\n\n\n3\n\n\n\n25\n\n\n\n\n\n\n\n\n\n\nBike\n\n\n\n1\n\n\n\n20-30 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\nBench Press\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\nPyramid intensity\n\n\n\n\n\n\n\nIncline Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nFlys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nDips\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\n\n\n\nTricep Pushdowns\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nDumbbell Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nAb Roller\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nSitting Twists\n\n\n\n3\n\n\n\n30\n\n\n\n\n\n\n\n\n\n\nWipers\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nRun\n\n\n\n1\n\n\n\n6 miles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nNotes\n\n\n\n\n\n\n\n\n\n\n\nRowerg\n\n\n\n1\n\n\n\n10 minutes\n\n\n\nWarmup\n\n\n\n\n\n\n\nBack Squat\n\n\n\n5\n\n\n\n10, 8, 6, 8, 10\n\n\n\nPyramid intensity\n\n\n\n\n\n\n\nLunges\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nGood Mornings\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nKettelbell Deadlift\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nMixed Cable\n\n\n\n3\n\n\n\n12\n\n\n\nLeg Curl, Side Step, Knee Raise\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n3\n\n\n\n12\n\n\n\nHold for 3 seconds at full extension\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nOne Leg Balance\n\n\n\n3\n\n\n\n45 seconds\n\n\n\nOn Incline\n\n\n\n\n\n\n\nGlute Bridges\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nRowerg\n\n\n\n1\n\n\n\n5 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nComments\n\n\n\n\n\n\n\n\n\n\n\nRowerg\n\n\n\n1.0\n\n\n\n10 minutes\n\n\n\nWarmup\n\n\n\n\n\n\n\nMilitary Press\n\n\n\n4.0\n\n\n\n12, 8, 5, 3\n\n\n\nFailure On Last Set\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n3.0\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nRear Delt Flys\n\n\n\n3.0\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nDumbell Superset\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBarbell Shrugs\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nOne Arm Dumbbell Snatch\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nKettlebell Swing\n\n\n\n3.0\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nFarmers Carry\n\n\n\n3.0\n\n\n\nFailure\n\n\n\n\n\n\n\n\n\n\nScissor Kicks\n\n\n\n3.0\n\n\n\n30 seconds\n\n\n\n\n\n\n\n\n\n\nL-sit\n\n\n\n3.0\n\n\n\nFailure\n\n\n\n\n\n\n\n\n\n\nToe Taps\n\n\n\n3.0\n\n\n\n20"
  },
  {
    "objectID": "content/recipes/turkey-meatballs.html",
    "href": "content/recipes/turkey-meatballs.html",
    "title": "Turkey Meatballs",
    "section": "",
    "text": "3 lbs ground turkey\n1 cup breadcrumbs\n1 tsp salt\n1 bunch scallions\n3 eggs\n3 garlic cloves\n1 tbsp worcestershire\n1 tbsp dijon mustard\n1 tbsp paprika\n\n\nMix all ingredients in a large bowl and mix.\nUse an ice cream scoop to scoop out the meatballs onto a parchment lined baking sheet, and bake at 550 degrees for 12 minutes.\nOptionally douse in buffalo sauce and toss."
  },
  {
    "objectID": "content/recipes/rice-pilaf.html",
    "href": "content/recipes/rice-pilaf.html",
    "title": "Rice Pilaf",
    "section": "",
    "text": "1 cup rice\n1/8 cup spaghetti noodle broken into 1/4 inch pieces\n1.5 tbsp butter\n1 tsp chicken bouillon\n2 cups water\n\nHeat the butter in a sauce pan then add rice and noodle. Add boillon to the water and mix to combine then add to sauce pan. Cook on high for 1 minute then cover and simmer for 40 minutes."
  },
  {
    "objectID": "content/recipes/ravioli.html",
    "href": "content/recipes/ravioli.html",
    "title": "Ravioli",
    "section": "",
    "text": "Spinach\nCelery\nYellow onions\nGarlic\nMeats\nMarjoram\nThyme\nJohnnies seasoning\nSeasoned breadcrumbs\nParmesan cheese (granulated)\nRicotta cheese"
  },
  {
    "objectID": "content/recipes/ravioli.html#filling",
    "href": "content/recipes/ravioli.html#filling",
    "title": "Ravioli",
    "section": "",
    "text": "Spinach\nCelery\nYellow onions\nGarlic\nMeats\nMarjoram\nThyme\nJohnnies seasoning\nSeasoned breadcrumbs\nParmesan cheese (granulated)\nRicotta cheese"
  },
  {
    "objectID": "content/recipes/ravioli.html#pasta",
    "href": "content/recipes/ravioli.html#pasta",
    "title": "Ravioli",
    "section": "Pasta",
    "text": "Pasta\n\n3 1/2 cups flour\n2/3 cup water\n3 eggs\n1/4 cup olive oil\n1 tsp salt"
  },
  {
    "objectID": "content/recipes/overnight-protein-oats.html",
    "href": "content/recipes/overnight-protein-oats.html",
    "title": "Overnight Protein Oats",
    "section": "",
    "text": "2 scoops protein powder\n1/2 cup greek yogurt\n1 banana\n1/3 cup berries\n1/2 tbsp flaxseed\n1/3 tbsp chia seed\n1 tbsp honey\nAdd water for desired consistency\n\nBlend until smooth."
  },
  {
    "objectID": "content/recipes/italian-dressing.html",
    "href": "content/recipes/italian-dressing.html",
    "title": "Italian Dressing",
    "section": "",
    "text": "1/2 cup extra virgin olive oil\n1/3 cup apple cider vinegar\n1/4 cup honey, use maple syrup for vegan\n2 teaspoons balsamic vinegar\n1 teaspoon dijon mustard\n2 cloves garlic finely minced\npinch of sea salt"
  },
  {
    "objectID": "content/recipes/egg-protein-bars.html",
    "href": "content/recipes/egg-protein-bars.html",
    "title": "Egg Protein Bars",
    "section": "",
    "text": "1.5 lbs sausage\n15 eggs\n10 button mushrooms\n20 cherry tomatoes\n5 ounce container baby spinach\n2 cups cheddar cheese\nSalt and pepper\n\n\nPreheat your oven to 350 degrees.\nHeat a large sautee pan on high heat and brown the sausage. Add the mushrooms and sautee until browned, about 3 minutes. Add the spinach and cook for an additional minute, until slightly browned, then add sliced cherry tomatoes, stir, and take off the heat.\nWhisk eggs, salt and pepper, and cheese together, then toss in your sausage and vegetables and whisk.\nLine a deep baking dish with parchment paper, then pour egg mixture on top, and bake at 350 degrees for 40 minutes, until cooked.\nRemove from oven, allow to cool for 20 minutes, then turn over, cut into 5 portions, and wrap."
  },
  {
    "objectID": "content/recipes/cougar-gold-mac.html",
    "href": "content/recipes/cougar-gold-mac.html",
    "title": "Cougar Gold Mac & Cheese",
    "section": "",
    "text": "6 cups water\n2 1/2 cups pasta\n2 tbsp butter\n2 tbsp all-purpose flour\n1 cup milk\n1/3 cup heavy cream\n1 1/2 cups Cougar Gold, grated\n1 1/2 cups Crimson Fire, grated\n1/2 cup ricotta\n1/2 tbsp chicken bouillon\nground black pepper, paprika to taste\n1/2 cup fresh breadcrumbs (optional)\n\nPreheat oven to 350° F.\nBring the water, salt and pasta to a rolling boil in a medium saucepan. Cook just until tender. Drain pasta, put into prepared baking dish and set aside. Meanwhile prepare the sauce. In a saucepan over medium-low heat, melt 2 tablespoons of butter. While whisking, gradually add the flour. Whisk for about 2 minutes or until golden and bubbling. Very slowly add the milk & cream, whisking constantly to avoid developing lumps. Simmer for 15 minutes until thickened (alfredo sauce consistency), stirring often to prevent mixture from burning. Remove from heat and stir in Cougar cheese, ricotta, chicken bouillon, paprika and black pepper to taste. Pour sauce onto cooked pasta (do not stir). Optionally top with breadcrumbs tossed in melted butter. Bake for 30 minutes until browned and bubbling."
  },
  {
    "objectID": "content/recipes/cougar-gold-mac.html#cougar-gold-mac-cheese",
    "href": "content/recipes/cougar-gold-mac.html#cougar-gold-mac-cheese",
    "title": "Cougar Gold Mac & Cheese",
    "section": "",
    "text": "6 cups water\n2 1/2 cups pasta\n2 tbsp butter\n2 tbsp all-purpose flour\n1 cup milk\n1/3 cup heavy cream\n1 1/2 cups Cougar Gold, grated\n1 1/2 cups Crimson Fire, grated\n1/2 cup ricotta\n1/2 tbsp chicken bouillon\nground black pepper, paprika to taste\n1/2 cup fresh breadcrumbs (optional)\n\nPreheat oven to 350° F.\nBring the water, salt and pasta to a rolling boil in a medium saucepan. Cook just until tender. Drain pasta, put into prepared baking dish and set aside. Meanwhile prepare the sauce. In a saucepan over medium-low heat, melt 2 tablespoons of butter. While whisking, gradually add the flour. Whisk for about 2 minutes or until golden and bubbling. Very slowly add the milk & cream, whisking constantly to avoid developing lumps. Simmer for 15 minutes until thickened (alfredo sauce consistency), stirring often to prevent mixture from burning. Remove from heat and stir in Cougar cheese, ricotta, chicken bouillon, paprika and black pepper to taste. Pour sauce onto cooked pasta (do not stir). Optionally top with breadcrumbs tossed in melted butter. Bake for 30 minutes until browned and bubbling."
  },
  {
    "objectID": "content/recipes/cauliflower-pizza-crust.html",
    "href": "content/recipes/cauliflower-pizza-crust.html",
    "title": "Cauliflower Pizza Crust",
    "section": "",
    "text": "1 head cauliflower\n1 egg\n1/3 cup soft goat cheese\n1 teaspoon italian seasoning\nSalt & pepper\n\nPreheat oven to 400F. Steam cauliflower until tender. Place the clauliflower in a food processor and process until it has a rice-like texture. Squeeze the excess moisture out of the cauliflower. In a large bowl, combine the cauliflower, egg, goat cheese, and seasonings. Mix until homogonized. Dump the dough onto a baking sheet covered with parchment paper, spreading with a spatula/hands. Bake for 30-35 minutes until dry and golden. Remove from oven, flip the crust, then top with your favorite pizza toppings. Place back in the overn and cook for another 7-10 minutes or until desired."
  },
  {
    "objectID": "content/recipes/bolognese.html",
    "href": "content/recipes/bolognese.html",
    "title": "Bolognese",
    "section": "",
    "text": "2 carrot sticks\n1 lb beef\n3/4 lb pork\n1 cup dried imported porcini mushrooms, soak in water & slice\n1/4 cup basil\n1/8 cup italian seasoning\npinch of nutmeg\n1/4 cup parsley\n4 jars Barilla tomato and basil sauce\n1 sq inch rhind from parmesan ragiano\n1/2 cup red wine\n1 cup beef broth\nOlive oil\n\nPlace beef and pork in food processer and mix until desired consistency. Soak dried porcini mushrooms in water then slice. Place carrots in food processor and mix until fine. In a large sauce pan, cook the carrots with olive oil on low until soft. Add in the cooked beef and pork. Add 1/2 jar of tomato sauce and cook for 5-10 minutes. Add in all the seasonings except nutmeg, cook for 5-10 minutes. Add mushrooms, 1.5 jars sauce, wine, beef broth, and nutmeg, cook for 5-10 minutes. Add the remaining 2 jars of sauce and cook on low."
  },
  {
    "objectID": "content/development/tools/miscellaneous.html",
    "href": "content/development/tools/miscellaneous.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "ffmpeg -i file.mov -s 600x400 -pix_fmt rgb24 -r 20 -f gif - | gifsicle --optimize=3 --delay=3 &gt; file.gif"
  },
  {
    "objectID": "content/development/tools/miscellaneous.html#convert-.mov-to-.gif",
    "href": "content/development/tools/miscellaneous.html#convert-.mov-to-.gif",
    "title": "Miscellaneous",
    "section": "",
    "text": "ffmpeg -i file.mov -s 600x400 -pix_fmt rgb24 -r 20 -f gif - | gifsicle --optimize=3 --delay=3 &gt; file.gif"
  },
  {
    "objectID": "content/development/tools/miscellaneous.html#convert-.svg-to-.ico",
    "href": "content/development/tools/miscellaneous.html#convert-.svg-to-.ico",
    "title": "Miscellaneous",
    "section": "Convert .SVG to .ICO",
    "text": "Convert .SVG to .ICO\nUses the imagemagick utility.\nconvert -background none logo.svg -define icon:auto-size favicon.ico"
  },
  {
    "objectID": "content/development/tools/git.html",
    "href": "content/development/tools/git.html",
    "title": "Git",
    "section": "",
    "text": "https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners\nhttps://docs.gitlab.com/ee/gitlab-basics/start-using-git.html"
  },
  {
    "objectID": "content/development/tools/git.html#initialize",
    "href": "content/development/tools/git.html#initialize",
    "title": "Git",
    "section": "Initialize",
    "text": "Initialize\n\nLaunch Git Bash\nNavigate to project directory\ninitialize git repository in the folder root: git init\ncreate new file in directory: touch filename.extension\nlist files in root: ls\ncheck which files git recognizes: git status"
  },
  {
    "objectID": "content/development/tools/git.html#staging",
    "href": "content/development/tools/git.html#staging",
    "title": "Git",
    "section": "Staging",
    "text": "Staging\nA commit is a record of what files you have changed since the last time you made a commit. Essentially, you make changes to your repo (for example, adding a file or modifying one) and then tell git to put those files into a commit. Commits make up the essence of your project and allow you to go back to the state of a project at any point.\nSo, how do you tell git which files to put into a commit? This is where the staging environment or index come in. When you make changes to your repo, git notices that a file has changed but won’t do anything with it (like adding it in a commit).\nTo add a file to a commit, you first need to add it to the staging environment. To do this, you can use the git add &lt;filename&gt; command.\nOnce you’ve used the git add command to add all the files you want to the staging environment, you can then tell git to package them into a commit using the git commit command. Note: The staging environment, also called ‘staging’, is the new preferred term for this, but you can also see it referred to as the ‘index’.\n\nAdd files to the staging environment: git add filename.extension\nCheck staging environment for new files: git status"
  },
  {
    "objectID": "content/development/tools/git.html#commit-locally",
    "href": "content/development/tools/git.html#commit-locally",
    "title": "Git",
    "section": "Commit Locally",
    "text": "Commit Locally\ngit commit -m \"Your message about the commit\""
  },
  {
    "objectID": "content/development/tools/git.html#branches",
    "href": "content/development/tools/git.html#branches",
    "title": "Git",
    "section": "Branches",
    "text": "Branches\nSay you want to make a new feature but are worried about making changes to the main project while developing the feature. This is where git branches come in.\nBranches allow you to move back and forth between ‘states’ of a project. For instance, if you want to add a new page to your website you can create a new branch just for that page without affecting the main part of the project. Once you’re done with the page, you can merge your changes from your branch into the master branch. When you create a new branch, Git keeps track of which commit your branch ‘branched’ off of, so it knows the history behind all the files.\n\ngit checkout -b &lt;my branch name&gt;\nShow list of branches: git branch"
  },
  {
    "objectID": "content/development/tools/git.html#commit-to-github",
    "href": "content/development/tools/git.html#commit-to-github",
    "title": "Git",
    "section": "Commit to Github",
    "text": "Commit to Github\n\nCreate new repo on GitHub\ngit remote add origin &lt;url produced on github for new repo&gt;\ngit push -u origin [master/main]"
  },
  {
    "objectID": "content/development/tools/git.html#push-a-branch-to-github",
    "href": "content/development/tools/git.html#push-a-branch-to-github",
    "title": "Git",
    "section": "Push a Branch to Github",
    "text": "Push a Branch to Github\ngit push origin &lt;my-new-branch&gt;\nYou might be wondering what that “origin” word means in the command above. What happens is that when you clone a remote repository to your local machine, git creates an alias for you. In nearly all cases this alias is called “origin.” It’s essentially shorthand for the remote repository’s URL. So, to push your changes to the remote repository, you could’ve used either the command: git push git@github.com:git/git.git yourbranchname or git push origin yourbranchname"
  },
  {
    "objectID": "content/development/tools/git.html#pull-request",
    "href": "content/development/tools/git.html#pull-request",
    "title": "Git",
    "section": "Pull Request",
    "text": "Pull Request\nA pull request (or PR) is a way to alert a repo’s owners that you want to make some changes to their code. It allows them to review the code and make sure it looks good before putting your changes on the master branch."
  },
  {
    "objectID": "content/development/tools/git.html#get-changes-on-github",
    "href": "content/development/tools/git.html#get-changes-on-github",
    "title": "Git",
    "section": "Get Changes on Github",
    "text": "Get Changes on Github\ngit pull origin master\ncheck all new commits: git log"
  },
  {
    "objectID": "content/development/tools/git.html#view-differences",
    "href": "content/development/tools/git.html#view-differences",
    "title": "Git",
    "section": "View Differences",
    "text": "View Differences\n\nrun: git diff"
  },
  {
    "objectID": "content/development/tools/git.html#remove-a-branch",
    "href": "content/development/tools/git.html#remove-a-branch",
    "title": "Git",
    "section": "Remove a Branch",
    "text": "Remove a Branch\n\nLocally\ngit branch -d &lt;branch_name&gt;\n\n\nRemote\ngit push &lt;remote_name&gt; --delete &lt;branch_name&gt;"
  },
  {
    "objectID": "content/development/tools/git.html#remove-tracked-filedirectory",
    "href": "content/development/tools/git.html#remove-tracked-filedirectory",
    "title": "Git",
    "section": "Remove tracked file/directory",
    "text": "Remove tracked file/directory\n\nFile\ngit rm --cached &lt;file&gt;\n\n\nDirectory\ngit rm --cahced -r dir/"
  },
  {
    "objectID": "content/development/tools/git.html#pre-commit",
    "href": "content/development/tools/git.html#pre-commit",
    "title": "Git",
    "section": "pre-commit",
    "text": "pre-commit\nPlease make sure to install our pre-commit hooks into your Git workflow. Pre-commit will help keep our code clean and make sure we are following best practices.\n\nInstall pre-commit hooks: python -m pre_commit install --install-hooks\nRun hooks on the entire codebase: python -m pre_commit run --all-files\n\nHooks will run on the current commit snapshot when executing a git commit. Pre-commit hooks allow us to check for potential issues and make sure we are applying standards to our code before pushing to GitHub.\nSee an example .pre-commit-config.yml"
  },
  {
    "objectID": "content/development/tools/git.html#merge",
    "href": "content/development/tools/git.html#merge",
    "title": "Git",
    "section": "Merge",
    "text": "Merge\nThe steps below can be used to merge two branches on your local machine. The braches used in this example are:\n\nmain: The authoritative or “production” code lives in this branch.\ndev: This branch is split from the main branch and a new feature or update is coded with the intent to merge changes back into the main branch.\n\n\nPull main and dev branches so local repo is up to date with the remote.\n\ngit checkout main\ngit pull origin main\ngit checkout dev\ngit pull origin dev\n\nCheckout the main branch so we can merge the dev branch into main\n\ngit checkout main\ngit merge dev\n\n\nCheck the branch status: git status\n\nEvaluate the two files with a conflict (ie. .gitignore and requirements.txt) and reconcile issues, then git add when ready.\nCommit the changes: git commit -m \"merge @tnelson-integral dev branch with main\"\nPush changes to the remote on GitHub: git push origin main\nCheck out the dev branch locally and pull the main branch changes into it so dev can be up-to-date with main\n\ngit checkout dev\ngit pull origin main\ngit push origin dev"
  },
  {
    "objectID": "content/development/server/rstudio-server.html",
    "href": "content/development/server/rstudio-server.html",
    "title": "Rstudio Server",
    "section": "",
    "text": "Install:\nwget https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2022.07.2-576-amd64.deb && \\\nsudo gdebi rstudio-server-2022.07.2-576-amd64.deb && \\\nrm rstudio-server-2022.07.2-576-amd64.deb && \\\nsudo adduser rstudio\nApache config\n&lt;VirtualHost *:80&gt;\n    ServerName &lt;DNS ENTRY&gt;\n    ServerSignature Off\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n\n    ProxyPreserveHost On\n    ProxyPass \"/\" \"http://localhost:8787/\"\n    ProxyPassReverse \"/\" \"http://localhost:8787/\"\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =&lt;DNS ENTRY&gt;\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n&lt;/VirtualHost&gt;\n\n&lt;IfModule mod_ssl.c&gt;\n    &lt;VirtualHost *:443&gt;\n        ServerName &lt;DNS ENTRY&gt;\n        ServerSignature Off\n\n        ErrorLog /var/log/apache2/redirect.error.log\n        LogLevel warn\n\n        ProxyPreserveHost On\n        ProxyPass \"/\" \"http://localhost:8787/\"\n        ProxyPassReverse \"/\" \"http://localhost:8787/\"\n\n        SSLCertificateFile /etc/letsencrypt/live/&lt;DNS ENTRY&gt;/fullchain.pem\n        SSLCertificateKeyFile /etc/letsencrypt/live/&lt;DNS ENTRY&gt;/privkey.pem\n        Include /etc/letsencrypt/options-ssl-apache.conf\n    &lt;/VirtualHost&gt;\n&lt;/IfModule&gt;\nGenerate SSL certs\nsudo certbot --apache certonly\nEnable the site\nsudo a2ensite rstudio.conf && \\\nsudo systemctl reload apache2 && \\\nsudo systemctl status apache2"
  },
  {
    "objectID": "content/development/server/jupyter-server.html",
    "href": "content/development/server/jupyter-server.html",
    "title": "Jupyter Server",
    "section": "",
    "text": "Create jupyter user\nsudo adduser jupyter && \\\nsudo usermod -a -G staff jupyter\nsudo su jupyter && \\\nInstall Jupyter Lab\nsource /home/jupyter/.venv/bin/activate && \\\npython -m pip install jupyterlab && \\\njupyter-lab --generate-config\nConfigure Jupyter\nc.NotebookApp.ip = \"*\"\nc.NotebookApp.notebook_dir = \"/home/jupyter/notebooks/\"\nc.NotebookApp.open_browser = False\nc.NotebookApp.password = \"\"  # hashed password\nc.NotebookApp.port = 9999\nConfigure Apache:\n&lt;VirtualHost *:80&gt;\n    ServerName &lt;DNS ENTRY&gt;\n    ServerSignature Off\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n\n    ProxyPreserveHost On\n    ProxyPass \"/\" \"http://localhost:9999/\"\n    ProxyPassReverse \"/\" \"http://localhost:9999/\"\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =&lt;DNS ENTRY&gt;\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n&lt;/VirtualHost&gt;\n\n&lt;IfModule mod_ssl.c&gt;\n    &lt;VirtualHost *:443&gt;\n        ServerName &lt;DNS ENTRY&gt;\n        ServerSignature Off\n\n        ErrorLog /var/log/apache2/redirect.error.log\n        LogLevel warn\n\n        ProxyPreserveHost On\n        ProxyPass \"/\" \"http://localhost:9999/\"\n        ProxyPassReverse \"/\" \"http://localhost:9999/\"\n\n    &lt;Location \"/api/kernels/\"&gt;\n        ProxyPass        ws://localhost:9999/api/kernels/\n            ProxyPassReverse ws://localhost:9999/api/kernels/\n    &lt;/Location&gt;\n\n        SSLCertificateFile /etc/letsencrypt/live/&lt;DNS ENTRY&gt;/fullchain.pem\n        SSLCertificateKeyFile /etc/letsencrypt/&lt;DNS ENTRY&gt;/privkey.pem\n        Include /etc/letsencrypt/options-ssl-apache.conf\n    &lt;/VirtualHost&gt;\n&lt;/IfModule&gt;\nEnable Apache modules\nsudo a2enmod proxy\nsudo a2enmod proxy_http\nsudo a2enmod proxy_wstunnel\nGenerate SSL certs\nsudo certbot --apache certonly\nEnable the site\nsudo a2ensite jupyter.conf && \\\nsudo systemctl reload apache2 && \\\nsudo systemctl status apache2\nCreate the Jupyter service: /lib/systemd/system/jupyter.service\n# service name:     jupyter.service\n# path:             /lib/systemd/system/jupyter.service\n\n[Unit]\nDescription=Jupyter Notebook Server\n\n[Service]\nType=simple\nPIDFile=/run/jupyter.pid\nExecStart=/bin/bash -c \"/home/jupyter/.venv/bin/jupyter lab --no-browser\"\nUser=jupyter\nGroup=staff\nWorkingDirectory=/home/jupyter/notebooks\nRestart=always\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target\nEnable the service\nsudo systemctl daemon-reload && \\\nsudo systemctl start jupyter.service && \\\nsudo service jupyter status"
  },
  {
    "objectID": "content/development/index.html",
    "href": "content/development/index.html",
    "title": "Best Practices",
    "section": "",
    "text": "General coding best practices are a set of guidelines and recommendations that help developers write clean, efficient, and maintainable code. While specific practices may vary depending on the programming language and the project’s requirements, here are some general coding best practices:\n\nConsistent and meaningful naming: Use descriptive names for variables, functions, classes, and other entities to make the code self-explanatory. Maintain consistency in naming conventions throughout the codebase.\nCode readability: Write code that is easy to read and understand. Use proper indentation, spacing, and formatting. Add comments to explain complex logic or important details.\nModular and reusable code: Break down the code into smaller, logical modules or functions that perform specific tasks. This improves code maintainability, readability, and allows for code reuse.\nDon’t repeat yourself (DRY): Avoid duplicating code. Instead, create reusable functions or abstractions to eliminate redundancy. This reduces the chances of introducing bugs and makes code maintenance easier.\nKeep functions and methods small: Aim for small, focused functions or methods that perform a single task. This improves code readability, testability, and makes it easier to reason about the behavior of the code.\nFollow the Single Responsibility Principle (SRP): Each module, class, or function should have a single responsibility. Separating concerns helps in maintaining and modifying code without impacting other parts of the system.\nError handling and validation: Implement proper error handling and input validation to make the code more robust. Validate user inputs, handle exceptions gracefully, and provide meaningful error messages.\nUse version control: Utilize a version control system (e.g., Git) to track changes to the codebase, collaborate with others, and easily revert or review code changes when needed.\nCode reviews: Encourage peer code reviews to catch bugs, ensure adherence to best practices, and maintain code quality. Reviews provide an opportunity to share knowledge and improve the overall quality of the codebase.\nTesting: Write automated tests to validate the behavior and correctness of the code. Unit tests, integration tests, and other testing techniques help catch bugs early, ensure code stability, and facilitate refactoring.\nPerformance optimization: Optimize code for performance when necessary. Identify bottlenecks, use appropriate data structures and algorithms, and minimize unnecessary computations or operations.\nSecurity considerations: Follow security best practices to protect against common vulnerabilities. Sanitize user inputs, use prepared statements or parameterized queries to prevent SQL injection, and encrypt sensitive data.\nDocumentation: Document the code to provide insights into its functionality, usage, and any specific requirements. Use inline comments, README files, and documentation tools to make it easier for others to understand and use the code. The README is the first file a person encounters when exploring a project, making it crucial to create one and provide comprehensive details to the best of your knowledge. There isn’t a single right way to structure a README, but it should cover some essential topics, including:\n\nThe purpose of the code.\nInstructions for installing the code.\nGuidelines on how to use the code effectively.\nAny other relevant information users need to know about the code.\n\nA well-crafted README fosters collaboration and ensures the project’s long-term development by making it accessible and understandable to a broader audience.\nKeep up with best practices: Stay updated with the latest best practices, coding standards, and programming language conventions. Regularly learn and improve your coding skills to write better code over time.\n\nRemember that best practices may vary depending on the specific programming language, domain, and project requirements. It’s important to adapt and adjust these practices as needed for each situation."
  },
  {
    "objectID": "content/development/index.html#best-practices",
    "href": "content/development/index.html#best-practices",
    "title": "Best Practices",
    "section": "",
    "text": "General coding best practices are a set of guidelines and recommendations that help developers write clean, efficient, and maintainable code. While specific practices may vary depending on the programming language and the project’s requirements, here are some general coding best practices:\n\nConsistent and meaningful naming: Use descriptive names for variables, functions, classes, and other entities to make the code self-explanatory. Maintain consistency in naming conventions throughout the codebase.\nCode readability: Write code that is easy to read and understand. Use proper indentation, spacing, and formatting. Add comments to explain complex logic or important details.\nModular and reusable code: Break down the code into smaller, logical modules or functions that perform specific tasks. This improves code maintainability, readability, and allows for code reuse.\nDon’t repeat yourself (DRY): Avoid duplicating code. Instead, create reusable functions or abstractions to eliminate redundancy. This reduces the chances of introducing bugs and makes code maintenance easier.\nKeep functions and methods small: Aim for small, focused functions or methods that perform a single task. This improves code readability, testability, and makes it easier to reason about the behavior of the code.\nFollow the Single Responsibility Principle (SRP): Each module, class, or function should have a single responsibility. Separating concerns helps in maintaining and modifying code without impacting other parts of the system.\nError handling and validation: Implement proper error handling and input validation to make the code more robust. Validate user inputs, handle exceptions gracefully, and provide meaningful error messages.\nUse version control: Utilize a version control system (e.g., Git) to track changes to the codebase, collaborate with others, and easily revert or review code changes when needed.\nCode reviews: Encourage peer code reviews to catch bugs, ensure adherence to best practices, and maintain code quality. Reviews provide an opportunity to share knowledge and improve the overall quality of the codebase.\nTesting: Write automated tests to validate the behavior and correctness of the code. Unit tests, integration tests, and other testing techniques help catch bugs early, ensure code stability, and facilitate refactoring.\nPerformance optimization: Optimize code for performance when necessary. Identify bottlenecks, use appropriate data structures and algorithms, and minimize unnecessary computations or operations.\nSecurity considerations: Follow security best practices to protect against common vulnerabilities. Sanitize user inputs, use prepared statements or parameterized queries to prevent SQL injection, and encrypt sensitive data.\nDocumentation: Document the code to provide insights into its functionality, usage, and any specific requirements. Use inline comments, README files, and documentation tools to make it easier for others to understand and use the code. The README is the first file a person encounters when exploring a project, making it crucial to create one and provide comprehensive details to the best of your knowledge. There isn’t a single right way to structure a README, but it should cover some essential topics, including:\n\nThe purpose of the code.\nInstructions for installing the code.\nGuidelines on how to use the code effectively.\nAny other relevant information users need to know about the code.\n\nA well-crafted README fosters collaboration and ensures the project’s long-term development by making it accessible and understandable to a broader audience.\nKeep up with best practices: Stay updated with the latest best practices, coding standards, and programming language conventions. Regularly learn and improve your coding skills to write better code over time.\n\nRemember that best practices may vary depending on the specific programming language, domain, and project requirements. It’s important to adapt and adjust these practices as needed for each situation."
  },
  {
    "objectID": "content/development/code/regex.html",
    "href": "content/development/code/regex.html",
    "title": "Regex",
    "section": "",
    "text": "Quickstart\nRegex101"
  },
  {
    "objectID": "content/development/code/regex.html#metacharacters-need-to-be-escaped",
    "href": "content/development/code/regex.html#metacharacters-need-to-be-escaped",
    "title": "Regex",
    "section": "MetaCharacters (Need to be escaped)",
    "text": "MetaCharacters (Need to be escaped)\n. ^ $ * + ? { } [ ] \\ | ( )"
  },
  {
    "objectID": "content/development/code/regex.html#characters",
    "href": "content/development/code/regex.html#characters",
    "title": "Regex",
    "section": "Characters",
    "text": "Characters\n. - Any Character Except New Line \\d - Digit (0-9) \\D - Not a Digit (0-9) \\w - Word Character (a-z, A-Z, 0-9, _) \\W - Not a Word Character \\s - Whitespace (space, tab, newline) \\S - Not Whitespace (space, tab, newline)"
  },
  {
    "objectID": "content/development/code/regex.html#character-classes",
    "href": "content/development/code/regex.html#character-classes",
    "title": "Regex",
    "section": "Character Classes",
    "text": "Character Classes\n[] - Matches Characters in brackets [^ ] - Matches Characters NOT in brackets [a-z] - Any lowercase character between a and z [A-Z] - Any UPPERCASE character between A and Z"
  },
  {
    "objectID": "content/development/code/regex.html#quantifiers",
    "href": "content/development/code/regex.html#quantifiers",
    "title": "Regex",
    "section": "Quantifiers",
    "text": "Quantifiers\n* - 0 or More + - 1 or More ? - 0 or One {3} - Exact Number {3,4} - Range of Numbers (Minimum, Maximum) {3,} - At least 3"
  },
  {
    "objectID": "content/development/code/regex.html#anchors-boundaries",
    "href": "content/development/code/regex.html#anchors-boundaries",
    "title": "Regex",
    "section": "Anchors & Boundaries",
    "text": "Anchors & Boundaries\n\\b - Word Boundary \\B - Not a Word Boundary ^ - Beginning of a String $ - End of a String"
  },
  {
    "objectID": "content/development/code/regex.html#logic",
    "href": "content/development/code/regex.html#logic",
    "title": "Regex",
    "section": "Logic",
    "text": "Logic\n| - Either Or ( ) - Group \\1 - Contents of group 1"
  },
  {
    "objectID": "content/development/code/regex.html#white-space",
    "href": "content/development/code/regex.html#white-space",
    "title": "Regex",
    "section": "White-space",
    "text": "White-space\n\\t - Tab \\r - Carriage return \\n - New line"
  },
  {
    "objectID": "content/development/code/regex.html#snippets",
    "href": "content/development/code/regex.html#snippets",
    "title": "Regex",
    "section": "Snippets",
    "text": "Snippets\n\nMarkdown link pattern\n\\[([^\\]]+)\\]\\(([^\\)]+)\\)"
  },
  {
    "objectID": "content/development/code/html.html",
    "href": "content/development/code/html.html",
    "title": "HTML",
    "section": "",
    "text": "Below is an example of a basic document header that includes local javascript and css document resources, plus hosted Bootstrap and jQuery libraries.\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\" dir=\"ltr\"&gt;\n\n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\" /&gt;\n        &lt;title&gt;Bootstrap 5&lt;/title&gt;\n        &lt;!-- Bootstrap CSS --&gt;\n        &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\" /&gt;\n        &lt;!-- Local assets --&gt;\n        &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"assets/css/main.css\" /&gt;\n        &lt;!-- Favicon --&gt;\n        &lt;link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"\" /&gt;\n        &lt;!-- jQuery --&gt;\n        &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js\"&gt;&lt;/script&gt;\n        &lt;!-- Bootstrap JS and Popper --&gt;\n        &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n        &lt;!-- Local scripts --&gt;\n        &lt;script type=\"text/javascript\" src=\"assets/js/main.js\"&gt;&lt;/script&gt;\n    &lt;/head&gt;\n\n    &lt;body&gt;\n    &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "content/development/code/html.html#document-header",
    "href": "content/development/code/html.html#document-header",
    "title": "HTML",
    "section": "",
    "text": "Below is an example of a basic document header that includes local javascript and css document resources, plus hosted Bootstrap and jQuery libraries.\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\" dir=\"ltr\"&gt;\n\n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\" /&gt;\n        &lt;title&gt;Bootstrap 5&lt;/title&gt;\n        &lt;!-- Bootstrap CSS --&gt;\n        &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\" /&gt;\n        &lt;!-- Local assets --&gt;\n        &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"assets/css/main.css\" /&gt;\n        &lt;!-- Favicon --&gt;\n        &lt;link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"\" /&gt;\n        &lt;!-- jQuery --&gt;\n        &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js\"&gt;&lt;/script&gt;\n        &lt;!-- Bootstrap JS and Popper --&gt;\n        &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n        &lt;!-- Local scripts --&gt;\n        &lt;script type=\"text/javascript\" src=\"assets/js/main.js\"&gt;&lt;/script&gt;\n    &lt;/head&gt;\n\n    &lt;body&gt;\n    &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "content/development/code/bash.html",
    "href": "content/development/code/bash.html",
    "title": "Bash",
    "section": "",
    "text": "sudo mount -t cifs //10.10.145.5/SourceDir /home/&lt;user&gt;/mnt/destination -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "content/development/code/bash.html#mount-a-share",
    "href": "content/development/code/bash.html#mount-a-share",
    "title": "Bash",
    "section": "",
    "text": "sudo mount -t cifs //10.10.145.5/SourceDir /home/&lt;user&gt;/mnt/destination -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "content/development/code/bash.html#generating-an-ssh-key",
    "href": "content/development/code/bash.html#generating-an-ssh-key",
    "title": "Bash",
    "section": "Generating an SSH Key",
    "text": "Generating an SSH Key\ncd ~/.ssh\n# Generate the Key\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Copy th ekey\ncat gh_actions.pub &gt;&gt; ~/.ssh/authorized_keys\n# Copy the key:\ncat ~/.ssh/gh_actions"
  },
  {
    "objectID": "content/data-management/processes/estimating-data-management-efforts.html",
    "href": "content/data-management/processes/estimating-data-management-efforts.html",
    "title": "Estimating Data Management Efforts",
    "section": "",
    "text": "Following is some information and some guidelines for evaluating data management needs for a project. When preparing project budgets, consultation with the project data manager or the DMA Practice Director is recommended."
  },
  {
    "objectID": "content/data-management/processes/estimating-data-management-efforts.html#potential-data-management-tasks",
    "href": "content/data-management/processes/estimating-data-management-efforts.html#potential-data-management-tasks",
    "title": "Estimating Data Management Efforts",
    "section": "Potential Data Management Tasks",
    "text": "Potential Data Management Tasks\nSome or all of the following data management tasks may be needed during a project, and should be considered when developing the project budget.\n\nProject planning\n\nReview of project scope and specification of tasks and budgets.\nIdentification of data needs.\nIdentification of technical requirements.\nSpecification of scopes and budgets for tasks or subtasks.\nReview of field sampling plans.\nDesign of sample identification schemes.\nStatus meetings and updates.\n\n\n\nData acquisition\n\nIdentification of data sources.\nNegotiation of data transfer formats and schedules with clients or other consultants.\nData downloading from online sources.\nAssistance with preparation of field sampling data for loading.\nFiling, inventory initiation and updating, tracking of completion, and status reporting.\n\n\n\nData quality assessment\n\nEvaluation of the referential integrity of acquired data sets.\nSummarization of acquired data in ways to support assessments performed by other technical staff.\n\n\n\nData organization, standardization, and centralization\n\nTechnology assessment (i.e., do we need to use new tools or practices, or develop new or customized database structures?)\nSoftware development (i.e., do we need to create or revise any data management tools?)\nAnalysis of aquired data sets to determine an appropriate relational structure, and possibly the creation and customization of data structures for these data.\nCorrections of errors, ambiguities, conflicts, and missing values in data sets. This may require repeated cycles of issue resolution with the data provider, if available, or additional research and information acquisition.\nTranslation and systematization of codes in acquired data set.\nLoading of field sampling information, possibly including creation and execution of custom scripts.\nLoading of laboratory analytical data, possibly including creation and execution of custom scripts.\nSummarization of data to support data validation, and updating of the database with validation results.\nProduction of data summaries to support quality assurance checks of the standardized data.\n\n\n\nDocument and file management\n\nSetup of a document management system.\nOperational support for ongoing document processing and uploading.\n\n\n\nData summarization, analysis, reporting, and visualization\n\nSetup and periodic execution of standard data summaries.\nDevelopment of custom summaries to support specific project tasks.\nProduction of maps and other GIS products for internal use and for project deliverables.\nProduction of data tables for project deliverables.\nDevelopment of custom summaries for unplanned project requirements.\nSetup and operation of an IWeb interface.\nCreation, setup, and maintenance of Shiny web applications.\n\n\n\nData exchange with clients, agencies, and other consultants\n\nNegotiation of data formats and schedules.\nDevelopment of scripts to produce data in the required formats.\nOngoing operation of the data export/exchange process.\n\n\n\nProject closeout\n\nDocumentation of data status at closeout.\nDatabase archiving.\n\nThese tasks are described in more detail on the pages Data Management Workflow for Sampling Data and Data Management Workflow for Historical Data. Although these workflows are described separately, many projects include both workflows.\nProjects often use a single data management budget, without any subdivision into separate subtasks (e.g., a WBS) or assignment of different billing codes to different subtasks. This approach runs the risk of overlooking subtasks during project planning, and therefore failing to budget for and schedule them. It also limits the ability of the project manager, and data managers, to know whether a particular subtask is taking more or less effort than anticipated. Identification of each data management subtask to be carried out, and creation of a separate billing code for each of them, is recommended. If data management is treated as a single monolithic task, it may also be treated as a black box, such that the project manager and other project staff don’t know much about the machinery inside the box, and that data managers, turning the gears inside the box, don’t see where the boundaries of the box are or how it is connected to other parts of the project.\nCorrection of data quality issues is potentially one of the most labor-intensive efforts, particularly for historical data sets. Because the extent and severity of data quality issues is unknown until efforts are underway to integrate a data set with other data, budgets for data acquisition should allow for this uncertainty, the scope for this task should include assumptions and contingencies to address unexpected types or numbers of data quality issues, and projects should be managed to allow early identification of requirements for unexpectedly high levels of effort or time requirements. Establishing specific budgets and scopes for individual data management tasks, particularly the integration of historical data, allows better tracking and control than having a single budget for all data management activities."
  },
  {
    "objectID": "content/data-management/processes/estimating-data-management-efforts.html#data-acquisition-and-loading-costs",
    "href": "content/data-management/processes/estimating-data-management-efforts.html#data-acquisition-and-loading-costs",
    "title": "Estimating Data Management Efforts",
    "section": "Data Acquisition and Loading Costs",
    "text": "Data Acquisition and Loading Costs\nData loading is frequently a major data management cost, though it can be highly variable because it depends on the amount, format, and quality of data to be loaded. When integrating data from multiple historical sources (as we ordinarily do), data entry encompasses the activities of:\n\nDeveloping an inventory of data sets and their current status.\nAcquisition and filing of the data.\nAnalysis of the format and content of the obtained data to develop a sufficient understanding to standardize and integrate it with other data.\nChecking the data for completeness, consistency, and other data quality issues, possibly including research to obtain missing information and development of rules (e.g., data translations and transformations) to establish consistency.\nDevelopment or customization of scripts to extract, clean and transform, and load (ETL) the data from the original source to the target database, and carrying out QA of those scripts. Scripts allow the ETL process to be easily QA’d and easily revised and re-applied to the same data set or other data sets of a similar format. Scripts also ordinarily (as a matter of our practice) include documentation of the steps carried out, and can be put under version control. The DMA practice maintains a library of scripts for commonly-received data formats (such as EDDs from frequently-used labs), but it is common for projects to receive data in formats for which there is no pre-existing script. In those cases, a custom loading script will need to be developed. If a project receives multiple data sets in the same format, then the data loading effort will be lower after the loading script has been developed. Even so, there may be errors in received data sets that need to be dealt with on a case-by-case basis. Those errors may be recorded in a data issue log.\nCreating additional documentation as necessary for each data set or the data management process as a whole (the latter typically in a data management plan or data manager’s manual).\n\n\n“Tidy datasets are all alike, but every messy dataset is messy in its own way.” –– Hadley Wickham\n\nIf data are not already available in a digital format, and key entry is required, then steps 3 and 4 above would be replaced by the processes to hand-enter the data and QA it, and step 5 would be reduced primarily to loading the data from the staging database, where hand entry is conducted, into the production database.\nData handling, including data entry, is typically a substantial fraction of the effort required when data from multiple sources must be integrated for analysis. Some recent estimates of the relative amount of this effort are presented in the article Ten commandments for good data management on the Dynamic Ecology website (see commandment #4), which cites a figure of 70%, and in the article Data Wrangling: Transforming (1/3) on the R-Bloggers website, which cites a figure of 60-80%. If much hand entry is required, however, that will push the level of effort up relative to obtaining data already in digital format. The Long Right Tail\nThe distribution of time required for loading has a long right tail: A relatively small fraction of data sets may require a disproportionately long time. Loading time can be highly dependent on the format in which the data are available, with time requirements generally increasing for each of these formats:\n\nDatabase tables (e.g., Access) with referential integrity constraints established\nDatabase tables without referential integrity constraints\nExcel and CSV files\nWord document tables\nPDF document tables\n\nData in all formats but the first are very frequently found to have data integrity errors, such as analytical results reported for a sample identifier where there is no sampling information. Data tables in Word and PDF tables are ordinarily in report formats that are designed for reading, and often these do not include information such as coordinate locations, dates, depths, and other details of the sampling and analytical methods–that information may be elsewhere in the document, or may need to be obtained from different sources. Data tables in PDF files must be carefully checked after extraction, if in fact they can even be extracted, because data values can slip from one row or column to another, and repeated or spurious data or other text can appear in the extracted data.\nThe worst-case scenario for data loading is:\n\nData are in tables of a PDF report\nTwo or three extraction tools need to be used to extract the data tables, which then need to be pieced together and thorougly QA’d\nThe data tables do not contain the minimum necessary information (e.g., sampling locations, dates, or depths are missing).\nThe data set contains numerous data integrity errors.\n\nData in spreadsheets and report tables also frequently need to be transformed from a crosstabbed format to a normalized format for data loading.\nA worst-case data set may require several tens of hours to load.\nThe best-case scenario is data from a well-established database format that we have previously obtained data from. For example, we have loaded a lot of data sets from the Washington Department of Ecology’s EIM system, and because the structure of EIM exports is standardized and well known, we have a script to automate the loading process. Development of that script required on the order of 36 hours, but it has been used to load dozens of data sets with multiple types of chemical and biological data. The minumum time ever achieved to download a data set from EIM and load it into a relational database is 20 minutes. This is about the lowest number that could ever be achieved for any data set. Typical times for loading an EIM data set are 60 to 90 minutes. The maximum time to load an EIM data set has been approximately 6 hours. That is because, although EIM’s structure is standardized, EIM does not constrain data submitters to use a consistent set of codes, so different data sets can (and do) use very different codes for chemicals and other things. We have translation tables to convert many of those codes to a standard set of codes, but nevertheless some data sets are so unusual that they require a large effort to standardize.\nWhen projects need to integrate data sets from a variety of source, where the structures are not standardized but vary from one data source to another, the minimum level of effort will be higher, but the overall distribution ordinarily also has a long right tail. One consequence of this is that estimates of data loading effort based on typical or most common costs (i.e., the mode of the distribution) will underestimate the true average and total cost.\n\nBudget Assumptions for Data Loading\nAny or all of the following assumptions may be appropriate to include in a budget estimate. These estimates are intended to protect against unanticipated costs by allowing us to identify those conditions that fall outside the assumptions on which the budget is based.\n\nA database received from another contractor is in the form of a relational database that is normalized to third normal form, that enforces entity, relational, and domain integrity, and that includes clear and complete documentation of the data model. (This may be somewhat jargon-heavy, but it is specific enough that we can clearly identify violations of this assumption.)\nHistorical data sets about which nothing is known are assumed to require:\n\nSupplementary investigation to understand ambiguous and conflicting codes and identifiers.\nStandardization of codes to establish domain integrity to allow accurate representation of the data and reliable and efficient data selection and summarization.\nRestructuring to clearly distinguish locations, sample collections, interpretive samples, analytical samples, laboratory samples, laboratory analyses, and laboratory replicates.\nDocumentation of all of the issues encountered during these steps, and the resolution of each issue, including caveats for data users regarding any issues that could not be resolved.\n\n\n\n\nLoading Data from Laboratory Data Packages\nIntegral has established an electronic data deliverable (EDD) format for analytical laboratories to use when the data will ultimately be incorporated into the database. A number of labs can produce in this data format, although the quality of their adherence to the requirements varies. Generally, use of this custom EDD format is the most cost-effective way of loading laboratory data. We also have data loading scripts for other formats, including the EqUIS 3-file format and California’s EDF format. Other formats may require the development of new custom loading scripts, the cost of which should be included in the budget of the project that uses those other formats.\nFollowing are some guidelines for the minimum, likely, and maximum effort estimates, in hours, for steps involved in loading of laboratory data.\n\nScripting loading of data from a new lab EDD format\n\n\n\nConfidence\nEstimate (hours)\n\n\n\n\nMinimum\n4\n\n\nMaximum\n16\n\n\nMost likely\n10\n\n\n\n\n\nScripting loading of validation results\n\n\n\nConfidence\nEstimate (hours)\n\n\n\n\nMinimum\n2\n\n\nMaximum\n8\n\n\nMost likely\n5\n\n\n\n\n\nLoading of each lab EDD (including receipt, filing, running the script, documentation, notification, and anything else)\n\n\n\nConfidence\nEstimate (hours)\n\n\n\n\nMinimum\n0.25\n\n\nMaximum\n8\n\n\nMost likely\n0.8\n\n\n\n\n\nLoading of each set of validation results\n\n\n\nConfidence\nEstimate (hours)\n\n\n\n\nMinimum\n0.25\n\n\nMaximum\n6\n\n\nMost likely\n0.4"
  },
  {
    "objectID": "content/data-management/processes/data-management-workflow-for-historical-data.html",
    "href": "content/data-management/processes/data-management-workflow-for-historical-data.html",
    "title": "Data Management Workflow for Historical Data",
    "section": "",
    "text": "This page describes data management processes and data flows for projects (or project tasks) that include the compilation of data obtained or received from other sources. The other sources may include literature, clients, other consultants, and websites. Data from such sources is referred to here as historical data, to distinguish it from data that are collected by Integral. An alternative workflow for data collected by Integra is described on the companion wiki page Data Management Workflow for Sampling Data.\nData management processes and data flows are shown in the following data flow diagram (DFD). Each of the processes and data flows are described in following sections. This DFD uses Yourdon/DeMarco symbology, as described by Wikipedia. Note that the arrows in the DFD do not represent sequence or control flow, as in a flowchart. A DFD is more useful than a flowchart for a data-based workflow. The contrast between flowcharts and DFDs is briefly discussed on the page Data Management Workflows for Sampling Data."
  },
  {
    "objectID": "content/data-management/processes/data-management-workflow-for-historical-data.html#processes",
    "href": "content/data-management/processes/data-management-workflow-for-historical-data.html#processes",
    "title": "Data Management Workflow for Historical Data",
    "section": "Processes",
    "text": "Processes\n\nP1. Data Management Planning\nThis process should ordinarily be conducted as part of project planning. It should be carried out jointly by the project manager and the project data manager. This process may include:\n\nIdentification of data needed for analysis and reporting based on project goals (the DQO process).\nIdentification of data needed for export, if data are to be transmitted to other organizations.\nCompletion of a project initiation checklist.\nDevelopment of a scope description for data management activities.\nIdentification of the data model needed\nIdentification of sources of historical data.\nDetermination of the coordinate reference system and basemap data to be used.\nBudgeting for data management activities.\nDevelopment of a Data Management Plan.\nInitiation of a project database and GIS project.\n\nOther activities may be conducted and products produced if the project will require other types of data, such as newly-collected sampling data.\n\n\nP11. Data Usability Assessment\nPrior to conducting statistical, geospatial, or other analyses, the usability of the selected data should be assessed. This process should be carried out by the data analyst and project data manager, depending on the type of assessment to be performed and the best tool or technology for the job. The data usability assessment may make use of descriptive and status information in the data set inventory, and metrics for the different dimensions of data quality. This process ordinarily starts with the production of appropriate data summaries by the project data manager.\n\n\nP12. Data Analysis and Visualization\nThis process should be carried out primarily by the data analyst, but the first step in analyses will frequently be the selection, extaction, and summarization of data from the database, and that step may be carried out by the project data manager. The Data Accessibility page describes a variety of different ways in which data can be accessed and summarized by either data managers or other project staff. A number of standard data summaries are available that meet many common data needs. These standard summaries may be used as is, or customized. More specialized data summaries may need to be created to support some analyses. Often data summarization could be carried out using either SQL or analysis software (e.g., R, Python, Julia), and the most appropriate approach should be identified through discussions between data analysts and data managers.\nSummaries of chemistry data should use the default rules for summarization of chemistry data or explicitly specify alternate data handling rules. Additional information on data summarization (primarily for data managers) is presented on the pages Producing Custom Data Summaries and Guidance and Tips for Data Managers. Custom scripts have also been developed to prepare data for some types of specialized analyses (e.g., unmixing, HCPCA, and t-SNE). Otherwise, custom data summarization scripts may need to be developed for particular analyses (if so, a budget for this effort should have been developed during the planning process).\nData analyses should follow relevant recommendations on Data Analysis Standards and Guidance and Best Practices for Data Analysis, and avoid the Worst Practices for Data Analysis. An initial stage of data exploration may be facilitated by using some of the available GUI Tools for Data Exploration.\nAlthough data analysis and visualization is described here as a single process, frequently it is a multi-step process that could could be described by its own flowchart, data flow diagram, work breakdown structure, or other type of process map. Describing and documenting the planned data analysis process during project planning, and accounting for variances, can help to ensure that this work is carried out in a way that clearly supports project goals and is performed in an orderly and efficient fashion.\n\n\nP13. Data Table Production\nThis process is carried out by the project data manager. Data to be reported as a project deliverable are extracted from the project database and restructured and formatted as appropriate. This process is ordinarily scripted. The wiki page Producing Data Summaries describes the major steps in this process. The section “Producing Data Tables for Reports or for Data Analyses” on the wiki page Guidance and Tips for Data Managers contains guidelines for the production of report data tables.\n\n\nP14. Data Reporting\nThis process is carried out cooperatively by multiple project staff. Reports may include data tables, graphics, and maps prepared by DMA staff and others. This process ordinarily requires that more effort be applied to consistency of data representation and adherence to publication standards than do the data analysis and visualization activities.\n\n\nP15. Identify Historical Data Sets\nThis process may be carried out by any project staff. It may require literature searches, online searches, and contacts with clients, agencies, and other consultants. The results of this process are ordinarily compiled into a data set inventory that includes a unique identifier for each data set, a general description of the data set, additional information on characteristics of particular interest to the projects, informatoin on data set availability, and indicators of the assessed value of the data set and whether or not it has been acquired and compiled with other data sets. The data set inventory may be maintained in a spreadsheet or a database.\n\n\n\nP16. Evaluate Value of Data Set\nThis process may be carried out by any project staff. The value of a data set may be evaluted at several different times during a project:\n\nWhen a data set is first identified, based on its description and availability. After a data set is obtained, based on its actual content and format.\nDuring the data loading process, based on the type and number of data quality issues that are found.\nAfter a data set has been loaded, based on the support it provides for project goals. There may be overlap or interactions between this assessment and the data usability assessments that are conducted prior to individual data analyses.\n\nThe data set evaluation checklist, or a customized version of the checklist, may be useful for carrying out this process. Systematic analysis of data quality issues in a data set may require the expertise of a data manager. The data set inventory should be updated, as appropriate, with the results of this evaluation.\nData quality issues that are identified during this assessment ordinarily should be tracked in a data issue log.\n\n\nP17. Acquire Historical Data Sets\nThis process may be carried out by any project staff. Each data set should be filed in the project data store–typically the “Data” subdirectory under the project directory for tabular data. The data set inventory should be updated when a data set is acquired.\n\n\nP18. Load Data Set\nThis process is carried out by the project data manager. The data set is loaded into the project database, and data quality checks, and any necessary cleaning and transformations of the data, are carried out as part of this process. This includes resolution of any data issues that have been identified. The standard procedure is to carry out this process using a SQL script. If data set is in a format that has been previously used, then an existing loading script may be used or customized; if not, a new script will have to be developed (and if so, a budget for this effort should have been established during the planning process).\n\n\nP19. Address Data Issues\nThis process may be carried out by various project staff, as appropriate to their knowledge and skills. Data issues that have been identified during initial data set review or during the data loading proocess (which may be interrupted when issues are found) may be addressed in a number of different ways. This process may require collection of additional information, application of assumptions and default values, or even a decision to omit some data. One or more resolutions may be proposed, and then reviewed and evaluated, before a final resolution is accepted. Some data issues may be unresolvable. Data issue resolutions should be documented in the data issue log and then used during the data loading process.\n\n\nP20. Correct Data Quality Issues\nInitiation of data analyses, and the associated data usability assessment (P11) may reveal data quality issues that were not identified during initial data compilation and loading. These issues are typically identified by a data analyst, and resolution of an issue may involve data managers, data analysts, field staff, and the project manager. These issues may be recorded in the data issue log (this interaction is not shown in the DFD above).\n\n\nP21. Export Data\nThis process may not be required for all projects. When data must be exported to a regulator or some other organization, the data ordinarily must be retrieved from the project database and prepared in a specified data exchange format or EDD. The EDD format that is used for data exchange between organizations is typically different from the EDD format in which laboratory results are delivered. The data exchange EDD format may be specified by the external organization or may be determined by a negotiation between Integral and that organization. Some external organizations may require data that Integral would not typically record (e.g., during field work), and so export data requirements must be considered during project and data management planning. This task is ordinarily carried out by data managers, but may require some support from other project staff.\nExporting data should be carried out following the process described on the wiki page Producing Data Summaries. Requests for data that are to be exported for others may be recorded in a request tracker."
  },
  {
    "objectID": "content/data-management/processes/data-management-workflow-for-historical-data.html#data-flows",
    "href": "content/data-management/processes/data-management-workflow-for-historical-data.html#data-flows",
    "title": "Data Management Workflow for Historical Data",
    "section": "Data Flows",
    "text": "Data Flows\nSee the page Workflows for Sampling Data for descriptions of data flows not listed below. The sections below describe only the data flows that are specific to historical data.\n\nF25. Data Sources\nThis data flow consists of the identities of all documents, websites, organizations, and individuals who are actual or potential sources of historical data sets or other information (e.g., metadata or additional data sources) about historical data sets.\n\n\nF26. Data Set Description and Status Information\nThis data flow consists of a unique identifier for each historical data set, a description of the data that it contains (e.g., spatial extent, dates, materials, chemicals), information about its location and availability, and whether it has been acquired, reviewed for usability, and incorporated into the project database.\n\n\nF27. Data Set Description\nThis data flow consists of information sufficient to evaluate the usability of each historical data set, including information such as its spatial extent, temporal extent, materials sampled and analyzed, and chemicals analyzed.\n\n\nF28. Data Set Status\nThis data flow consists of information on whether a historical data set is usable and should be acquired (if it has not been already) and incorporated into the project database. It may also indicate relative value or priority of a data set.\n\n\nF29. Data Set Description\nThis data flow consist of information about a historical data set sufficient to obtain that data set from the data source.\n\n\nF30. Data Set Status\nThis data flow consists of information about the results of efforts to obtain a historical data set. This may indicate that the data set is not available, will be received at a future date, or has been obtained and is filed in a specific location (e.g., in the project data library).\n\n\nF31. Historical Data Sets\nThis data flow consists of all historical data sets that are obtained, including any associated metadata documents, transmittal documentation, and the identifier for that data set that is used in the data set inventory.\n\n\nF32. Data Set Characteristics\nThis data flow consists of information produced from a review of a data set, such as detailed information on locations sampled and measurements made, as needed to evaluate the data set’s usability.\n\n\nF33. Data Set\nThis data flow consists of all data and metadata for a historical data set that will be loaded to the project database or that is needed to support data loading.\n\n\nF34. Standardized Data\nThis data flow consists of historical data that have been transformed into the structure of the project database and that conforms to other requirements such as a consistent set of valid values. This data flow is electronic and occurs between the data loading script and the database server.\n\n\nF35. Data Set Status\nThis data flow consists of information that characterizes the completion of data loading. This may consist of information such as ‘loading complete’, ‘loading paused pending resolution of data issues’, and ‘partially loaded’.\n\n\nF36. Compiled Data Characteristics\nThis data flow consists of characteristics of any data sets in the project database that may be useful to assess the value of these or any other historical data sets.\n\n\nF37. Data Issues\nThis data flow consists of a description, priority, and supporting data for any data issue that is identified during evaluation o fthe usability of the data set. This information may include a proposed or accepted resolution of the issue.\n\n\nF38. Data Issues\nThis data flow consists of a description, priority, and supporting data for any data issue that is identified during an attempt to load a data set. This infomation may include a proposed or accepted resolution of the issue.\n\n\nF39. Data Issues\nThis data flow consists of all information in the issue log pertaining to unresolved data issues.\n\n\nF40. Data Issue Resolutions\nThis data flow consists of proposed or accepted resolutions for data issues.\n\n\nF41. Data Issue Resolutions\nThis data flow consis of accepted resolutions for data issues."
  },
  {
    "objectID": "content/data-management/guidance-and-tips.html",
    "href": "content/data-management/guidance-and-tips.html",
    "title": "Guidance and Tips",
    "section": "",
    "text": "EPA Guidance on Systematic Planning Using the Data Quality Objectives Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElement\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nOrganization\n\n\n\nIdentification and involvement of the project manager, sponsoring organization and responsible official, project personnel, stakeholders, scientific experts, etc. (e.g., all customers and suppliers).\n\n\n\n\n\n\n\nProject Goal\n\n\n\nDescription of the project goal, objectives, and study questions and issues.\n\n\n\n\n\n\n\nSchedule\n\n\n\nIdentification of project schedule, resources (including budget), milestones, and any applicable requirements (e.g., regulatory requirements, contractual requirements).\n\n\n\n\n\n\n\nData Needs\n\n\n\nIdentification of the type of data needed and how the data will be used to support the project’s objectives.\n\n\n\n\n\n\n\nCriteria\n\n\n\nDetermination of the quantity of data needed and specification of performance criteria for measuring quality.\n\n\n\n\n\n\n\nData Collection\n\n\n\nDescription of how and where the data will be obtained (including existing data) and identification of any constraints on data collection.\n\n\n\n\n\n\n\nQuality Assurance (QA)\n\n\n\nSpecification of needed QA and quality control (QC) activities to assess the quality performance criteria (e.g., QC samples for both field and laboratory, audits, technical assessments, performance evaluations, etc.).\n\n\n\n\n\n\n\nAnalysis\n\n\n\nDescription of how the acquired data will be analyzed (either in the field or the laboratory), evaluated (i.e., QA review/verification/validation), and assessed against its intended use and the quality performance criteria.\n\n\n\n\n\n\n\n\n\n\n\nWhen specifying the project goal (element #2 in Table 1), a key activity is to determine the key questions which the study will address once data and information are properly collected and analyzed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nSoundness\n\n\n\nThe extent to which the scientific and technical procedures, measures, methods or models employed to generate the information are reasonable for, and consistent with, the intended application.\n\n\n\n\n\n\n\nApplicability and Utility\n\n\n\nThe extent to which the information is relevant for the Agency’s intended use.\n\n\n\n\n\n\n\nClarity and Completeness\n\n\n\nThe degree of clarity and completeness with which the data, assumptions, methods, quality assurance, sponsoring organizations and analyses employed to generate the information are documented.\n\n\n\n\n\n\n\nUncertainty and Variability\n\n\n\nThe extent to which the variability and uncertainty (quantitative and qualitative) in the information or the procedures, measures, methods or models are evaluated and characterized.\n\n\n\n\n\n\n\nEvaluation and Review\n\n\n\nThe extent of independent verification, validation, and peer review of the information or of the procedures, measures, methods or models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchieving clarity in a project’s development becomes straightforward when using systematic planning, as almost every element of the planning process contributes to understanding how the project’s assumptions, methods, and proposed analyses will be conducted.\nPlanning for analyzing the data and information before collection clearly meets the intent of the GAFs.\nClear statements on the goals of the project developed through systematic planning leads to a better understanding of purpose and credibility of the results.\nSystematic planning leads to a clear statement of information needs and how the information will be collected, and leads to transparency in data quality."
  },
  {
    "objectID": "content/data-management/guidance-and-tips.html#data-quality",
    "href": "content/data-management/guidance-and-tips.html#data-quality",
    "title": "Guidance and Tips",
    "section": "",
    "text": "EPA Guidance on Systematic Planning Using the Data Quality Objectives Process\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nElement\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nOrganization\n\n\n\nIdentification and involvement of the project manager, sponsoring organization and responsible official, project personnel, stakeholders, scientific experts, etc. (e.g., all customers and suppliers).\n\n\n\n\n\n\n\nProject Goal\n\n\n\nDescription of the project goal, objectives, and study questions and issues.\n\n\n\n\n\n\n\nSchedule\n\n\n\nIdentification of project schedule, resources (including budget), milestones, and any applicable requirements (e.g., regulatory requirements, contractual requirements).\n\n\n\n\n\n\n\nData Needs\n\n\n\nIdentification of the type of data needed and how the data will be used to support the project’s objectives.\n\n\n\n\n\n\n\nCriteria\n\n\n\nDetermination of the quantity of data needed and specification of performance criteria for measuring quality.\n\n\n\n\n\n\n\nData Collection\n\n\n\nDescription of how and where the data will be obtained (including existing data) and identification of any constraints on data collection.\n\n\n\n\n\n\n\nQuality Assurance (QA)\n\n\n\nSpecification of needed QA and quality control (QC) activities to assess the quality performance criteria (e.g., QC samples for both field and laboratory, audits, technical assessments, performance evaluations, etc.).\n\n\n\n\n\n\n\nAnalysis\n\n\n\nDescription of how the acquired data will be analyzed (either in the field or the laboratory), evaluated (i.e., QA review/verification/validation), and assessed against its intended use and the quality performance criteria.\n\n\n\n\n\n\n\n\n\n\n\nWhen specifying the project goal (element #2 in Table 1), a key activity is to determine the key questions which the study will address once data and information are properly collected and analyzed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFactor\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nSoundness\n\n\n\nThe extent to which the scientific and technical procedures, measures, methods or models employed to generate the information are reasonable for, and consistent with, the intended application.\n\n\n\n\n\n\n\nApplicability and Utility\n\n\n\nThe extent to which the information is relevant for the Agency’s intended use.\n\n\n\n\n\n\n\nClarity and Completeness\n\n\n\nThe degree of clarity and completeness with which the data, assumptions, methods, quality assurance, sponsoring organizations and analyses employed to generate the information are documented.\n\n\n\n\n\n\n\nUncertainty and Variability\n\n\n\nThe extent to which the variability and uncertainty (quantitative and qualitative) in the information or the procedures, measures, methods or models are evaluated and characterized.\n\n\n\n\n\n\n\nEvaluation and Review\n\n\n\nThe extent of independent verification, validation, and peer review of the information or of the procedures, measures, methods or models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchieving clarity in a project’s development becomes straightforward when using systematic planning, as almost every element of the planning process contributes to understanding how the project’s assumptions, methods, and proposed analyses will be conducted.\nPlanning for analyzing the data and information before collection clearly meets the intent of the GAFs.\nClear statements on the goals of the project developed through systematic planning leads to a better understanding of purpose and credibility of the results.\nSystematic planning leads to a clear statement of information needs and how the information will be collected, and leads to transparency in data quality."
  },
  {
    "objectID": "content/data-management/guidance-and-tips.html#benefits-of-using-the-dqo-process",
    "href": "content/data-management/guidance-and-tips.html#benefits-of-using-the-dqo-process",
    "title": "Guidance and Tips",
    "section": "Benefits of Using the DQO Process",
    "text": "Benefits of Using the DQO Process\n\nThe interaction amongst a multidisplinary team results in a clear understanding of the problem and the options available. Organizations that have used the DQO Process have found the structured format facilitated good communicaitons, documentation, and data collection design, all of which facilitated rapid peer review and approval.\n\n\nThe structure of the DQO Process provides a convenient way to document activities and decisions and to communicate the data collection design to others.\nThe DQO Process is an effective planning tool that can save resources by making data collection operations more resource-effective.\nThe DQO Process enables data users and technical experts to participate collectively in planning and to specify their needs prior to data collection. The DQO Process helps to focus studies by encouraging data users to clarify vague objectives and document clearly how scientific theory motivating this project is applicable to the intended use of the data.\nThe DQO Process provides a method for defining performance requirements appropriate for the intended use of the data by considering the consequences of drawing incorrect conclusions and then placing tolerable limits on them.\nThe DQO Process encourages good documentation for a model-based approach to investigate the objectives of a project, with discussion on how the key parameters were estimated or derived, and the robustness of the model to small perturbations. Upon implementing the DQO Process, your environmental programs can be strengthened"
  },
  {
    "objectID": "content/data-management/guidance-and-tips.html#chemistry",
    "href": "content/data-management/guidance-and-tips.html#chemistry",
    "title": "Guidance and Tips",
    "section": "Chemistry",
    "text": "Chemistry\n\nResources\n\nHazardous Waste Test Methods\nNational Environmental Methods Index\nSubstance Registry Service\nEIM Valid Values\nVerification and Validation\nQualifiers\nData Review\nChemical Lists\nPCBs\nWashington Water Resources Data Defs\nMeasurement Basis Conversions\nhttps://pubs.usgs.gov/wdr/WDR-WA-03-1/pdf/ADR_N.pdf\nhttp://www.eccsmobilelab.com/resources/literature/?Id=117\nConversions\n\n\n\nTEFs\n\nVan den Berg source document\nVan den Berg TEF table\n\n\n\nChemical Groups\n\nDioxin & Furans\nReference\n\n\nPCBs\nLearn about PCBs\n\nGeneral\nPCBs are a group of man-made organic chemicals consisting of carbon, hydrogen and chlorine atoms. The number of chlorine atoms and their location in a PCB molecule determine many of its physical and chemical properties. PCBs have no known taste or smell, and range in consistency from an oil to a waxy solid.\nPCBs belong to a broad family of man-made organic chemicals known as chlorinated hydrocarbons. PCBs were domestically manufactured from 1929 until manufacturing was banned in 1979. They have a range of toxicity and vary in consistency from thin, light-colored liquids to yellow or black waxy solids. Due to their non-flammability, chemical stability, high boiling point and electrical insulating properties, PCBs were used in hundreds of industrial and commercial applications including:\n\nElectrical, heat transfer and hydraulic equipment\nPlasticizers in paints, plastics and rubber products\nPigments, dyes and carbonless copy paper\nOther industrial applications\n\nCommercial Uses for PCBs\nAlthough no longer commercially produced in the United States, PCBs may be present in products and materials produced before the 1979 PCB ban. Products that may contain PCBs include:\n\nTransformers and capacitors\nElectrical equipment including voltage regulators, switches, re-closers, bushings, and electromagnets\nOil used in motors and hydraulic systems\nOld electrical devices or appliances containing PCB capacitors\nFluorescent light ballasts\nCable insulation\nThermal insulation material including fiberglass, felt, foam, and cork\nAdhesives and tapes\nOil-based paint\nCaulking\nPlastics\nCarbonless copy paper\nFloor finish\n\nThe PCBs used in these products were chemical mixtures made up of a variety of individual chlorinated biphenyl components known as congeners. Most commercial PCB mixtures are known in the United States by their industrial trade names, the most common being Arochlor.\nRelease and Exposure of PCBs\nToday, PCBs can still be released into the environment from:\n\nPoorly maintained hazardous waste sites that contain PCBs\nIllegal or improper dumping of PCB wastes\nLeaks or releases from electrical transformers containing PCBs\nDisposal of PCB-containing consumer products into municipal or other landfills not designed to handle hazardous waste\nBurning some wastes in municipal and industrial incinerators\n\nPCBs do not readily break down once in the environment. They can remain for long periods cycling between air, water and soil. PCBs can be carried long distances and have been found in snow and sea water in areas far from where they were released into the environment. As a consequence, they are found all over the world. In general, the lighter the form of PCB, the further it can be transported from the source of contamination.\nPCBs can accumulate in the leaves and above-ground parts of plants and food crops. They are also taken up into the bodies of small organisms and fish. As a result, people who ingest fish may be exposed to PCBs that have bioaccumulated in the fish they are ingesting.\nThe National Center for Health Statistics, a division of the Centers for Disease Control and Prevention, conducts the National Health and Nutrition Examination Surveys (NHANES). NHANES is a series of U.S. national surveys on the health and nutrition status of the noninstitutionalized civilian population, which includes data collection on selected chemicals. Interviews and physical examinations are conducted with approximately 10,000 people in each two-year survey cycle. PCBs are one of the chemicals where data are available from the NHANES surveys.\n\n\nPCB Congeners\nA PCB congener is any single, unique well-defined chemical compound in the PCB category. The name of a congener specifies the total number of chlorine substituents, and the position of each chlorine. For example: 4,4’-Dichlorobiphenyl is a congener comprising the biphenyl structure with two chlorine substituents - one on each of the #4 carbons of the two rings. In 1980, a numbering system was developed which assigned a sequential number to each of the 209 PCB congeners.\n\n\nPCB Homologs\nHomologs are subcategories of PCB congeners that have equal numbers of chlorine substituents. For example, the tetrachlorobiphenyls are all PCB congeners with exactly 4 chlorine substituents that can be in any arrangement.\n\n\nPCB Aroclor\nAroclor is a PCB mixture produced from approximately 1930 to 1979. It is one of the most commonly known trade names for PCB mixtures. There are many types of Aroclors and each has a distinguishing suffix number that indicates the degree of chlorination. The numbering standard for the different Aroclors is as follows:\n\nThe first two digits usually refer to the number of carbon atoms in the phenyl rings (for PCBs this is 12)\nThe second two numbers indicate the percentage of chlorine by mass in the mixture. For example, the name Aroclor 1254 means that the mixture contains approximately 54% chlorine by weight.\n\n\n\n\n\nQualifiers\n\nLabs may apply whatever flags they want to a result. Some data qualifiers are defined by EPA’s Functional Guidelines documents, which describe how data validation is to be conducted, and the use and interpretation of U, J, and R qualifiers is pretty universal (but older standards for Puget Sound data used E instead of J). Because the U, J, and R qualifiers are pretty universal and have implications for data usability, they are the only ones that are represented as Boolean fields in the meas_value column. All lab flags are put into the lab_flags column, and there is no lookup table for them, and there is no defined use for them. Similarly, the validator qualifiers (U, J, R, and possibly others) are put in the validator_flags column. If any of those three common qualifiers is in the validator_flags column, then the corresponding flags in meas_value should be set. The lab_conc_qual column is something of a relic, left over from the days when data were commonly provided in EPA’s Contract Laboratory Program (CLP) data format, which had a corresponding column. The lab_conc_qual column was meant to either contain “U” or be null. We don’t ordinarily use that column any more. Of the qualifiers you listed above, other than U, J, and R, N is commonly used to flag a tentatively identified compound, which means that the analyte code itself is uncertain. The d_labresult.tic column is meant to hold that information. The tic column is not part of the measurement_result data type because it is not used in any way during data aggregation (averaging or summing). I see that Jerry added other flags and qualifiers to the e_concqual table, but needn’t—really shouldn’t—be there. The e_concqual dictionary should have only “U” defined. It may seem odd to define a lookup table for only one value when a check constraint on the concentration qualifier columns could be used instead, but it’s easier to check relational constraints than to check check constraints programmatically.\n\n\n\nDuplicates\n“Duplicate” is a somewhat ambiguous term, but in practice it most commonly refers to field duplicates, which we ordinarily refer to as splits to avoid that ambiguity. Some QC data, particularly spikes, are frequently duplicated, so when we have lab QC data we may have values for spikes and spike duplicates. When we receive lab results in one big flat table that includes both analytical results for natural samples and results of lab QC samples, the word or code “DUP” in a column header or table cell could mean a couple of different things. Without seeing the original data source, I’m not sure where the “DUP” code in the “labqc_samp” column of your “d_labsample” table came from. I’m going to assume that it refers to a field duplicate, and not a spike duplicate.\nIdeally, samples are submitted to the laboratory “blind” so that the laboratory does not know which field samples are duplicates of one another. This is to prevent them from seeing that there’s a lot of variation between some pair of duplicates and deciding to re-run one or both of them. If the lab is producing highly variable data, we don’t want them to be able to hide it. Unfortunately, many field sampling programs use a suffix of “-D” or “-DUP” or something like that on the sample ID, so the lab knows which samples are field duplicates. If they know, they may pass that information back in their EDD.\nAlthough field duplicates are used as a QC check on laboratory performance, they are not lab QC samples themselves. They are just normal field samples (which have been split), and don’t need to have a laboratory QC sample ID assigned to them. Thus, field duplicates should not be listed in the “d_labqcsamp” table, so that table looks fine as it appears below. The same is true for the “d_labresult” table.\nThere are a couple of things to be changed about the “d_labsample” table as shown below:\n\nThe values in the “labqc_samp” column should be identifiers that appear in the “d_labqcsamp” table, not codes. The codes for the lab QC type should be in the d_labqcsamp.qc_type column, and neither “Natural” nor “DUP” should be used there.\nThe “d_labsample” table should have values in the “study_id” and “sample_no” columns, or a value in the “labqc_samp” column, but not both. There are other invalid combinations of columns also. The “d_labsample” table may have any one of the following tables as a parent: d_sampsplit, d_fldqcsplit, d_labqcsamp, d_bioaccum_samp, d_samptreatsplit, or d_bioasrepsamp. The “ck_one_sample” check constraint on the table enforces this rule. Check constraints like this are not run by the upsert scripts, so a set of staged data may pass all the checks performed by the upsert script and yet the INSERT into d_labsample will fail.\n\n\n\nMeasurement Basis\nOrgMassSpecR\n\nData for soil and sediment are almost always reported on a dry-weight basis. If there’s anything to indicate a different basis, that deserves a closer look. Almost the only legitimate reason for a different basis for soil or sediment samples is when a leaching procedure has been applied (e.g., the Toxicity Characteristic Leaching Procedure, or TCLP); in those cases the data may be reported as the concentration in the leachate, so the basis may be “Wet” or “Whole” or “Unfiltered” – anything indicating an unfractionated liquid sample.\nData for tissues should ordinarily be reported in wet weight. Organisms’ homeostasis means that they maintain a nearly constant moisture content in their tissues, whereas the same is not true of materials like soil or sediment. If tissue data are reported in dry weight, check it carefully: labs can be sloppy about that.\nWater data are where things can be complex, because often water samples are filtered or centrifuged to remove particulates, which results in the water samples have a ‘dissolved’ basis. If the particulates are analyzed, and the results are then expressed in terms of the volume of the original sample, then the data will have a ‘particulate’ basis. Unfiltered, or whole, water, should have a basis of ‘Unfilt’, ‘Whole’, or sometimes ‘Wet’. Either of the first two of these are preferred, “Wet” is better used as a counterpart to “Dry” for soil, sediment, or tissue samples.\nThere are variations in the way things have been done in different databases. You may find that the measurement basis code for whole water samples differs from one to another, as in the third bullet above.\nThere is an implicit association between measurement bases and units. For example, if the measurement basis is “Dry”, the units should not be “mg/L” because “…/L” implies a liquid, not a solid.\nThe measurement basis refers to the form of the sample material, which is represented in the denominator of concentration units. So codes of “Sediment” “Arsenic”, “mg/kg”, “Dry” should be read as “mg of arsenic per kg of dry sediment.”\n\n\nWet Weight\n\nWet weight (or as-is) basis means no calculation has been made to compensate for the moisture content of a sample. Wet weight refers to the weight of animal tissue or other substance including its contained water. (See also “Dry weight”)\n\n\n\nDry Weight\n\nDry weight basis means the lab has measured moisture content of a sample and calculated concentrations based on the percent solids present. Dry weight refers to the weight of animal tissue after it has been dried in an oven at 65°C until a constant weight is achieved. Dry weight represents total organic and inorganic matter in the tissue. (See also “Wet weight”).\n\n\n\nLipid\n\nLipid is any one of a family of compounds that are insoluble in water and that make up one of the principal components of living cells. Lipids include fats, oils, waxes, and steroids. Many environmental contaminants such as organochlorine pesticides are lipophilic.\n\n\n\n\n\nConversions\n\nWet to Dry\n\\[DryWt = \\frac{WetWt}{Percent Solids} * 100\\]\n\n\nDry to Wet\n\\[WetWt = DryWt * \\frac{PercentSolids}{100}\\]\n\n\nOrganic Carbon Normalization\n\\[OCnorm = \\frac{DryWt}{\\frac{PercentTOC}{100}}\\]\nDryWt & WetWt = concentration\nPercentSolids = percentage (no decimal)\n\n\nResource\n\n\n\nCalcs\n\n\n\n\n\n\nAnalytical Blanks\n\n\nTrip Blank\nThe trip blank is designed to identify levels of contamination from the exposure of the reagent or sorbent bed to the same atmospheres exposed to the analyte reagent or sorbent bed. The trip blank is prepared in the laboratory with the other reagents or adsorbents prior to shipping to the field. However, the trip blank is never exposed to the field atmospheres. It is simply sent along with the field samples to and from the site. The trip blank identified areas of exposure such as shipping temperatures and pressures, laboratory preparation of field samples and laboratory preparation of field samples for analysis.\n\nField Blank\nThe field blank is similar to the trip blank in that it is also prepared during the preparation of the field reagents or adsorbents. However, the field blank is exposed to the same atmospheres in the field as the field samples. This means that the field blank is opened during the charging of impingers or sorbents in the sample train. The field blank is also exposed during the exchanging of cartridges in SW-846, Method 0030 or when field reagents are being exchanged during a test run. In summary, field blanks consist of additional sample collection media (e.g., sorbent tubes, reagents, filters) which are transported to the monitoring site, exposed briefly at the site when the samples are exposed (but no stack gas is actually pulled through these blanks), and transported back to the laboratory for analysis, similar to a field sample. At least one field blank should be collected and analyzed for each test series.\n\n\nLaboratory Blank\nThe laboratory blank is a sample of the reagents or sorbents used during the sample train reagent preparation or recovery. The laboratory blank is a sample of the extraction solvent, the rinses used during sample recovery, or a sample from the batch of sorbent used to preparing sampling cartridges. Laboratory blanks include both method blanks and instrument blanks. Method blanks are carried through all steps of the measurement process (from extraction through analysis). A method blank is typically analyzed with each sample batch. Instrument blanks are used to demonstrate that an instrument system is free of contamination. Instrument blanks are typically analyzed prior to sample analysis and following the analysis of highly contaminated samples.\n\n\nReagent Blank\nThe reagent blank is a sample of the solvents used during recovery of the sample train after the test is completed. You recall, reagent blanks for both multi-metal and chromium +6 require that the reagent blank be the same volume as the renses used to recover the samples, from probe to impinger. This is because the blank value is substracted from the sample to obtain a final concentration.\n\n\nDiagram\n\n\n\n\nDetection Limits\nPresentation\n\nWhat affects detection limits?\n\nSample size\nConcentration of other constituents\nSample clean-up\nMethodology\nLab Performance\n\nExperience\nExtraction technique\nInstrument type and maintenance\n\n\ndetection_limit - the lowest possible value an instrument/method can sense a compound is present (think of it like a whisper - you can barely hear it, but know its there). This is better known as the “method detection limit”\nquantification_limit - the limit in which an instrument/method can actually start to quantify the amount of something which is present. If the result is between the detection_limit and the quantification_limit, the result is estimated, because the instrument/method cant confidently identify the amount of something until it reaches the quantification_limit.\nreporting_limit - usually project or dataset specific. this limit is used for data analysis/statistics. the reporting_limit is equal to either the detection_limit or quantification_limit. This is better known as the “reporting detection limit”.\n\n\nMethod Detection Limit (MDL)\n\nStatistically determined\nThe minimum concentration that can be measured with 99% confidence that the concentration is greater than zero\nConcentrations near MDL are estimates\nLaboratory, instrument, matrix, method, and analyte specific\nConcentrations at MDL expected to be a false positive 1% of the time, but false negatives 50% of the time\n\n\n\nMethod Reporting Limit (MRL)\n\nMay also be referred to as QL (quantitation limit), sample quantitation limit, or just RL (reporting limit)​\nDetermined by the lowest point of the calibration​\nNot as specific as MDL, labs can adust​\nConcentrations at MRL can be reliably quantified​\nMRL &gt; MDL​\nAlso laboratory, instrument, matrix, method, & analyte specific\n\n\n\nMDL & MRL Relationship\n\n\n\nOther Detection Limits\n\nPQL\n\nConsidered to be lowest concentration that can be reliably quantified by a method\nLimit of Detection (LOD); Lowest concentration that can be detected with a 1% false negative rate.\n\nGenerally 2x to 3X MDL\n\nLimit of Quantitation (LOQ); similar to MRL\n\n\n\nPCDD/F & PCB specific\n2.5 times signal to noise\n\nEQL: Estimated Quantitation Limit\nEDL: Estimated Detection Limit\nSDL: Sample Detection Limit\nEMPC\n\nEstimated Maximum Possible Concentration (EMPC)\nPeak present but not all of the identification criteria is met\nAlways greater than MDL, may be greater than MRL\nGenerally treated a non-detect in TEQ calculations\nEMPCs can present data management difficulties and need to be reviewed in QC checks"
  },
  {
    "objectID": "content/data-management/data-literacy.html",
    "href": "content/data-management/data-literacy.html",
    "title": "Data Literacy",
    "section": "",
    "text": "All staff who use data should be data-literate. The level of any person’s data literacy should increase over time as they gain professional experience, skills, and knowledge of the principles, requirements, and constraints of data management. The table below summarizes components of data literacy in terms of knowledge, skills, and experience. This table is excerpted from Meeting the Challenges of Data Quality Management. A few amendations relevant to environmental data management work are included in square brackets, italicized."
  },
  {
    "objectID": "content/data-management/data-literacy.html#data-literacy-levels",
    "href": "content/data-management/data-literacy.html#data-literacy-levels",
    "title": "Data Literacy",
    "section": "Data Literacy Levels",
    "text": "Data Literacy Levels\nThe corresponding data literacy levels are described as:\n\nLevel 1: Knowledge of data input and output of business [project] processes for which they have direct reponsibility.\nLevel 2: Knowledge of data-as-data, how to organize and prepare data for presentation or use.\nLevel 3: Knowledge of how to query and manipulate complex data and prepare it for presentation or use; can use data to influence decisions or to formulate options for solving problems.\nLevel 4: Knowledge of how to combine data and how to apply statistical [and geospatial] methodologies to existing data to understand and interpret data.\nLevel 5: Advanced knowledge of statistics, knowledge of how to create predictive models through data selection, hopythesis formation, testing, and training.\n\nIn addition to the above, technical knowledge relevant to specific project work (e.g., chemistry, ecology, hydrogeology, environmental engineering, regulations) is a key component of data literacy for data management staff."
  },
  {
    "objectID": "content/data-management/data-literacy.html#data-literacy-components",
    "href": "content/data-management/data-literacy.html#data-literacy-components",
    "title": "Data Literacy",
    "section": "Data Literacy Components",
    "text": "Data Literacy Components\n\n\n\n\n\n\n\n\n​Literacy Component\n​Data Literacy Component\nDetail\n\n\n\n\n​Knowledge\n​Knowledge of \"data as data\"–structure and life cycle\n​Understanding of how data can be collected, structured, and organized for use, how data may change over time and have different uses in different circumstances (e.g., knowledge of the data life cycle).\n\n\n​Knowledge\n​Knowledge of the things that can go wrong with data\n​Understanding of how a process relies on data input and creates data output; ability to see data as the product of a process and to recognize factors that contribute to or diminish the reliability of data.\n\n\n​Knowledge\n​Knowledge of ways of looking at data\n​Knowledge of data visualization and understanding of how data may be combined, aggregated, and calculated before visualization techniques are applied.  This knowledge comtributes to the ability to \"tell a story\" (interpret) data.\n\n\n​Knowledge\n​Knowledge of statistical methods\n​Basic understanding of statistics and how statistical techniques are applied to data.  Knowledge of the riske associated with applying inappropriate techniques.\n\n\n​Knowledge\n​Knowledge of data domains\n​Knowledge of data domains, whether understood through industry (health care data financial data, educational data [, environmental data]) or by master data concepts (customer, product, geography).\n\n\n​Knowledge\n​Knowledge of peculiarities of data within a given organization\n​Understanding of data as the DNA of an organization, the source of knowledge about the ways the organization works.\n\n\n​Knowledge\n​Knowledge of the impact of technology on data\n​Understanding of how different tools work, how they enable one to manipulate and use data, and what the best tool is for the job in any given instance.\n\n\n​Knowledge\n​Knowledge of the quality expectations for data\n​Knowledge of what it means for a data set to be complete, valid, and current.  This includes understanding concepts related to data quality management in general, and knowledge of the particular data quality requirements of the organization or industry.\n\n\n​Skills\n​Technical skills that enable access to data\n​Ability to query data effectively\n\n\n​Skills\n​Skill at identifying and evaluating the appropriateness of a data set to the task at hand\n​Understanding the level of detail, criteria for completeness, and ability to inform the question at hand.\n\n\n​Skills\n​Skill at organizing data for analysis\n​Getting data at the right level of detail, de-normalizing it, combining it, and aggregating it.\n\n\n​Skills\n​Skill at applying appropriate analysis techniques\n​Determining the best way to test hypotheses [also: to conduct exploratory data analyses, to formulate hypotheses, to develop and apply predictive models, and to compute relative likelihoods].\n\n\n​Skills\n​Skill at robustly testing hypotheses while interpreting data\n​Avoiding cognitive traps.\n\n\n​Skills\n​Skill at organizing data for presentation\n​Ability to make a business case, influence decsisions, or solve problems; ability to use data visualization to support both interpretation of data and communications around data.\n\n\n​Experience\n​Experience querying data\n​Finding the approriate data through querying for specific analyses.\n\n\n​Experience\n​Experience evaluating the quality of data\n​From basic inspection and comparison to rules for assessment of the reliability of the source.\n\n\n​Experience\n​Experience preparing data for presentation or analysis\n​From the basics of choosing the appropriate data to present, getting rid of the noise, combining and visualizing data, and so on.  This starts with running reports and actually thinking about the results and continues through being able to create sophisticated models.  It may include anything from a cost-benefit analysis to an annual report.  Preparing data for presentation or analysis is really a way of learning to think about data.\n\n\n​Experience\n​Experience tracing the root causes of data issues\n​Although root cause analysis is usually associated with data quality work, many analysts who find data issues do a lot of digging to try to uncover the sources of their frustrateion; doing so increases their knowledge of organizational [project] data, the data chain, and the risks associated with not managing data purposefully.\n\n\n​Experience\n​Experience interpreting data\n​Evaluating quality is a necessary step to interpreting data (actually drawing conclusions from what you see, or at least formulating hypotheses that can be tested).  Interpreting data is more than just saying what you think it means.  For complex data, it also includes learning how to apply different analysis techniques and choosing techniques that are most appropriate to the task at hand.\n\n\n​Experience\n​Experience using data tools\n​From basic query tools and spreadsheets to more sophisticated data visualization tools, all the way through to tools that allow you to combine disparate data sets for complex analyses.  This is not so much about \"reliance on technology\" as it is about understanding how empowering some technologies can be and choosing the right one for the job."
  },
  {
    "objectID": "content/data-management/best-practices.html",
    "href": "content/data-management/best-practices.html",
    "title": "Best Practices",
    "section": "",
    "text": "The following summary of data management best practices applies specifically to operations of the Data Management, but are relevant to any data management activities carried out by other staff.\n\nKnow the use of the data\n\nFor the overall project\nFor each data summary request\n\nUse a single authoritative data store.\nTabular project data are ordinarily maintained in a client-server database (PostgreSQL) that is accessible across the network. Spatial project data are maintained in project-specific directories on the server used with ArcGIS.\nBack up important data.\nPostgreSQL databases are backed up nightly.\nVerify or characterize data quality.\nSee the Data Quality and Data Issue Tracking pages.\nControl access to sensitive data.\nPassword-controlled access to PostgreSQL databases must be approved by the project manager.\nTrack data changes.\nPerform all updates with SQL scripts that are created in date-tagged directories to preserve the order of operations. The script processor ordinarily used to run these scripts automatically maintains a log of scripts run, data files imported, and users’ responses to interactive prompts. Custom logs may also be created during data loading or ediing. The database software can also creates an audit log recording every change to every row of every table.\nDocument data management practices\n\nDefault practices (on this wiki; see the links below and on the DMA Home page)\nProject-specific\n\nPlanned: Data Management Plan\nad-hoc: Data Manager’s Manual\n\nTask-specific\n\nScript header notes\nCustom logfiles\n\n\nPreserve original data\nAny data cleaning, transformation, re-coding, and re-structuring of data that is done when loading data into a database is carried out on a copy of the original data.\nScript all data revisions and data summarizations.\nSee the page on scripting of data management operations.\nUse version control for things that may change\n\nScripts (using Mercurial or Git)\nData summary outputs (using date-tagged directories or files)\n\nRecord metadata\n\nFor both incoming and outgoing data\nMetadata includes\n\nProvenance: Who created, provided, or produced the data the data.\nContent: What the data set contains\nPurpose: What the data are intended to be used for.\nMethod: How the data were generated or selected and summarized.\nHistory: The history of any revisions made to the data or the data summarization method.\n\nForms of metadata include:\n\nCopies of emails or other documents that transmit data or request data.\nHeader notes in scripts.\n‘Info’ pages and glossary pages in data summaries.\nCustom log files created by scripted data operations.\n\n\nDate-tag directory and file names where the sequence of changes may affect their validity."
  },
  {
    "objectID": "content/blog/posts/quarto-tabsets/index.html",
    "href": "content/blog/posts/quarto-tabsets/index.html",
    "title": "Quarto Tabsets",
    "section": "",
    "text": "Tabsets can be useful when you want to show multiple flavors of the same thing. For example, what’s the command to copy files on PowerShell, CMD, and Bash?\n\nBashPowerShellCMD\n\n\nCopy a file or directory.\ncp -rf file.txt ./output/\n\n\nCopy a file or directory.\nCopyItem -Recurse -Force file.txt .\\output\\\n\n\nCopy a file or directory.\nxcopy file.txt .\\output\\ /E/H"
  },
  {
    "objectID": "content/blog/posts/linux-permissions/index.html",
    "href": "content/blog/posts/linux-permissions/index.html",
    "title": "Linux Permissions",
    "section": "",
    "text": "General: _rwxrwxrwx 1 owner group\n_ | rwx | rwx | rwx = Special | Owner | Group | All Users\n\n\n\n4 | r = Read\n2 | w = Write\n1 | x = Execute\n\n\n\n\n\n0 = ---\n1 = --x\n2 = -w-\n3 = -wx\n4 = r-\n5 = r-x\n6 = rw-\n7 = rwx"
  },
  {
    "objectID": "content/blog/posts/linux-permissions/index.html#syntax",
    "href": "content/blog/posts/linux-permissions/index.html#syntax",
    "title": "Linux Permissions",
    "section": "",
    "text": "General: _rwxrwxrwx 1 owner group\n_ | rwx | rwx | rwx = Special | Owner | Group | All Users\n\n\n\n4 | r = Read\n2 | w = Write\n1 | x = Execute\n\n\n\n\n\n0 = ---\n1 = --x\n2 = -w-\n3 = -wx\n4 = r-\n5 = r-x\n6 = rw-\n7 = rwx"
  },
  {
    "objectID": "content/blog/posts/linux-permissions/index.html#commands",
    "href": "content/blog/posts/linux-permissions/index.html#commands",
    "title": "Linux Permissions",
    "section": "Commands",
    "text": "Commands\nchgrp = Change group\nExample: sudo chgrp -R &lt;group&gt; &lt;folder&gt;\nchown = Change ownership\nExample: sudo chown -R &lt;user&gt;:&lt;group&gt; &lt;file/folder&gt;\nchmod = Change permissions\nExample: sudo chmod -R 774 &lt;file/folder&gt;\nMake new files inherit the group: sudo chmod g+s &lt;folder&gt;\n\nExample\nCreate a shared directory for a group.1. Create a shared directory for users to access: /share\n\nAssign users to a common group (staff): sudo usermod -a -G staff &lt;user&gt;\nVerify user groups: groups &lt;user&gt;\nCreate shared directory and assign permissions:\n\nsudo mkdir /share && \\\nsudo chgrp -R staff /share && \\ # assign group\nsudo chmod -R g+w /share && \\ # permissions\nsudo chmod -R +s /share # inherit permissions for newly created files/folders"
  },
  {
    "objectID": "content/blog/posts/diff-pdf-and-docx/index.html",
    "href": "content/blog/posts/diff-pdf-and-docx/index.html",
    "title": "DIFFing PDF and DOCX files",
    "section": "",
    "text": "If you have a PDF and DOCX of the same document and want to check for difference in text, use diff to compare them. Since we’re using --word-diff, it doesn’t matter that the two files use wildly different line wrapping.\ngs -q -sDEVICE=txtwrite -o- file1.pdf &gt; file1.txt\npandoc -t plain file2.docx &gt; file2.txt\ngit diff --no-index --word-diff file1.txt file2.txt\nOr create a shortcut…\nalias pdfcat='gs -q -sDEVICE=txtwrite -o-'\nalias doccat='pandoc -t plain'\npdfcat file1.pdf &gt; file1.txt\ndoccat file2.docx &gt; file2.txt\ngit diff --no-index --word-diff file1.txt file2.txt\nCredits"
  },
  {
    "objectID": "content/blog/posts/change-postgres-password/index.html",
    "href": "content/blog/posts/change-postgres-password/index.html",
    "title": "Changing your PostgreSQL password",
    "section": "",
    "text": "Open command prompt or PowerShell (Start &gt; type “CMD”&gt; press enter) and enter the command below after you’ve replaced USERNAME and YOUR NEW PASSWORD. Make sure you don’t delete any of the quotes in this command:\npsql -U USERNAME -h env4 -d db_inventory -c \"alter role USERNAME with password 'YOUR NEW PASSWORD'\"\nThe command will ask you for your current password once, then set your new password."
  },
  {
    "objectID": "content/blog/index.html",
    "href": "content/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\nAuthor\n\n\nReading Time\n\n\nCategories\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nDIFFing PDF and DOCX files\n\n\nCompare DIFFerences in text between PDF and DOCX files.\n\n\nCaleb Grant\n\n\n1 min\n\n\nLinux,Utility\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nQuarto Tabsets\n\n\nCreating tabsets with Quarto\n\n\nCaleb Grant\n\n\n1 min\n\n\nQuarto\n\n\n\n\n\n\n\nJul 11, 2023\n\n\nChanging your PostgreSQL password\n\n\nSteps for any user to change their PostgreSQL password\n\n\nCaleb Grant\n\n\n1 min\n\n\nPostgreSQL\n\n\n\n\n\n\n\nJul 7, 2023\n\n\nSFTP\n\n\nSteps for pulling server files to local machine via sftp\n\n\nCaleb Grant\n\n\n1 min\n\n\nLinux\n\n\n\n\n\n\n\nJul 5, 2023\n\n\nMounting a Windows Share\n\n\nMount a windows share drive to a Linux machine\n\n\nCaleb Grant\n\n\n2 min\n\n\nLinux\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nGit Merge\n\n\nMerging git branches locally\n\n\nCaleb Grant\n\n\n2 min\n\n\nGit\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nLinux Permissions\n\n\nLinux permission cheatsheet and examples\n\n\nCaleb Grant\n\n\n1 min\n\n\nLinux\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nCreating a Blog Post\n\n\nThis is an example of how to create a new blog post.\n\n\nCaleb Grant\n\n\n1 min\n\n\nQuarto,Blog\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nAccessing Quarto Variables\n\n\nAccess quarto variables in a document.\n\n\nCaleb Grant\n\n\n1 min\n\n\nQuarto,Variables\n\n\n\n\n\n\nNo matching items\n\n\n\n RSS"
  },
  {
    "objectID": "content/blog/posts/accessing-quarto-variables/index.html",
    "href": "content/blog/posts/accessing-quarto-variables/index.html",
    "title": "Accessing Quarto Variables",
    "section": "",
    "text": "You can access dynamic variables within documents, which can be useful for externalizing content that varies depending on context. As an example, you can reference file metadata using the syntax {&lt; meta title &gt;}, which would include the title of this article: Accessing Quarto Variables.\nIsn’t that cool?"
  },
  {
    "objectID": "content/blog/posts/creating-a-post/index.html",
    "href": "content/blog/posts/creating-a-post/index.html",
    "title": "Creating a Blog Post",
    "section": "",
    "text": "To add a post, create a copy of blog/posts/_template, modify the folder name with the title of the post, then update the index.qmd with your content. New posts under blog/posts will show up in the blog listings when the site is rendered."
  },
  {
    "objectID": "content/blog/posts/git-merge/index.html",
    "href": "content/blog/posts/git-merge/index.html",
    "title": "Git Merge",
    "section": "",
    "text": "The steps below can be used to merge two branches on your local machine. The braches used in this example are:\n\nmain: The authoritative or “production” code lives in this branch.\ndev: This branch is split from the main branch and a new feature or update is coded with the intent to merge changes back into the main branch.\n\n\nPull main and dev branches so local repo is up to date with the remote.\n\ngit checkout main\ngit pull origin main\ngit checkout dev\ngit pull origin dev\n\nCheckout the main branch so we can merge the dev branch into main\n\ngit checkout main\ngit merge dev\n\n\nCheck the branch status: git status\n\nEvaluate the two files with a conflict (ie. .gitignore and requirements.txt) and reconcile issues, then git add when ready.\nCommit the changes: git commit -m \"merge @tnelson-integral dev branch with main\"\nPush changes to the remote on GitHub: git push origin main\nCheck out the dev branch locally and pull the main branch changes into it so dev can be up-to-date with main\n\ngit checkout dev\ngit pull origin main\ngit push origin dev"
  },
  {
    "objectID": "content/blog/posts/mounting-a-windows-share/index.html",
    "href": "content/blog/posts/mounting-a-windows-share/index.html",
    "title": "Mounting a Windows Share",
    "section": "",
    "text": "The command below can be used to create a temporary mount point although the mount will disassociate on next server reboot.\nmkdir /home/&lt;user&gt;/mnt/DataManagement && \\\nsudo mount -t cifs //10.10.145.5/DataManagement /home/&lt;user&gt;/mnt/DataManagement -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "content/blog/posts/mounting-a-windows-share/index.html#quick-reference",
    "href": "content/blog/posts/mounting-a-windows-share/index.html#quick-reference",
    "title": "Mounting a Windows Share",
    "section": "",
    "text": "The command below can be used to create a temporary mount point although the mount will disassociate on next server reboot.\nmkdir /home/&lt;user&gt;/mnt/DataManagement && \\\nsudo mount -t cifs //10.10.145.5/DataManagement /home/&lt;user&gt;/mnt/DataManagement -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "content/blog/posts/mounting-a-windows-share/index.html#persistent-mounts",
    "href": "content/blog/posts/mounting-a-windows-share/index.html#persistent-mounts",
    "title": "Mounting a Windows Share",
    "section": "Persistent Mounts",
    "text": "Persistent Mounts\n\nCreate a mount point to a Windows share name (e.g. Q): sudo mkdir /mnt/share/Q\nAppend the following similar line to /etc/fstab– to associate the mount automatically upon machine boot:\n//192.168.50.19/Transfer /mnt/share/Q cifs credentials=/home/DatamanBU/.smbpasswd,uid=1011,gid=1005,dir_mode=0775,file_mode=0775,rw 0 0\nNotes:\n\nuid=1011: mount as user username. Find UID with id -u &lt;username&gt;\ngid=1005: mount as group datamgrs\ndir_mode=0775, file_mode=0775: readable/writeable by any users that are members of datamgrs group\nThe credentials file /home/DatamanBU/.smpbasswd must already exist (or be created) by root. It contains 2 lines:\nusername=DatamanBU\npassword=password\nMake the credentials file only readable by root: sudo chmod 700 /home/DatamanBU/.smbpasswd\n\nAssociate the mount immediately: sudo mount -a -v -t cifs"
  },
  {
    "objectID": "content/blog/posts/sftp/index.html",
    "href": "content/blog/posts/sftp/index.html",
    "title": "SFTP",
    "section": "",
    "text": "Allow secure connection for local IP on remote machine\nsudo ufw allow from 73.239.143.170 to any port 22\nCreate SFTP connection to remote from local machine\nsftp hostname@167.99.102.213\nsftp hostname@167.99.102.213's password: SuperSecret\nPull remote files to local machine\nget {remote-path} {local-path}"
  },
  {
    "objectID": "content/data-management/data-issues.html",
    "href": "content/data-management/data-issues.html",
    "title": "Data Issue Tracking",
    "section": "",
    "text": "Data sets that are received from agencies, clients, and other consultants ordinarily have problems. These problems may include:\nIn addition to the checklists for data evaluation and data loading, the problems that are identified, unless they are few and minor, should be tracked. The workbook attached to this page provides a structure for logging these issues and tracking their resolution. The workbook includes a “Data Issues” worksheet, and may contain additional worksheets that contain illustrative or supporting information for each issue. Each issue should be described, a resolution identified, that resolution reviewed and revised if appropriate, and the implementation of each resolution tracked.\nThe specific procedure for use of this workbook is:\nThese instructions are also contained on the first page of the workbook.\nA template for a data issue tracking workbook can be downloaded from the link below. Examples of data set issues are on the wiki page Example of Data Set Issues."
  },
  {
    "objectID": "content/data-management/data-issues.html#creating-issue-logs-from-within-sql-scripts",
    "href": "content/data-management/data-issues.html#creating-issue-logs-from-within-sql-scripts",
    "title": "Data Issue Tracking",
    "section": "Creating Issue Logs From Within SQL Scripts",
    "text": "Creating Issue Logs From Within SQL Scripts\nSQL scripts are a primary means for loading, manipulating, and summarizing data, and data quality issues may be identified during these processes–particularly during data loading. Often many of these issues can be dealt with immediately. Some, however, may not be readily addressed and should be captured in an issue log so that they can be tracked and their resolution documented. The SQL scripts included here (below) simplify the creation of an issue log from within SQL scripts that are used to carry out other data management operations. These scripts are intended to be run using execsql.\nThe script log_issue.sql creates issue log entries. It should be imported into the main data manipulation script at every point at which there is an issue to be logged. Two or three execsql substitution variables should be set (using the SUB metacommand) each time before the log_issue.sql script is imported. These substitution variables are:\n\ndataset: A name for the data set to which this issue applies.\nissue: A description of the issue.\nissue_data: The name of a table or view containing a data summary that illustrates the issue. This substitution variable need not be defined if no illustrative data are necessary or applicable. Use the RM_SUB metacommand to un-define this variable if it has been previously used.\n\nIssues will be logged in a SQLite database named issue_log.sqlite in the same directory as the script. In this database the issue_log table will contain the data set and issue descriptions. This table will also contain additional columns that are present in the issue log template (attached above). The primary key of this table is the data set name and the issue description. If the same issue is found multiple times for the same data set, it will appear only once in the issue log. Each data summary that illustrates an issue will be stored in its own table, with a name corresponding to the automatically-assigned issue number.\nThe SQLite database containing the issue log will be automatically created if it does not exist.\nThe script issue_log_to_ods.sql (and issue_log_to_ods_sheets.sql, which is called by the main script) will convert the issue log from the SQLite database to an OpenDocument spreadsheet.\nHeader notes in the scripts provide additional information about their usage.\n\nlog_issue.sql\nissue_log_to_ods.sql\nissue_log_to_ods_sheets.sql\n\nThe following code snippet illustrates the use of the log_issue.sql script during data loading. Both SQL statements and execsql metacommands are used in this code."
  },
  {
    "objectID": "content/data-management/data-issues.html#example-of-data-set-issues",
    "href": "content/data-management/data-issues.html#example-of-data-set-issues",
    "title": "Data Issue Tracking",
    "section": "Example of Data Set Issues",
    "text": "Example of Data Set Issues\nReview of newly received data sets typically reveals issues such as:\n\nMissing information.\nAmbiguous terminology.\nConflicting information.\nDuplication of data.\nLack of standardization of codes.\nTypographical errors.\nInconsistent data qualifiers.\nLack of data integrity (no enforced primary keys or foreign keys).\n\nThese issues should be identified and either resolved or deliberately chosen to be ignored before the data are used. Lack of data integrity cannot be ignored if the data are to be put into a relational database.\nThe number of such problems in any given data set is unknown until the data are carefully examined. The attached workbook can be used to document the issues that are found in a data set.\n\n\nExample 1\nA data set was received in the form of a Microsoft Access file from another consultant working for Integral’s client. The client was under the impression that because the data were in Access, they were all correct and complete. A copy of attached data issue tracker was used to describe each of the 72 issues that were found in this data set. Whereas loading a data set like this one into the environmental database might require only a couple of hundred lines of SQL code if the data were clean, implementing corrections to the issues that were found during data loading required writing over 6,000 lines of SQL code. Identification of the issues, determination of an appropriate resolution strategy for each issue, scripting those resolutions, conducting a QA review of the scripts, and finally executing those scripts required considerable data management expertise, time, and coordination with other staff. Not every data set has as many issues–though some may have more–but the extent of this kind of work is typically unknown when a data set is first obtained. Project planning should account for the high likelihood that data sets will require some work of this nature.\n\n\nExample 2\nBelow is a table of analytical results as it might be received or constructed from another consultant, client, or report. It superficially appears to be clean and usable.\n\nHowever, a close look at this data set reveals a number of data quality issues, which are highlighted in the following image and listed below.\n\nThe data quality issues in this small data set are:\n\nInvalid coordinate values\nInconsistent units\nConcentration values and qualifiers packed into the same column\nAn apparent duplicate record with different results\nAmbiguous differences in sample media\nAmbiguous result values\nInconsistent result units\nAmbiguous or inconsistent project areas\nInconsistent and ambiguous significant digits\nMultiple coordinates for a single location\nIncomplete or ambiguous replicate identifiers\nTypographic errors\n\nThese are representative of errors typically found in data sets that we receive from other consultants, agencies, and clients. Large data sets tend to have both more errors and a larger number of types of errors than small data sets. Data set size is not the sole determinant of data quality however, nor of the level of effort that is needed to correct those errors. Data don’t create themselves, they are created by people, and the quality of an incoming data set is therefore dependent upon the standards and practices of those who generated and compiled the data."
  },
  {
    "objectID": "content/data-management/data-quality.html",
    "href": "content/data-management/data-quality.html",
    "title": "Data Quality",
    "section": "",
    "text": "Good decisions require high-quality data–and analytics, and engineering, and other disciplines, but data is the foundation of most decision support services. The data we use is typically generated by processes that provide a broad scope for errors resulting from faulty recording, transcription, encoding, tabulation, presentation, and documentation. Poor data quality may also result from omissions: sampling depths or depth units weren’t recorded because of course everybody knows what they are; coordinates are not included in the data table because they’re in a GIS file (that we did not get); report tables omit information so that they fit on the page better. Data preparation and presentation may also introduce data quality issues, such as when information is represented only by colors, font changes, footnotes, or other annotations. Data don’t fall from the sky in pure and perfect form, and so an important–and necessary–part of our work is to ensure that decisions and other deliverables are founded on high-quality data."
  },
  {
    "objectID": "content/data-management/data-quality.html#data-quality-dimensions",
    "href": "content/data-management/data-quality.html#data-quality-dimensions",
    "title": "Data Quality",
    "section": "Data Quality Dimensions",
    "text": "Data Quality Dimensions\nThe term “data quality” is so all-encompassing that it can be hard to grasp or measure, with the result that it can be hard to determine or describe the level of data quality for a data set. To make it more tractable, data quality can be described in terms of several specific sets of attributes. These sets of attributes are called “data quality dimensions”. Various authors writing on data quality have defined data quality dimensions differently, but the dimensions of data quality that are most relevant to environmental data are:\n\nData integrity\nUnambiguity\nConsistency\nCompleteness\nCorrectness\nDocumentation\n\nEach of these is a scalar quantity–i.e., can range from low to high, or bad to good–and although ideally every data set is at the top of the scale on all of these dimensions, this is rarely the case in practice. Compromises often must be made, and ideally those are informed and deliberate compromises. In most cases improvements in data quality can be made along any of the dimensions listed above. If a data quality assessment is not performed, data uses may be compromised by data quality issues of unknown type and severity.\nConsideration of these specific dimensions is useful when trying to identify and describe data quality issues. These dimenstions are not completely distinct however. For example, the overarching goal of establishing semantic consistency within the database relates to the dimensions of data integrity, unambiguity, and consistency.\nData quality assessments of new data sets is guided by the data entry and loading checklist. An initial evaluation is typically performed by visual examination, by filtering the data set in various ways if it is in a spreadsheet, or by importing it into a staging table (or tables) in a database and running queries to identify possible issues. When compiling multiple data sets into a single database, data quality assessments must consider both each data set individually and issues related to its compatibility with other data sets. The quality of geographic coordinate data is assessed by check plots: mapping the given coordinates with one or several different assumptions about the applicable spatial reference system, and then comparing the result to location descriptions, to physical features (such as river or shoreline boundaries), and to any maps available from the original data source. If the data are to be loaded into any database with properly designed relational constraints, during the loading process the database itself may identify additional data quality issues that are not easily perceived by simple interactive review of the data.\nDepending on the number and type of data integrity issues, the data manager may record these in an issue log to document both the nature of each issue and its ultimate resolution. Use of issue logs may be required, if specified in the project plan or data management plan for a project, or may otherwise be created at the discretion of the data manager."
  },
  {
    "objectID": "content/data-management/data-quality.html#data-integrity",
    "href": "content/data-management/data-quality.html#data-integrity",
    "title": "Data Quality",
    "section": "Data Integrity",
    "text": "Data Integrity\n\nWhat data integrity means\nData integrity means that different types of information are properly identified and related to one another. There are three elements of data integrity:\n\nUniqueness. For example, each field sample that is collected must be uniquely identified.\nRelational integrity. For example, each analytical result must be related to (the measurement made on) a single sample.\nDomain integrity. For example, a concentration should be numeric, not a combination of a number and some qualifier or other annotation. Also, coded values such as analytes and units must be within the set of valid values.\n\nData integrity problems are extremely common in data sets that data managers receive from third parties, particularly when a large data set is provided in the form of a single table. Data integrity problems also arise from hand-entry of field sampling information, even in work conducted by technical staff.\n\n\nHow we establish or improve data integrity\nThe best way to establish and maintain data integrity is to define and enforce appropriate rules for uniqueness and relational integrity. A properly designed relational database includes those rules and automatically enforces them. Establishment of data integrity is one of the advantages of using a relational database, and is the primary tool used by data mangement staff to guarantee data integrity.\nMigration of a data set that is lacking data integrity into a database that enforces data integrity requires that the integrity issues be addressed. This may be done by:\n\nAcquiring additional information to resolve the problems. This information may come from work plans, field forms, reports, or interviews with individuals responsible for data collection or preparation.\nMaking and documenting assumptions about which data sources or data items are most reliable. For example, if there are multiple collection dates for the same sample identifier, either the identifier or the recorded date may be assumed to be the correct value. Such assumptions may be based on overall characteristics of the data set such as the rules used to construct sample identifiers or the consistency of a sequence of sampling dates.\nMaking and documenting decisions about acceptable compromises to data quality. For example if there are multiple collection dates recorded for a sample, if date is not important to data use, then a decision may be made to use either the first or last recorded date; if date is important, then more effort will need to be expended to resolve the issue."
  },
  {
    "objectID": "content/data-management/data-quality.html#unambiguity",
    "href": "content/data-management/data-quality.html#unambiguity",
    "title": "Data Quality",
    "section": "Unambiguity",
    "text": "Unambiguity\n\nWhat unambiguity means\nThere are several types of ambiguity that are frequently found in data sets:\n\nColumn names. Is a column named Sample type meant to distinguish between sediment, water, and tissue samples; between natural and field QC samples; between site and background samples; between original and confirmation samples, or something else?\nCoordinates. Geographic coordinates that are not accompanied by a spatial reference system identifier (i.e., a datum and possibly a projection) are ambiguous. For examples, coordinates in decimal degrees may be based on either a NAD83 or WGS84 datum (or possibly others), and the difference may amount to a perceptible and important difference in where locations are shown on maps.\nDates and times. Does 5/8/2016 mean May 8 or August 5? Does a time of 7:32 represent AM or PM? What time zone is that in?\nCodes. Units of % can be ambiguous if it is not known whether this is a mass percent or volume percent; this ambiguity can prevent conversion of units. In field records containing fish species codes, species codes such as FP can be ambiguous if not accompanied by an unambiguous taxon name. In analytical chemistry results that include analytes named PAH and Total PAH, these can be ambiguous if it is not clear whether or not they are meant to be the same thing.\n\nAmbiguity of coordinates and codes is very common, and often not easily resolved by inspection of the data or by reasonable assumptions.\n\n\nHow we reduce data ambiguity\nThe approach used to resolve an ambiguity vary depending on the nature of that ambiguity, but may include:\n\nSearching for clarifying information in work plans, final reports, and other related documentation.\nFactoring codes into two or more distinct sets of unambiguous valid values.\nApplying default assumptions (e.g., for date formats).\nUsing unambiguous column names."
  },
  {
    "objectID": "content/data-management/data-quality.html#consistency",
    "href": "content/data-management/data-quality.html#consistency",
    "title": "Data Quality",
    "section": "Consistency",
    "text": "Consistency\n\nWhat consistency means\nConsistency means that the same type of information is always represented in the same way. Consistency issues almost always arise when integrating data from different sources. Valid value lists ordinarily differ between different data sources, but inconsistency in the use of valid values is often also found within individual data sets. Consistency issues often underlie issues of ambiguity and completeness. Data sets may differ in the consistency of data structure and reporting detail; for example, one data set may include analytical data down to the individual laboratory replicate, whereas another may contain only data summarized to the level of the interpretive sample; one data set may contain a detailed description of the type of material collected and analyzed, whereas another may contain only the laboratory’s characterization of the material (so that sediment is identified as soil, for example).\n\n\nHow we establish or improve data consistency\nResolving each data consistency issue is ordinarily carried out by data-set-specific code in the SQL scripts that are used to load the data. Consistency of valid values is established by using translation tables that convert codes in source data sets to the valid values."
  },
  {
    "objectID": "content/data-management/data-quality.html#completeness",
    "href": "content/data-management/data-quality.html#completeness",
    "title": "Data Quality",
    "section": "Completeness",
    "text": "Completeness\n\nWhat completeness means\nThere are two aspects to data completeness:\n\nWe have all of the relevant data sets\nNo required data are missing from any of the data sets.\n\nWhether an individual data set is complete therefore depends on what data are required, which may be project-specific. The database should impose a minimum set of requirements necessary to ensure data integrity. Any additional requirements should be specified in the project plan, Data Management Plan, Data Manager’s Manual, or in the project initiation checklist.\n\n\nHow we establish or improve data completeness\nDetermination of whether or not we have all of the data is sometimes simple: we collected the data ourselves, or the client sent us the data and assured us that all of the data have been provided (this is not always true, of course). In other circumstances we may need to spend substantial effort to identify and collect all of the data relevant to a particular project. To support such efforts, we have a template for a data set inventory (database) and procedures for populating the inventory, prioritizing its contents, and tracking data loading efforts.\nThe ideal method to resolve completeness issues for an individual data set is to obtain the missing information from accessory sources of information such as work plans, field sampling plans, field logbooks and notes, laboratory data packages, and data reports. When this approach is insufficient because the documents are unavailable or are themselves incomplete, a fallback approach, to be used when necessary and appropriate, is to use default or synthesized values. Examples of this approach are:\n\nWhen dates are provided only to the month, but dates are required to be specified to the day, the first day of the month is assigned.\nWhen location identifiers are missing, they are synthesized from the sample identifiers or other available information as appropriate.\nWhen environmental sample identifiers are missing, they may be synthesized by concatenating location identifiers, dates, and depths.\nWhen laboratory sample identifiers are missing, the environmental sample identifiers are used.\nWhen laboratory replicate identifiers are missing, default values of “1”, “2”, etc., are assigned."
  },
  {
    "objectID": "content/data-management/data-quality.html#correctness",
    "href": "content/data-management/data-quality.html#correctness",
    "title": "Data Quality",
    "section": "Correctness",
    "text": "Correctness\n\nWhat correctness means\nCorrectness for environmental data means that measurement results accurately represent environmental conditions at the location, date, and depth indicated. In general this means that the data retain fidelity to the original data source after all of the translations and transformations that are carried to to standardize the data and resolve other data quality issues. Deviations from the original data source may legitimately result from corrections or clarifications that result from resolving data quality issues.\n\n\nHow we establish or improve data correctness\nThe two primary methods of assessing and improving data correctness are:\n\nEstablished quality management program, specifically QA checks of data loading scripts and data compilations\nData validation for analytical chemistry data.\n\nSome data correctness issues are only identified during data analyses. For example, statistical analyses may flag certain data points as outliers, and further investigation may reveal that those values are the result of transcription or other errors, often originating in the primary data source. Correctness is a data quality issue that should be attended to throughout all data uses within a project."
  },
  {
    "objectID": "content/data-management/data-quality.html#documentation",
    "href": "content/data-management/data-quality.html#documentation",
    "title": "Data Quality",
    "section": "Documentation",
    "text": "Documentation\nThere are three types of documentation that are important with regard to data quality:\n\nProvenance: where the data came from.\nSource documentation: work plans, data reports, and other documentation that accompanied the data themselves\nDocumentation: project plans, data management plans, completed checklists, issue logs, data manager’s manuals, script header notes, automatically-generated execsql logs, custom script logs, and QA forms.\n\nStudy descriptions, location definitions, sample descriptions, and analytical chemical results (and other things) all must be linked to a source document citation. Not all of the source documents may be used for this purpose, but all of the available documentation, of all types, can be added to the database to maintain a centralized record of all data-related documents. A DocMgr system may also be set up for any project, and may include data-related documents as well as others.\nDocumentation of a data set’s provenance is often missing. The origin of a data set may be known to some project staff, but not others. This can be a limitation when questions or issues need to be communicated back to the data provider, or when issues are identified and the PIC asks “Where did these data come from?”. The recommended approach to documenting a data set’s provenance is to create a file containing that information in the same directory (ordinarily under the project’s Data directory) in which the data files themselves are saved. If the data were received by email, printing the email to a PDF file is recommended; otherwise, the information should be documented in a text or Word file."
  },
  {
    "objectID": "content/data-management/data-quality.html#application-specifics",
    "href": "content/data-management/data-quality.html#application-specifics",
    "title": "Data Quality",
    "section": "Application Specifics",
    "text": "Application Specifics\nThe table at the link below identifies specific considerations for each of the data quality dimensions, for several different stages of the data lifecycle.\n applications-of-data-quality-dimensions.xlsx"
  },
  {
    "objectID": "content/data-management/data-quality.html#summary",
    "href": "content/data-management/data-quality.html#summary",
    "title": "Data Quality",
    "section": "Summary",
    "text": "Summary\nData quality is important and multifaceted. The DMA practice has established standards and procedures to address all the dimensions of data quality that are within their purview. An understanding of how to assess data quality and address data quality issues is also a component of data literacy."
  },
  {
    "objectID": "content/data-management/data-quality.html#federal-and-state-guidance-on-data-quality",
    "href": "content/data-management/data-quality.html#federal-and-state-guidance-on-data-quality",
    "title": "Data Quality",
    "section": "Federal and State Guidance on Data Quality",
    "text": "Federal and State Guidance on Data Quality\nThe following table is a compilation of federal and state guidance documents that pertain to data quality. These almost all pertain specifically to chemistry data quality, rather than data quality in general.\n\n\n\nAuthority\nGovernment Organization\nReference / Link\n\n\n\n\nFederal\nUS Department of Defense (DoD) \nUniform Federal Policy (UFP) for Quality Assurance Project Plans (QAPPs)\n\n\nFederal\nUS Department of the Navy (DoN)\nDON Sampling and Analysis Plan (SAP)\n\n\nFederal\nUS Environmental Protection Agency (USEPA) \nHow EPA Manages the Quality of its Environmental Information - US EPA\n\n\nFederal\nUS Environmental Protection Agency (USEPA) \nGuidance for Quality Assurance Project Plans, EPA QA/G-5 - US EPA\n\n\nFederal\nUS Environmental Protection Agency (USEPA) \nGuidance on Systematic Planning Using the Data Quality Objectives Process, EPA QA/G-4 - US EPA\n\n\nFederal\nUS Environmental Protection Agency (USEPA) \nAgency-wide Quality Program Documents - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 1 \nManaging the Quality of Environmental Data at EPA Region 1 - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 2 \nUS EPA Region 2 Guidance for the Development of Quality Assurance Project Plans for Environmental Monitoring Projects - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 3 \nEPA Region 3 Quality Assurance Project Plans - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 4 \nRegion 4 Quality Assurance Project Plan (QAPP) Tool Box - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 5 \nManaging the Quality of Environmental Data at EPA Region 5 - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 6 \nManaging the Quality of Environmental Data at EPA Region 6 - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 7 \nManaging the Quality of Environmental Data at EPA Region 7 - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 8 \nManaging the Quality of Environmental Data at EPA Region 8 - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 9 \nRegion 9 Wetlands Quality Assurance Project Plan Guidance and Template - US EPA\n\n\nRegional EPA  Guidance\nUS EPA Region 10 \nQuality Assurance Project Plans for Tribes in Region 10 - US EPA\n\n\nState Guidance\nAlaska (ADEC)\nDivision of Water Quality AssuranceDivision of Air Quality Monitoring and Quality Assurance\n\n\nState Guidance\nConnecticut (DEEP)\nQuality Assurance (ct.gov)\n\n\nState Guidance\nIllinois (IEPA) \nGuidance - Resource Assessments (illinois.gov)\n\n\nState Guidance\nIndiana (IDEM) \nIDEM: Nonpoint Source: Quality Assurance Project Plan Template Instructions\n\n\nState Guidance\nMassachusetts (DEP)\nhttps://www.mass.gov/lists/quality-assurance-project-plans\n\n\nState Guidance\nMinnesota (MPCA) \nMPCA Quality Assurance Project Plan Guidance (p-aeo2-13)Underground Storage Tank and Petroleum Remediation Quality Assurance Program Plan (p-eao2-04)Toxic Substances Control Act Polychlorinated Biphenyl Inspection Program QAPP (p-eao2-08)\n\n\nState Guidance\nMissouri (DNR) \nMissouri Department of Natural Resources Data Quality Management Brownfields Quality Assurance Project Plan TemplatesData Quality Management - Missouri Department of Natural Resources (mo.gov)\n\n\nState Guidance\nNew Jersey (DEP)\nQuality Assurance Project Plan Technical Guidance\n\n\nState Guidance\nNew York State (NYSDEC) \nQuality Assurance and Quality Control of Water Quality Data\n\n\nState Guidance\nSouth Carolina (DHEC) \nGuidance Document for Preparing Quality Assurance Project Plans (QAPPs) for Environmental Monitoring Projects/StudiesUnderground Storage Tank (UST) Quality Assurance Program Plan (QAPP)\n\n\nState Guidance\nTexas (TCEQ)\nQuality Assurance Project Plans for Nonpoint Source ProjectsQuality Assurance and Monitoring Procedures for Surface Water Quality MonitoringAir Quality Research: Quality AssuranceTexas Risk Reduction Program\n\n\nState Guidance\nWashington State (Ecology) \nGuidelines for Preparing Quality Assurance Project Plans for Environmental StudiesWashington State Department of Ecology Quality Management Plan 2020"
  },
  {
    "objectID": "content/data-management/index.html",
    "href": "content/data-management/index.html",
    "title": "Data Management",
    "section": "",
    "text": "Data management staff support internal and external clients by compiling, organizing, and standardizing data so that it is available and suitable for technical analyses; by conducting geospatial analyses and producing map figures; by carrying out a variety of data summarization and data analysis tasks including statistical analyses, graphic visualization, and data modeling; and by developing web sites for presentation of data and analytical results.\nData are the foundation of most of the decision support services. Interpretations, assessments, judgments, and recommendations can only be as good as the underlying data. Because data are often voluminous and complex, data management practices should have established goals, standards, practices, and tools for handling data efficiently and consistently."
  },
  {
    "objectID": "content/data-management/index.html#goals",
    "href": "content/data-management/index.html#goals",
    "title": "Data Management",
    "section": "Goals",
    "text": "Goals\nData management standards and practices are designed to meet the following goals:\n\n\n\nEstablish and maintain the highest level of data quality that is consistent with the needs of each project. Six dimensions of data quality, and the approaches to handling them, are described on the Data Quality page.\nEstablish data security: ensuring that clients’ data are available only to authorized project staff, and that data revisions are controlled and documented.\nEnsure that data are accessible and available to project staff either directly or with the assistance of experienced data managers.\nCarry out data summarization and analysis using consistent, efficient, and technically appropriate methods, including both standard and cutting-edge analysis methods.\nDocument the provenance, handling, and history of each data set, providing traceability analogous to chain-of-custody procedures for digital data."
  },
  {
    "objectID": "content/data-management/index.html#data-management-standards",
    "href": "content/data-management/index.html#data-management-standards",
    "title": "Data Management",
    "section": "Data Management Standards",
    "text": "Data Management Standards\nThe following data management standards help assure that data management activities are carried out efficiently, consistently, flexibly, and reliably within and across projects.\n\nUse of a centralized, standardized, authoritative database (ordinarily IDB for environmental data).\nAdhering to data management best practices.\nUse of scripts to perform all changes to data.\nDocumentation\n\nData Management Plan\nCompleted checklists\nScript documentation\nIssue logs\n\nTemplates for entry and uploading of field sampling information.\nChecklists for project initiation, data set evaluation, and other activities\nEDD templates for laboratory analytical results"
  },
  {
    "objectID": "content/data-management/index.html#data-management-activities",
    "href": "content/data-management/index.html#data-management-activities",
    "title": "Data Management",
    "section": "Data Management Activities",
    "text": "Data Management Activities\nData management is not a single activity that occurs at a discrete point in a project. Data management consists of a set of interrelated activities–acquisition, organization, summarization, analysis, and presentation of data, among others–that runs through, and connects, other project tasks. Project tasks, and their data management elements, are described briefly below.\n\n\n\n\n\n\nProject Planning\nIncorporation of data management planning into overall project planning helps to ensure that appropriate staff, tools, and budget are available, and that schedules and responsibilities are aligned so that data quality objectives are met. The pages on data management plans, data management checklists, workflows for sampling and historical data, and budgeting provide additional information and tools.\n\n\nData Acquisition\nProject data may be obtained from a variety of sources, each needing different strategies to identify, obtain, review, and standardize the data. Data management staff can lead, guide, or perform these activities.\n\n\nData Quality Assessment\nAssessment of data quality may be required at several different points in a project, and vary depending on the type and origin of the data. Data management staff work closely with analytical chemists, quality assurance specialists, and other technical staff to perform or support screening-level and in-depth technical analyses of data quality. The Data Quality page contains more in-depth information on the dimensions of data quality and our approaches to assess, improve, and document it.\n\n\n\n\n\n\n\nData Organization, Standardization, and Centralization\nCore elements of data quality are that data are unambiguous, do not contain internal inconsistencies, and are centrally available to avoid problems resulting from multiple versions of a data set. Organization of data to meet these requirements is one of the core functions of data management activities. Key goals of these data management activities are to ensure that data are:\n\nAs correct as possible, internally consistent, and unambiguous\nHandled consistently across all projects\nCentralized to prevent proliferation of multiple inconsistent versions\nReadily available to project technical staff in whatever formats are needed\nManaged efficiently and consistently across changes in staff availability.\n\nData management staff use databases instead of spreadsheets because databases provide better data integrity, speed, documentation, replicability of operations, QA-ability of operations, security, and automated backups.\n\n\nDocument and file management\nDocuments and files must not only ordinarily be managed to establish the provenance of project data, but management of documents or files may be a completely separate project goal or requirement. Different tools are appropriate–and available–for document and file management in different circumstances.\n\n\n\n\n\n\n\nData Summarization, Analysis, Reporting, and Visualization\nMany complex data summaries can be automated so that they can be conducted reliably and efficiently, and easily revised and updated. Quality assurance reviews can be more easily conducted on automated (scripted) processes than on point-and-click processes. Database, GIS, and statistical tools all support automation of data summarization, analysis and visualization tasks. A set of standard data summaries can be easily adapted to meet most common data summarization and analysis needs.\n\n\n\n\n\n\n\nData exchange with clients, agencies, and other consultants\nWhen data are to be provided to clients or other consultants, the data should be complete, have well-established data integrity, have documented data quality, and be provided in a well-defined format.\n\n\nProject closeout\nWhen a project is completed, or even when it is put on hold for a lengthy period, there may be tracking and documentation tasks to be completed. Project data may be set to read-only so that it cannot be subsequently modified in any way for any reason. Automatic backups of project data should ordinarily be discontinued and a final archive copy of the complete set of data created. These steps will ensure that the future value of the data will not be lost to the client.\nMany project activities such as sample collection, sample analysis, and data analysis occur in a sequence, like beads on a thread. However, data management activities, like project management activities, run through and connect all of these other activities.\n\n\n\n\n\nAn analogy can also be made to the somatic systems of a vertebrate: there are organs with specialized functions like the lungs, liver, kidneys, eyes, skin, and musculo-skeletal systems, and there are also systems that connect these so that they interact and work together: the nervous system (a project management analogue) and the circulatory system (a data management analogue).\nThe data management activities that are carried out throughout a project’s lifecycle are conducted partly in sequence and partly in parallel."
  },
  {
    "objectID": "content/data-management/index.html#data-management-planning",
    "href": "content/data-management/index.html#data-management-planning",
    "title": "Data Management",
    "section": "Data Management Planning",
    "text": "Data Management Planning\n\n\nPlanning for data management activities helps to avoid later surprises about the type or amount of work required to support creation of project deliverables. Data management activities should be integrated with other project activities, and both the tasks to be completed and the level of effort required should be planned so that they contribute to meeting project goals. Planning data management activities early in the project helps to ensure that they are carried out consistently and efficiently, and that appropriate tools and skilled personnel are available. In some cases you may get by without planning, relying only on the skills and experience of data management staff. In those cases, however, data management activities for your project will most likely be carried out according to the plan for some other project–or maybe even pieces and parts of several different projects if several people work on the project without any common plan.\nData management activities should be incorporated into the project plan. A standalone data management plan (DMP) may be required for projects conducted under the oversight of regulatory agencies, but even when it is not required, a DMP will provide all project staff with a reference and resource for conducting the work consistently and efficiently. Documenting data management standards and approaches is particularly valuable for projects where personnel may change over time. A DMP need not be complex, and may evolve over the course of a project. To simplify the creation of a project-specific DMP, a template is available to use as a foundation. This template describes default standards and approaches for data management activities, and may be useful for project planning even if a project-specific DMP is not required."
  },
  {
    "objectID": "content/data-management/index.html#documentation",
    "href": "content/data-management/index.html#documentation",
    "title": "Data Management",
    "section": "Documentation",
    "text": "Documentation\nDocumentation of project data management activities includes some or all of the following components:\n\nA data management plan (DMP)\nA data managers’ manual (DMM)\nData management SOPs\nA revision date and the data manager’s name automatically recorded on every row of every table\nAn audit log that automatically records every addition, deletion, and modification to every table\nNotes in the header of SQL scripts\nLogs of data issues and resolutions\nLogs of script actions\nCustom logs of script actions that may be created by the data manager and some standard scripts.\n\nSee the page on data management procedures, processes, and workflows for additional information."
  },
  {
    "objectID": "content/data-management/index.html#data-management-resources-and-tools",
    "href": "content/data-management/index.html#data-management-resources-and-tools",
    "title": "Data Management",
    "section": "Data Management Resources and Tools",
    "text": "Data Management Resources and Tools\n\nBoilerplate text and presentations on data management topics.\nExecSQL, a Python program that is the primary tool for automating data management operations.\nAdditional software tools for data manipulation, available to network and Citrix users. Some of these are open-source, others were developed by data management staff.\nDesign documents and tools for environmental databases, including:\n\nSQL code to initialize a new PostgreSQL instance of the database\nSpecifications for field and laboratory electronic data deliverables (EDDs), including import procedures and SQL scripts to carry out QA checks and data loading.\nA library of SQL code to carry out common (or complex) data summarizations\nA (Python) software tool for running a SQL script on multiple PostgreSQL databases sequentially.\nAn Access template for loading and export of data (using the dbmigrator program, see below)\n\nTools to review the inventory of project databases.\nSoftware tools to assist with manipulation of data files (e.g., automated editing and crosstabbing and un-crosstabbing CSV files).\nA (Python) software tool to assist with exporting database audit log data.\nArcGIS templates.\nCommon data, such as data sets with national scope.\nProject-specific data sets and ArcGIS project files used by GIS staff.\nBoilerplate text and presentations related to data analysis\nTechnical approaches and guidance on a variety of topics\nA library of R scripts for data analysis and presentation\nA library of MATLAB scripts for data analysis"
  },
  {
    "objectID": "content/data-management/processes/data-management-workflow-for-sampling-data.html",
    "href": "content/data-management/processes/data-management-workflow-for-sampling-data.html",
    "title": "Data Management Workflow for Sampling Data",
    "section": "",
    "text": "This page describes data management processes and data flows for projects (or project tasks) that include the collection and chemical analysis of environmental samples. An alternative workflow for the compilation and synthesis of data from other sources is described on the companion wiki page Data Management Workflow for Historical Data.\nData management processes and data flows are shown in the following data flow diagram (DFD). Each of the processes and data flows are described in following sections. This DFD uses Yourdon/DeMarco symbology, as described by Wikipedia. Note that a DFD is not a flowchart, and the arrows in the DFD do not represent sequence or control flow. A flowchart analogue follows the DFD, but the DFD is a more detailed and useful representation for a data-based workflow.\nThe following diagrams can be viewed more clearly by zooming this page or by downloading the images or viewing them in a separate browser tab."
  },
  {
    "objectID": "content/data-management/processes/data-management-workflow-for-sampling-data.html#processes",
    "href": "content/data-management/processes/data-management-workflow-for-sampling-data.html#processes",
    "title": "Data Management Workflow for Sampling Data",
    "section": "Processes",
    "text": "Processes\n\nP1. Data Management Planning\nThis process should ordinarily be conducted as part of project planning. It should be carried out jointly by the project manager and the project data manager. This process may include:\n\nIdentification of data needed for analyses and reporting, based on project goals (the DQO process).\nIdentification of data needed for export, if data are to be transmitted to other organizations.\nCompletion of a project initiation checklist.\nDevelopment of a scope description for data management activities.\nIdentification of the data model needed and requirements for field sample identification.\nIdentification of analytical laboratories, analytical requirements, and electronic data deliverable (EDD) formats.\nDetermination of the coordinate reference system and basemap data to be used.\nBudgeting for data management activities.\nDevelopment of a Data Management Plan.\nInitiation of a project database and GIS project.\n\nOther activities may be conducted and products produced if the project will require other types of data, such as historical data.\n\n\nP2. Field Program Planning\nThis process should be carried out jointly by the field program lead, field staff, and project data manager. The process may include:\n\nSpecification of procedures, tools, and forms to be used to collect information in the field.\nSpecification of rules for naming and labelling locations, collections, interpretive samples, analytical samples, and containers, and documentation in a SAP or FSP.\nInitialization of GPS hardware, software, and data.\nTraining of field staff in field documentation requirements and procedures.\n\n\n\nP3. Field Sampling\nThis process is carried out by field staff. The essential activity with respect to data management is the collection of field documentation as specified in the SAP or FSP.\n\n\nP4. Field EDD Compilation\nSampling information collected in the field must ordinarily be compiled into a format that represents location and sampling information in a form consistent with the data model used for the project. Field EDD forms can be used for this purpose, for data that will be loaded into the database. This process should be carried out by field staff, with support from the project data manager.\n\n\nP5. Import Field Data Into the Database\nField sampling information is imported into the project database from the field EDD formats (or other formats as determined during planning). This process is carried out by the data manager, but may require consultation with field staff if data quality issues are found with the field records. The standard procedure is to carry out this process using a SQL script. The wiki page Workflows for Loading Data to Postgres describes the data loading process in more detail.\n\n\nP6. Laboratory Oversight\nThis process should be carried out by the project chemist. Laboratory specifications, laboratory deliverables, and any additional analysis requests should all be handled by the project chemist as part of this process. Coordination with data validators is also carried out as part of this process. Ordinarily this process includes communication between the project chemist and the data manager regarding timing, format, and quality of the laboratory deliverables and validation results.\n\n\nP7. Load Lab EDD\nThis process is carried out by the project data manager. Laboratory EDDs are loaded into the project database, and quality assurance checks of the data are carried out as part of this process. The standard procedure is to carry out this process using a SQL script. If the laboratory EDD format is one that has been previously used, then an existing loading script may be used or customized; if not, a new script will have to be developed (and if so, a budget for this effort should have been established during the planning process). The wiki page Workflows for Loading Data to Postgres describes the data loading process in more detail, particularly when a script must be developed for a new EDD format. If problems are found with the EDD format or content, the data manager should communicate these to the project chemist for resolution with the laboratory.\n\n\nP8. Produce Validation Summary\nThis process is carried out by the project data manager. After each laboratory EDD is loaded to the database, those data are extracted and summarized into a spreadsheet for use by the data validators. This procedure is carried out using a SQL script.\n\n\nP9. Validate Analytical Results\nThis process is carried out by validation chemists, either in-house or contracted. This process results in the assignment of validation qualifiers to analytical results. Those qualifiers should be recorded in the validation summary spreadsheet.\n\n\nP10. Load Validation Qualifiers\nThis process is carried out by the project data manager. Data validation qualifiers are extracted from the data validation summary spreadsheet and added to the corresponding values in the database. This procedure is carried out using a SQL script.\n\n\nP11. Data Usability Assessment\nPrior to conducting statistical, geospatial, or other analyses, the usability of the selected data should be assessed. This process should be carried out by the data analyst and project data manager, depending on the type of assessment to be performed and the best tool or technology for the job. The data usability assessment may make use of metrics for the different dimensions of data quality. This process ordinarily starts with the production of appropriate data summaries by the data manager.\n\n\nP12. Data Analysis and Visualization\nThis process should be carried out primarily by the data analyst, but the first step in analyses will frequently be the selection, extaction, and summarization of data from the database, and that step may be carried out by the project data manager. The Data Accessibility page describes a variety of different ways in which data can be accessed and summarized by either data managers or other project staff. A number of standard data summaries are available that meet many common data needs. These standard summaries may be used as is, or customized. More specialized data summaries may need to be created to support some analyses. Often data summarization could be carried out using either SQL or analysis software (e.g., R, Python, Julia), and the most appropriate approach should be identified through discussions between data analysts and data managers.\nSummaries of chemistry data should use the default rules for summarization of chemistry data or explicitly specify alternate data handling rules. Additional information on data summarization (primarily for data managers) is presented on the pages Producing Custom Data Summaries and Guidance and Tips for Data Managers. Custom scripts have also been developed to prepare data for some types of specialized analyses (e.g., unmixing, HCPCA, and t-SNE). Otherwise, custom data summarization scripts may need to be developed for particular analyses (if so, a budget for this effort should have been developed during the planning process).\nData analyses should follow relevant recommendations on Data Analysis Standards and Guidance and Best Practices for Data Analysis, and avoid the Worst Practices for Data Analysis. An initial stage of data exploration may be facilitated by using some of the available GUI Tools for Data Exploration.\nRequests for data that are to be used for analysis and visualization may be recorded in a request tracker. Templates and examples are in M:_trackers.\nAlthough data analysis and visualization is described here as a single process, frequently it is a multi-step process that could could be described by its own flowchart, data flow diagram, work breakdown structure, or other type of process map. Describing and documenting the planned data analysis process during project planning, and accounting for variances, can help to ensure that this work is carried out in a way that clearly supports project goals and is performed in an orderly and efficient fashion.\n\n\nP13. Data Table Production\nThis process is carried out by the project data manager. Data to be reported as a project deliverable are extracted from the project database and restructured and formatted as appropriate. This process is ordinarily scripted. The wiki page Producing Data Summaries describes the major steps in this process. The section “Producing Data Tables for Reports or for Data Analyses” on the wiki page Guidance and Tips for Data Managers contains guidelines for the production of report data tables.\nRequests for data that are to be used for data table production may be recorded in a request tracker. Templates and examples are in M:_trackers.\n\n\nP14. Data Reporting\nThis process is carried out cooperatively by multiple project staff. Reports may include data tables, graphics, and maps prepared by DMA staff and others. This process ordinarily requires that more effort be applied to consistency of data representation and adherence to publication standards than do the data analysis and visualization activities.\n\n\nP20. Correct Data Quality Issues\nInitiation of data analyses, and the associated data usability assessment (P11) may reveal data quality issues that were not identified during initial data compilation and loading. These issues are typically identified by a data analyst, and resolution of an issue may involve data managers, data analysts, field staff, and the project manager. These issues may be recorded in a data issue log while a resolution is being developed (use of a data issue log is shown as part of the workflow for historical data).\n\n\nP21. Export Data\nThis process may not be required for all projects. When data must be exported to a regulator or some other organization, the data ordinarily must be retrieved from the project database and prepared in a specified data exchange format or EDD. The EDD format that is used for data exchange between organizations is typically different from the EDD format in which laboratory results are delivered. The data exchange EDD format may be specified by the external organization or may be determined by a negotiation between Integral and that organization. Some external organizations may require data that Integral would not typically record (e.g., during field work), and so export data requirements must be considered during project and data management planning. This task is ordinarily carried out by data managers, but may require some support from other project staff.\nExporting data should be carried out following the process described on the wiki page Producing Data Summaries. Requests for data that are to be exported for others may be recorded in a request tracker."
  },
  {
    "objectID": "content/data-management/processes/data-management-workflow-for-sampling-data.html#data-flows",
    "href": "content/data-management/processes/data-management-workflow-for-sampling-data.html#data-flows",
    "title": "Data Management Workflow for Sampling Data",
    "section": "Data Flows",
    "text": "Data Flows\n\nF1. Planning Products\nThe planning products in this data flow may contain the following items (note that items below link to more detailed documentation):\n\nA project initiation checklist\nData quality goals and standards for the project\nA data management plan (DMP) or equivalent information in a work plan\nSpecifications for location and sample identifiers to be used in the field, specified in a DMP, work plan, or field sampling plan (FSP)\nSpecification for electronic data deliverables (EDDs) for field and laboratory data\nIdentification of standard or custom data management SOP\nSpecifications of analyses to be conducted, methods to be used, and chemical data validation procedures to be followed\nAn initial data manager’s manual (DMM).\n\n\n\nF2. Inputs for Field Program Planning\nThis data flow consists of a subset of planning products from process P1, including:\n\nSpecifications for location and sample identifiers to be used in the field, specified in a DMP, work plan, or field sampling plan (FSP)\nSpecification for electronic data deliverables (EDDs) for field data\nStandard or custom data management SOPs\nA data dictionary for information to be recorded on field forms and in field EDDs.\n\n\n\nF3. Field Program Planning Documents for Library\nThis data flow consists of documents produced by the field program planning process that are to be maintained as a resource and reference. This include:\n\nThe field sampling plan (FSP)\nAnalytical requirements to be provided to the laboratory.\n\n\n\nF4. Field Program Planning Documents for Field Crew\nThis data flow consists of information produced by the field planning process that are to be used by field staff. This information is ordinarily compiled into an FSP, which is the sole component of this data flow. (Other documents to be used by the field staff, such as a Health and Safety Plan, are not part of the data management workflow.)\n\n\nF5. Field Sampling Records\nThis data flow consists of all digital and paper documents produced during the course of field sampling and may include, but not be limited to:\n\nField logbooks\nCompleted field forms\nGPS records\nInstrument data records\nChain of custody (COC) forms\nAnalysis request forms\nAll emails and other recorded communications pertaining to the conduct of field operations, specifically related to data collection.\n\n\n\nF6. COCs and Analytical Requests\nThis data flow consist of all documents and information conveyed from field staff to the analytical laboratories, including COC forms, analytical requests, and emails and records of other communications pertaining to sample analysis and data handling.\n\n\nF7. Standardized Field Sampling Records\nThis data flow consist of:\n\nField sampling information compiled into a field EDD format, or an alternative as specified during the planning process.\nGPS data\nField instrument data\n\n\n\nF8. Analytical Plans\nThis data flow consists of specifications for laboratory analyses, including:\n\nAnalyses to be conducted\nAnalytical methods to be used\nData quality standards to be met by the laboratory\nThe laboratory EDD format to be used\nLevel of data validation to be conducted.\n\n\n\nF9. Analytical Specifications for the Laboratory\nThis data flow consists of specifications and work requests provided to the analytical laboratory, and ordinarily includes:\n\nA description of analyses to be conducted, the methods to be used, and the format and content of laboratory reports and EDDs to be produced by the laboratory, typically in the form of a Statement of Work (SOW)\nAny requests for sample reanalyses that are needed following initial review of laboratory deliverables.\n\nThe first of these items will ordinarily be transmitted to the laboratory one time, after project planning is completed, whereas the second item (reanalysis requests) may be transmitted on multiple occasions.\n\n\nF10. Analytical Laboratory Results\nThis data flow consists of laboratory deliverables as specified in the SOW, and ordinarily includes:\n\nA laboratory report for each sample delivery group (SDG) as a PDF file\nOne or more completed laboratory EDDs in the specified format.\n\n\n\nF11. Normalized Field Sampling Records\nThis data flow consists of field sampling information that is transformed from the field EDD format into the structure of database tables. This data flow is electronic, and occurs between the data loading script and the database server. The data loading process (P5) is described in more detail on the wiki page Workflows for Loading Data to Postgres.\n\n\nF12. Laboratory EDD\nThis data flow consists of the laboratory EDD, transmitted from laboratory oversight staff to data management staff after review, tracking, and approval.\n\n\nF13. Normalized Analytical Laboratory Results\nThis data flow consists of data from the laboratory EDD that is transformed into the structure of database tables. This data flow is electronic, and occurs between the data loading script and the database server. The data loading process (P7) is described in more detail on the wiki page Workflows for Loading Data to Postgres.\n\n\nF14. Analytical Laboratory Results Extracted from Database\nThis data flow consist of analytical results for one or more SDGs that is used to prepare data summaries to be used by data validators. This data flow is electronic and occurs between the database server and the data summary script.\n\n\nF15. Analytical Laboratory Results Summarized for Data Validators\nThis data flow consist of analytical results for one or more SDGs that is extracted from the project database (F14), summarized in a format that is designed to be annotated by the data validators.\n\n\nF16. Results of Data Validation\nThis data flow consists of the summary of analytical results (F15) annotated with data validation qualifiers and validator comments, and possibly including revisions to data values and detection limits.\n\n\nF17. Data Validation Qualifiers\nThis data flow consists of new information provided by data validators (F16) that is used to update existing analytical results in the project database. This data flow is electronic and occurs between the data loading script and the database server.\n\n\nF18. Field Records for Library\nThis data flow consists of all digital and paper documents produced during the course of field sampling and may include, but not be limited to:\n\nField logbooks\nCompleted field forms\nGPS records\nInstrument data records\nChain of custody (COC) forms\nAnalysis request forms\nAll emails and other recorded communications pertaining to the conduct of field operations, specifically related to data collection.\n\n\n\nF19. Analytical Laboratory and Lab Oversight Records for Library\nThis data flow consists of all laboratory deliverables and laboratory oversight records and may include but not be limited to:\n\nLaboratory SOWs\nData reports and EDDs produced by the laboratory\nTracking records used to oversee correct and timely completion of laboratory wor\nReanalysis requests sent to the laboratory\nEmails and other communications pertaining to the analysis and reporting of data.\n\n\n\nF20. Data for a Task- or Analysis-Specific Data Analysis\nThis data flow ordinarily will consist of one or more subsets of data that are selected based on the needs of a specific data analysis. The work flow is electronic, occurring between the database server and the SQL script used to select, summarize, and restructure the data as needed for analyses to be conducted using ArcGIS, R, QGIS, Orange, Python, or other tools.\n\n\nF21. Data for Deliverable Data Tables\nThis data flow consists of all sampling information and analytical data that are to be included in deliverable data reports.\n\n\nF22. Selected or Annotated Data For Analysis or Visualization\nThis data flow consists of the data from F20, summarized and restructured as appropriate for each data analysis to be conducted.\n\n\nF23. Final Analysis Results for Interpretation and Deliverable Preparation\nThis data flow consist of all the results of data analyses that are essential for understanding, decision-making, and reporting. It may include:\n\nData tables used as input to the analysis\nData tables produced by the analysis\nMaps showing data or the results of data analyses\nStatistical graphics showing data or the results of data analyses.\n\n\n\nF24. Report Data Tables\nThis data flow consists of all sampling information and analytical data that are to be included in deliverable data reports (F21), restructured into deliverable data tables. Guidance for production of these tables is in the section titled “Producing Data Tables for Reports or for Data Analyses” of the wiki page Guidance and Tips for Data Managers. The tables in this data flow ordinarily will require formatting (typically by production staff) and addition of table numbers and titles.\n\n\nF25. Deliverable Data Tables, Maps, and Graphics\nThis data flow consist of all of the materials from data flows F23 and F24, compiled and prepared in a form for delivery to the client.\n\n\nF26. Data Quality Issues\nThis data flow consists of a description, and possibly examples, of data quality issues that are identified when data are used. This data flow may include emails, spreadsheets, example scripts, and entries in an issue tracking log.\n\n\nF27. Data Updates\nThis data flow consists of revisions to the project database and ordinarily takes the form of changes made using scripted SQL statements.\n\n\nF28. Sampling Information and Analytical Results for Data Export\nThis data flow consists of all project data that is to be exported from the database for transmittal to another organization. This data flow is ordinarily completely electronic, through a connection between the database and the export script.\n\n\nF29. Data Export EDD\nThis data flow consists of project data that has been transformed into a data exchange EDD format as specified by, or agreed to with, the receiving organization."
  },
  {
    "objectID": "content/data-management/processes/index.html",
    "href": "content/data-management/processes/index.html",
    "title": "Data Management Procedures, Processes, and Workflows",
    "section": "",
    "text": "This page contains a centralized list of various data management processes, guidelines, and other information describing common operations.\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\nReading Time\n\n\n\n\n\n\nEstimating Data Management Efforts\n\n\nConsiderations for estimating data management efforts.\n\n\n12 min\n\n\n\n\nData Management Workflow for Sampling Data\n\n\nData management workflow for sampling data.\n\n\n19 min\n\n\n\n\nData Management Workflow for Historical Data\n\n\nData management wokflow for historical data.\n\n\n13 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/development/code/cmd.html",
    "href": "content/development/code/cmd.html",
    "title": "CMD",
    "section": "",
    "text": "@echo off\ncls\n\n@REM Set the path to the script root (using UNC path). Note the string is not quoted.\nset SCRIPT_DIR=\\\\integral-corp.com\\data\\&lt;Project Number Range&gt;\\&lt;Project Folder&gt;\\Working_Files\\DataManagement\\IDB\\export\\script\n\n@REM Name of the script to execute. Note the string is not quoted.\nset SCRIPT_NAME=export.sql\n\n@REM Prompt the user for their Postgres password\n@REM This allows for users to run the execsql script who might not have\n@REM a custom execsql.conf file specified with their username.\nset /p \"DB_USER=Enter you PostgreSQL username: \"\n\n@REM Run the execsql script\nPowerShell -NoProfile -ExecutionPolicy Bypass -Command \"& Set-Location -Path %SCRIPT_DIR%; M:\\DataManagement\\bin\\execsql.py -u %DB_USER% %SCRIPT_NAME%\"\n\npause"
  },
  {
    "objectID": "content/development/code/cmd.html#run-a-python-script",
    "href": "content/development/code/cmd.html#run-a-python-script",
    "title": "CMD",
    "section": "",
    "text": "@echo off\ncls\n\n@REM Set the path to the script root (using UNC path). Note the string is not quoted.\nset SCRIPT_DIR=\\\\integral-corp.com\\data\\&lt;Project Number Range&gt;\\&lt;Project Folder&gt;\\Working_Files\\DataManagement\\IDB\\export\\script\n\n@REM Name of the script to execute. Note the string is not quoted.\nset SCRIPT_NAME=export.sql\n\n@REM Prompt the user for their Postgres password\n@REM This allows for users to run the execsql script who might not have\n@REM a custom execsql.conf file specified with their username.\nset /p \"DB_USER=Enter you PostgreSQL username: \"\n\n@REM Run the execsql script\nPowerShell -NoProfile -ExecutionPolicy Bypass -Command \"& Set-Location -Path %SCRIPT_DIR%; M:\\DataManagement\\bin\\execsql.py -u %DB_USER% %SCRIPT_NAME%\"\n\npause"
  },
  {
    "objectID": "content/development/code/index.html",
    "href": "content/development/code/index.html",
    "title": "Code",
    "section": "",
    "text": "Bourne Again SHell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand Prompt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperText Markup Language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJavaScript programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollection of Jupyter Notebooks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft PowerShell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular expressions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructured Query Language\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/development/code/index.html#code-pages",
    "href": "content/development/code/index.html#code-pages",
    "title": "Code",
    "section": "",
    "text": "Bourne Again SHell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand Prompt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperText Markup Language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJavaScript programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollection of Jupyter Notebooks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft PowerShell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular expressions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructured Query Language\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/development/code/index.html#character-encoding",
    "href": "content/development/code/index.html#character-encoding",
    "title": "Code",
    "section": "Character Encoding",
    "text": "Character Encoding\nWe deal with character encoding issues when importing data to databases all the time. All databases are encoded in UTF-8, and the most common non-ASCII, non-UTF-8 format is Microsoft’s custom encoding, which is CP-1252 (also known as win-1252 and a few other things). The character encoding of a file can’t necessarily be definitively determined by examination, but the Linux command-line tool file generally does a good job. If you want to change the encoding of a file rather than importing it in a known format, the Linux command-line tool iconv will do that for you. There’s a Python library on PyPI named chardet that will also diagnose file encodings.\nFor data managers, the workflow is to first guess that encoding errors on data import are due to the file being cp-1252. That covers about 90% of cases. Our import tool also automatically diagnoses instances where a file starts with a byte order mark (BOM), which covers most of the rest of the cases. For the remainders, Geany is usually the quickest way to check the file encoding.\nEverything that comes out of our databases is always in UTF-8, so I, at least, don’t ordinarily have encoding issues when importing data to R. For those who use data from other sources, it is a good idea to document a recommended workflow and set of tools."
  },
  {
    "objectID": "content/development/code/notebooks/index.html",
    "href": "content/development/code/notebooks/index.html",
    "title": "Notebooks",
    "section": "",
    "text": "Data Science\n\n\nIntro to data science with Python\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGRIB\n\n\nManipulating gridded datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogging\n\n\nBasics of the standard library logging module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML Notes\n\n\nMachine Learning (supervised learning) notes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML Training\n\n\nTest a machine learning model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPySpark Practice\n\n\nIntroduction to Apache Spark | PySpark\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Fundamentals\n\n\nConcepts and methods on the fundamentals of Python\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSleep Data\n\n\nLoad sleep data into PostgreSQL collected from the Sleep Cycle phone app\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization with Seaborn\n\n\nExploring Matplotlib + Seaborn visualization capabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\n\nDownload .grb2 files from WAVEWATCH III web server\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/development/code/python.html",
    "href": "content/development/code/python.html",
    "title": "Python",
    "section": "",
    "text": "import glob\nimport os\n\nfor filename in glob.glob(\"./**/*.ext\", recursive=True):\n    new_name = \"-\".join(filename.split(\"_\"))\n    os.rename(filename, new_name)"
  },
  {
    "objectID": "content/development/code/python.html#batch-file-rename",
    "href": "content/development/code/python.html#batch-file-rename",
    "title": "Python",
    "section": "",
    "text": "import glob\nimport os\n\nfor filename in glob.glob(\"./**/*.ext\", recursive=True):\n    new_name = \"-\".join(filename.split(\"_\"))\n    os.rename(filename, new_name)"
  },
  {
    "objectID": "content/development/code/python.html#logging-basics",
    "href": "content/development/code/python.html#logging-basics",
    "title": "Python",
    "section": "Logging Basics",
    "text": "Logging Basics\nimport argparse\nimport logging\nimport os\nfrom datetime import datetime\n\n# Do not specify __name__ to use root log level\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\n    \"%(asctime)s : %(msecs)04d : %(name)s : %(levelname)s : %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlog_file = (\n    f\"{os.path.splitext(__file__)[0]}_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.log\"\n)\nstream_handler = logging.StreamHandler()\nstream_handler.setFormatter(formatter)\n\n\ndef clparser() -&gt; argparse.ArgumentParser:\n    \"\"\"Create a parser to handle input arguments and displaying a help message.\"\"\"\n    desc_msg = \"\"\"My logging program.\"\"\"\n    parser = argparse.ArgumentParser(description=desc_msg)\n    parser.add_argument(\n        \"-l\",\n        \"--logfile\",\n        action=\"store_true\",\n        help=\"Write log messages to a file.\",\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Control the amount of information to display.\",\n    )\n    return parser\n\n\nif __name__ == \"__main__\":\n    args = clparser().parse_args()\n    if args.verbose:\n        logger.addHandler(stream_handler)\n    if args.logfile:\n        file_handler = logging.FileHandler(filename=log_file)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n    logger.info(\"Begin my test module\")"
  },
  {
    "objectID": "content/development/code/python.html#sending-http-requests",
    "href": "content/development/code/python.html#sending-http-requests",
    "title": "Python",
    "section": "Sending HTTP Requests",
    "text": "Sending HTTP Requests\nimport logging\n\nimport requests\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_request(request_type: str, url: str, **kwargs) -&gt; requests.Response:\n    \"\"\"Send an HTTP request.\n\n    Args:\n    ----\n        request_type (str): Accepts \"GET\" or \"POST\"\n        url (str): Request URL\n\n    Returns:\n    -------\n        requests.Response: Request response.\n    \"\"\"\n    valid_methods = (\"GET\", \"POST\")\n    if request_type.upper() not in valid_methods:\n        raise ValueError(f\"Invalid request type. Supported types: {valid_methods}\")\n    try:\n        response = requests.request(request_type.upper(), url, **kwargs)\n        response.raise_for_status()  # Raises an exception if status code &gt;= 400\n        return response\n    except requests.exceptions.RequestException as err:\n        logger.error(f\"{err}. Request type: {request_type}. URL: {url}. Args: {kwargs}\")\n        raise err\n\n# Example GET request\nresponse = send_request(\"GET\", \"https://api.publicapis.org/entries\", timeout=5)\nlogger.info(response.json())\n\n# Example POST request\nresponse = send_request(\"POST\", \"https://someurl.com\", headers={}, timeout=5)\nlogger.info(response.json())"
  },
  {
    "objectID": "content/development/code/python.html#regular-expressions",
    "href": "content/development/code/python.html#regular-expressions",
    "title": "Python",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nSee here for more information on regex syntax.\n\n\nCode\nimport re\n\ntext_string = \"\"\"\nHello world\n\n8001234567\n800-321-7654\n900.987.6543\n\nsome.email@email.com\nmycompany@company.net\nwierd-12-address-4@somedomain.blah\n\"\"\"\n\npattern = re.compile(r\"[0-9]{3}[.-]?[0-9]{3}[.-]?[0-9]{4}\")\nmatches = re.finditer(pattern, text_string)\n\nfor match in matches:\n    print(match)\n    print(match.span())\n    print(text_string[match.start() : match.end()])\n\n\n&lt;re.Match object; span=(14, 24), match='8001234567'&gt;\n(14, 24)\n8001234567\n&lt;re.Match object; span=(25, 37), match='800-321-7654'&gt;\n(25, 37)\n800-321-7654\n&lt;re.Match object; span=(38, 50), match='900.987.6543'&gt;\n(38, 50)\n900.987.6543"
  },
  {
    "objectID": "content/development/code/python.html#oop-basics",
    "href": "content/development/code/python.html#oop-basics",
    "title": "Python",
    "section": "OOP Basics",
    "text": "OOP Basics\n\n\nCode\nclass Employee:\n    \"\"\"Create an employee object with relevant attributes.\"\"\"\n\n    annual_raise_pct = 0.04\n    employee_no = 1\n\n    def __init__(self, first_name, last_name, position, years_employed=1):\n        \"\"\"Function called when new object is initiated.\"\"\"\n        self.first_name = first_name\n        self.last_name = last_name\n        self.position = position\n        self.years_employed = years_employed\n        self.employee_number = Employee.employee_no\n        self.email = self.first_name + \".\" + self.last_name + \"@company.com\"\n        self.salary = self.calculate_salary()\n        Employee.employee_no += 1\n\n    def starting_salary(self):\n        \"\"\"Return the starting salary for each position at company\"\"\"\n        if self.position == \"HR\":\n            return 25000\n        elif self.position == \"Management\":\n            return 50000\n        elif self.position == \"Developer\":\n            return 100000\n        elif self.position == \"CEO\":\n            return 200000\n        else:\n            return None\n\n    def calculate_salary(self):\n        salary = self.starting_salary()\n        for year in range(self.years_employed):\n            salary = int(salary + (salary * Employee.annual_raise_pct))\n        return salary\n\n    def __repr__(self):\n        \"\"\"Create representational object string\"\"\"\n        return f\"\"\"Employee(first_name = {self.first_name}, last_name = {self.last_name}, position = {self.position}, years_employed = {self.years_employed})\"\"\"\n\n\ncurrent_employees = [\n    Employee(\"Geo\", \"Coug\", \"Developer\", 6),\n    Employee(\"Jane\", \"Doe\", \"HR\", 4),\n    Employee(\"John\", \"Doe\", \"Management\", 15),\n    Employee(\"Bob\", \"Loblaw\", \"CEO\", 24),\n]\n\nfor e in current_employees:\n    print(f\"Employee Number: {e.employee_number}\")\n    print(e)\n    print(\"Email:\", e.email)\n    print(\"Salary: ${:,}\\n\".format(e.salary))\n\n\nEmployee Number: 1\nEmployee(first_name = Geo, last_name = Coug, position = Developer, years_employed = 6)\nEmail: Geo.Coug@company.com\nSalary: $126,530\n\nEmployee Number: 2\nEmployee(first_name = Jane, last_name = Doe, position = HR, years_employed = 4)\nEmail: Jane.Doe@company.com\nSalary: $29,245\n\nEmployee Number: 3\nEmployee(first_name = John, last_name = Doe, position = Management, years_employed = 15)\nEmail: John.Doe@company.com\nSalary: $90,039\n\nEmployee Number: 4\nEmployee(first_name = Bob, last_name = Loblaw, position = CEO, years_employed = 24)\nEmail: Bob.Loblaw@company.com\nSalary: $512,645"
  },
  {
    "objectID": "content/development/code/python.html#dbms-data-types",
    "href": "content/development/code/python.html#dbms-data-types",
    "title": "Python",
    "section": "DBMS Data Types",
    "text": "DBMS Data Types\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"../../../static/development/data-types.csv\")\ndf.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nData Type\n\n\n\nPostgres\n\n\n\nMariaDB\n\n\n\nSQL Server\n\n\n\nFirebird\n\n\n\nMS-Access\n\n\n\nSQLite\n\n\n\n\n\n\n\n\n\n\n\nTimestamp with time zone\n\n\n\n1184.0\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nTimestamp\n\n\n\n1184.0\n\n\n\n7\n\n\n\nNaN\n\n\n\ntype ‘datetime.datetime’\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nDatetime\n\n\n\nNaN\n\n\n\n12\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\n\n\n\n\nDate\n\n\n\n1082.0\n\n\n\n10\n\n\n\nclass ‘datetime.date’\n\n\n\ntype ‘datetime.date’\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\n\n\n\n\nTime\n\n\n\n1083.0\n\n\n\n11\n\n\n\nclass ‘datetime.time’\n\n\n\ntype ‘datetime.time’\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\n\n\n\n\nBoolean\n\n\n\n16.0\n\n\n\n16\n\n\n\nclass ‘bool’\n\n\n\nNaN\n\n\n\nclass ‘bool’\n\n\n\nNaN\n\n\n\n\n\n\n\nSmall integer\n\n\n\n21.0\n\n\n\n1\n\n\n\nclass ‘int’\n\n\n\nNaN\n\n\n\nclass ‘int’\n\n\n\nNaN\n\n\n\n\n\n\n\nInteger\n\n\n\n23.0\n\n\n\n2\n\n\n\nclass ‘int’\n\n\n\ntype ‘int’\n\n\n\nclass ‘int’\n\n\n\nNaN\n\n\n\n\n\n\n\nLong integer\n\n\n\n20.0\n\n\n\n3\n\n\n\nclass ‘int’\n\n\n\ntype ‘long’\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nSingle\n\n\n\n701.0\n\n\n\n4\n\n\n\nclass ‘float’\n\n\n\ntype ‘float’\n\n\n\nclass ‘float’\n\n\n\nNaN\n\n\n\n\n\n\n\nDouble precision\n\n\n\n701.0\n\n\n\n5\n\n\n\nclass ‘float’\n\n\n\ntype ‘float’\n\n\n\nclass ‘float’\n\n\n\nNaN\n\n\n\n\n\n\n\nDecimal\n\n\n\n1700.0\n\n\n\n0\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nCurrency\n\n\n\n790.0\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nNaN\n\n\n\n\n\n\n\nCharacter\n\n\n\n1042.0\n\n\n\nNaN\n\n\n\nclass ‘str’\n\n\n\ntype ‘str’\n\n\n\nclass ‘str’\n\n\n\nNaN\n\n\n\n\n\n\n\nCharacter varying\n\n\n\n1043.0\n\n\n\n15\n\n\n\nclass ‘str’\n\n\n\ntype ‘str’\n\n\n\nclass ‘str’\n\n\n\nNaN\n\n\n\n\n\n\n\nText\n\n\n\n25.0\n\n\n\nNaN\n\n\n\nclass ‘str’\n\n\n\ntype ‘str’\n\n\n\nclass ‘str’\n\n\n\nNaN\n\n\n\n\n\n\n\nBinary / BLOB\n\n\n\n17.0\n\n\n\n249,250,251,252\n\n\n\nclass ‘bytearray’\n\n\n\ntype ‘str’\n\n\n\ntype ‘bytearray’\n\n\n\nNaN"
  },
  {
    "objectID": "content/development/code/python.html#dbms-libraries",
    "href": "content/development/code/python.html#dbms-libraries",
    "title": "Python",
    "section": "DBMS Libraries",
    "text": "DBMS Libraries\n\nPostgres: psycopg2\nMariaDB: pymysql\nSQL Server: pyodbc\nFirebird: fdb\nMS-Access: pyodbc\nSQLite: sqlite3"
  },
  {
    "objectID": "content/development/code/python.html#db-table-dependencies",
    "href": "content/development/code/python.html#db-table-dependencies",
    "title": "Python",
    "section": "DB Table Dependencies",
    "text": "DB Table Dependencies\ndef dependency_order(dep_list):\n    rem_tables = list(set([t[0] for t in dep_list] + [t[1] for t in dep_list]))\n    rem_dep = copy.copy(dep_list)\n    sortkey = 1\n    ret_list = []\n    while len(rem_dep) &gt; 0:\n        tbls = [tbl for tbl in rem_tables if tbl not in [dep[0] for dep in rem_dep]]\n        ret_list.extend([(tb, sortkey) for tb in tbls])\n        rem_tables = [tbl for tbl in rem_tables if tbl not in tbls]\n        rem_dep = [dep for dep in rem_dep if dep[1] not in tbls]\n        sortkey += 1\n    if len(rem_tables) &gt; 0:\n        ret_list.extend([(tb, sortkey) for tb in rem_tables])\n    ret_list.sort(cmp=lambda x, y: cmp(x[1], y[1]))\n    return [item[0] for item in ret_list]"
  },
  {
    "objectID": "content/development/code/python.html#csv-sniffer",
    "href": "content/development/code/python.html#csv-sniffer",
    "title": "Python",
    "section": "CSV Sniffer",
    "text": "CSV Sniffer\nimport re\n\nclass CsvDiagError(Exception):\n    def __init__(self, msg):\n        self.value = msg\n\n    def __str__(self):\n        return self.value\n\nclass CsvLine:\n    escchar = \"\\\\\"\n\n    def __init__(self, line_text):\n        self.text = line_text\n        self.delim_counts = {}\n        self.item_errors = []  # A list of error messages.\n\n    def __str__(self):\n        return \"; \".join(\n            [\n                \"Text: &lt;&lt;%s&gt;&gt;\" % self.text,\n                \"Delimiter counts: &lt;&lt;%s&gt;&gt;\"\n                % \", \".join(\n                    [\n                        \"%s: %d\" % (k, self.delim_counts[k])\n                        for k in self.delim_counts.keys()\n                    ]\n                ),\n            ]\n        )\n\n    def count_delim(self, delim):\n        # If the delimiter is a space, consider multiple spaces to be equivalent\n        # to a single delimiter, split on the space(s), and consider the delimiter\n        # count to be one fewer than the items returned.\n        if delim == \" \":\n            self.delim_counts[delim] = max(0, len(re.split(r\" +\", self.text)) - 1)\n        else:\n            self.delim_counts[delim] = self.text.count(delim)\n\n    def delim_count(self, delim):\n        return self.delim_counts[delim]\n\n    def _well_quoted(self, element, qchar):\n        # A well-quoted element has either no quotes, a quote on each end and none\n        # in the middle, or quotes on both ends and every internal quote is either\n        # doubled or escaped.\n        # Returns a tuple of three booleans; the first indicates whether the element is\n        # well-quoted, the second indicates whether the quote character is used\n        # at all, and the third indicates whether the escape character is used.\n        if qchar not in element:\n            return (True, False, False)\n        if len(element) == 0:\n            return (True, False, False)\n        if element[0] == qchar and element[-1] == qchar and qchar not in element[1:-1]:\n            return (True, True, False)\n        # The element has quotes; if it doesn't have one on each end, it is not well-quoted.\n        if not (element[0] == qchar and element[-1] == qchar):\n            return (False, True, False)\n        e = element[1:-1]\n        # If there are no quotes left after removing doubled quotes, this is well-quoted.\n        if qchar not in e.replace(qchar + qchar, \"\"):\n            return (True, True, False)\n        # if there are no quotes left after removing escaped quotes, this is well-quoted.\n        if qchar not in e.replace(self.escchar + qchar, \"\"):\n            return (True, True, True)\n        return (False, True, False)\n\n    def record_format_error(self, pos_no, errmsg):\n        self.item_errors.append(\"%s in position %d.\" % (errmsg, pos_no))\n\n    def items(self, delim, qchar):\n        # Parses the line into a list of items, breaking it at delimiters that are not\n        # within quoted stretches.  (This is a almost CSV parser, for valid delim and qchar,\n        # except that it does not eliminate quote characters or reduce escaped quotes.)\n        self.item_errors = []\n        if qchar is None:\n            if delim is None:\n                return self.text\n            else:\n                if delim == \" \":\n                    return re.split(r\" +\", self.text)\n                else:\n                    return self.text.split(delim)\n        elements = []  # The list of items on the line that will be returned.\n        eat_multiple_delims = delim == \" \"\n        # States of the FSM:\n        # _IN_QUOTED: An opening quote has been seen, but no closing quote encountered.\n        #  Actions / transition:\n        #   quote: save char in escape buffer / _ESCAPED\n        #   esc_char : save char in escape buffer / _ESCAPED\n        #   delimiter: save char in element buffer / _IN_QUOTED\n        #   other: save char in element buffer / _IN_QUOTED\n        # _ESCAPED: An escape character has been seen while _IN_QUOTED (and is in the escape buffer).\n        #  Actions / transitions\n        #   quote: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        #   delimiter: save escape buffer in element buffer, empty escape buffer,\n        #    save element buffer, empty element buffer / _BETWEEN\n        #   other: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        # _QUOTE_IN_QUOTED: A quote has been seen while _IN_QUOTED (and is in the escape buffer).\n        #  Actions / transitions\n        #   quote: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        #   delimiter: save escape buffer in element buffer, empty escape buffer,\n        #    save element buffer, empty element buffer / _DELIMITED\n        #   other: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        #     (An 'other' character in this position represents a bad format:\n        #     a quote not followed by another quote or a delimiter.)\n        # _IN_UNQUOTED: A non-delimiter, non-quote has been seen.\n        #  Actions / transitions\n        #   quote: save char in element buffer / _IN_UNQUOTED\n        #    (This represents a bad format.)\n        #   delimiter: save element buffer, empty element buffer / _DELIMITED\n        #   other: save char in element buffer / _IN_UNQUOTED\n        # _BETWEEN: Not in an element, and a delimiter not seen.  This is the starting state,\n        #   and the state following a closing quote but before a delimiter is seen.\n        #  Actions / transition:\n        #   quote: save char in element buffer / _IN_QUOTED\n        #   delimiter: save element buffer, empty element buffer / _DELIMITED\n        #    (The element buffer should be empty, representing a null data item.)\n        #   other: save char in element buffer / _IN_UNQUOTED\n        # _DELIMITED: A delimiter has been seen while not in a quoted item.\n        #  Actions / transition:\n        #   quote: save char in element buffer / _IN_QUOTED\n        #   delimiter: if eat_multiple: no action / _DELIMITED\n        #     if not eat_multiple: save element buffer, empty element buffer / _DELIMITED\n        #   other: save char in element buffer / _IN_UNQUOTED\n        # At end of line: save escape buffer in element buffer, save element buffer.  For a well-formed\n        # line, these should be empty, but they may not be.\n        #\n        # Define the state constants, which will also be used as indexes into an execution vector.\n        (\n            _IN_QUOTED,\n            _ESCAPED,\n            _QUOTE_IN_QUOTED,\n            _IN_UNQUOTED,\n            _BETWEEN,\n            _DELIMITED,\n        ) = range(6)\n        #\n        # Because of Python 2.7's scoping rules:\n        # * The escape buffer and current element are defined as mutable objects that will have their\n        #  first elements modified, rather than as string variables.  (Python 2.x does not allow\n        #  modification of a variable in an enclosing scope that is not the global scope, but\n        #  mutable objects like lists can be altered.  Another approach would be to implement this\n        #  as a class and use instance variables.)\n        # * The action functions return the next state rather than assigning it directly to the 'state' variable.\n        esc_buf = [\"\"]\n        current_element = [\"\"]\n\n        def in_quoted():\n            if c == self.escchar:\n                esc_buf[0] = c\n                return _ESCAPED\n            elif c == qchar:\n                esc_buf[0] = c\n                return _QUOTE_IN_QUOTED\n            else:\n                current_element[0] += c\n                return _IN_QUOTED\n\n        def escaped():\n            if c == delim:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _BETWEEN\n            else:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                current_element[0] += c\n                return _IN_QUOTED\n\n        def quote_in_quoted():\n            if c == qchar:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                current_element[0] += c\n                return _IN_QUOTED\n            elif c == delim:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                current_element[0] += c\n                self.record_format_error(\n                    i + 1, \"Unexpected character following a closing quote\"\n                )\n                return _IN_QUOTED\n\n        def in_unquoted():\n            if c == delim:\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += c\n                return _IN_UNQUOTED\n\n        def between():\n            if c == qchar:\n                current_element[0] += c\n                return _IN_QUOTED\n            elif c == delim:\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += c\n                return _IN_UNQUOTED\n\n        def delimited():\n            if c == qchar:\n                current_element[0] += c\n                return _IN_QUOTED\n            elif c == delim:\n                if not eat_multiple_delims:\n                    elements.append(current_element[0])\n                    current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += c\n                return _IN_UNQUOTED\n\n        # Functions in the execution vector must be ordered identically to the\n        # indexes represented by the state constants.\n        exec_vector = [\n            in_quoted,\n            escaped,\n            quote_in_quoted,\n            in_unquoted,\n            between,\n            delimited,\n        ]\n        # Set the starting state.\n        state = _BETWEEN\n        # Process the line of text.\n        for i, c in enumerate(self.text):\n            state = exec_vector[state]()\n        # Process the end-of-line condition.\n        if len(esc_buf[0]) &gt; 0:\n            current_element[0] += esc_buf[0]\n        if len(current_element[0]) &gt; 0:\n            elements.append(current_element[0])\n        return elements\n\n    def well_quoted_line(self, delim, qchar):\n        # Returns a tuple of boolean, int, and boolean, indicating: 1) whether the line is\n        # well-quoted, 2) the number of elements for which the quote character is used,\n        # and 3) whether the escape character is used.\n        wq = [self._well_quoted(el, qchar) for el in self.items(delim, qchar)]\n        return (\n            all([b[0] for b in wq]),\n            sum([b[1] for b in wq]),\n            any([b[2] for b in wq]),\n        )\n\ndef diagnose_delim(linestream, possible_delimiters=None, possible_quotechars=None):\n    # Returns a tuple consisting of the delimiter, quote character, and escape\n    # character for quote characters within elements of a line.  All may be None.\n    # If the escape character is not None, it will be u\"\\\".\n    # Arguments:\n    # * linestream: An iterable file-like object with a 'next()' method that returns lines of text\n    #  as bytes or unicode.\n    # * possible_delimiters: A list of single characters that might be used to separate items on\n    #  a line.  If not specified, the default consists of tab, comma, semicolon, and vertical rule.\n    #  If a space character is included, multiple space characters will be treated as a single\n    #  delimiter--so it's best if there are no missing values on space-delimited lines, though\n    #  that is not necessarily a fatal flaw unless there is a very high fraction of missing values.\n    # * possible_quotechars: A list of single characters that might be used to quote items on\n    #  a line.  If not specified, the default consists of single and double quotes.\n    if not possible_delimiters:\n        possible_delimiters = [\"\\t\", \",\", \";\", \"|\"]\n    if not possible_quotechars:\n        possible_quotechars = ['\"', \"'\"]\n    lines = []\n    for i in range(100):\n        try:\n            ln = linestream.next()\n        except StopIteration:\n            break\n        except:\n            raise\n        while len(ln) &gt; 0 and ln[-1] in (\"\\n\", \"\\r\"):\n            ln = ln[:-1]\n        if len(ln) &gt; 0:\n            lines.append(CsvLine(ln))\n    if len(lines) == 0:\n        raise CsvDiagError(\"CSV diagnosis error: no lines read\")\n    for ln in lines:\n        for d in possible_delimiters:\n            ln.count_delim(d)\n    # For each delimiter, find the minimum number of delimiters found on any line, and the number of lines\n    # with that minimum number\n    delim_stats = {}\n    for d in possible_delimiters:\n        dcounts = [ln.delim_count(d) for ln in lines]\n        min_count = min(dcounts)\n        delim_stats[d] = (min_count, dcounts.count(min_count))\n    # Remove delimiters that were never found.\n    for k in delim_stats.keys():\n        if delim_stats[k][0] == 0:\n            del delim_stats[k]\n\n    def all_well_quoted(delim, qchar):\n        # Returns a tuple of boolean, int, and boolean indicating: 1) whether the line is\n        # well-quoted, 2) the total number of lines and elements for which the quote character\n        # is used, and 3) the escape character used.\n        wq = [l.well_quoted_line(delim, qchar) for l in lines]\n        return (\n            all([b[0] for b in wq]),\n            sum([b[1] for b in wq]),\n            CsvLine.escchar if any([b[2] for b in wq]) else None,\n        )\n\n    def eval_quotes(delim):\n        # Returns a tuple of the form to be returned by 'diagnose_delim()'.\n        ok_quotes = {}\n        for q in possible_quotechars:\n            allwq = all_well_quoted(delim, q)\n            if allwq[0]:\n                ok_quotes[q] = (allwq[1], allwq[2])\n        if len(ok_quotes) == 0:\n            return (delim, None, None)  # No quotes, no escapechar\n        else:\n            max_use = max([v[0] for v in ok_quotes.values()])\n            if max_use == 0:\n                return (delim, None, None)\n            # If multiple quote characters have the same usage, return (arbitrarily) the first one.\n            for q in ok_quotes.keys():\n                if ok_quotes[q][0] == max_use:\n                    return (delim, q, ok_quotes[q][1])\n\n    if len(delim_stats) == 0:\n        # None of the delimiters were found.  Some other delimiter may apply,\n        # or the input may contain a single value on each line.\n        # Identify possible quote characters.\n        return eval_quotes(None)\n    else:\n        if len(delim_stats) &gt; 1:\n            # If one of them is a space, prefer the non-space\n            if \" \" in delim_stats.keys():\n                del delim_stats[\" \"]\n        if len(delim_stats) == 1:\n            return eval_quotes(delim_stats.keys()[0])\n        # Assign weights to the delimiters.  The weight is the square of the minimum number of delimiters\n        # on a line times the number of lines with that delimiter.\n        delim_wts = {}\n        for d in delim_stats.keys():\n            delim_wts[d] = delim_stats[d][0] ** 2 * delim_stats[d][1]\n        # Evaluate quote usage for each delimiter, from most heavily weighted to least.\n        # Return the first good pair where the quote character is used.\n        delim_order = sorted(delim_wts, key=delim_wts.get, reverse=True)\n        for d in delim_order:\n            quote_check = eval_quotes(d)\n            if quote_check[0] and quote_check[1]:\n                return quote_check\n        # There are no delimiters for which quotes are OK.\n        return (delim_order[0], None, None)\n    # Should never get here\n    raise CsvDiagError(\"CSV diagnosis error: an untested set of conditions are present\")"
  },
  {
    "objectID": "content/development/code/python.html#import-csv-to-a-database",
    "href": "content/development/code/python.html#import-csv-to-a-database",
    "title": "Python",
    "section": "Import CSV to a Database",
    "text": "Import CSV to a Database\nclass CsvFile(object):\n    \"\"\"CsvFile class automatically opens a file and creates a CSV reader, reads the first row containing column headers, and stores those headers so that they can be used to construct the INSERT statement.\"\"\"\n    def __init__(self, filename):\n        self.fn = filename\n        self.f = None\n        self.open()\n        self.rdr = csv.reader(self.f)\n        self.headers = next(self.rdr)\n    def open(self):\n        if self.f is None:\n            mode = \"rb\" if sys.version_info &lt; (3,) else \"r\"\n            self.f = open(self.fn, mode)\n    def reader(self):\n        return self.rdr\n    def close(self):\n        self.rdr = None\n        self.f.close()\n        self.f = None\n\n\nclass Database(object):\n    \"\"\"The Database class and subclasses provide a database connection for each type of DBMS, and a method to construct an INSERT statement for a given CsvFile object, using that DBMS's parameter substitution string.  The conn_info argument is a dictionary containing the host name, user name, and password.\"\"\"\n    def __init__(self, conn_info):\n        self.paramstr = '%s'\n        self.conn = None\n    def insert_sql(self, tablename, csvfile):\n        return \"insert into %s (%s) values (%s);\" % (\n                tablename,\n                \",\".join(csvfile.headers),\n                \",\".join([self.paramstr] * len(csvfile.headers))\n                )\n\n\nclass PgDb(Database):\n    def __init__(self, conn_info):\n        self.db_type = \"p\"\n        import psycopg2\n        self.paramstr = \"%s\"\n        connstr = \"host=%(server)s dbname=%(db)s user=%(user)s password=%(pw)s\" % conn_info\n        self.conn = psycopg2.connect(connstr)\n\n    def postgres_copy(csvfile, db):\n        \"\"\"Postgres COPY command. Fastest implementation\"\"\"\n        curs = db.conn.cursor()\n        rf = open(csvfile.fn, \"rt\")\n        # Read and discard headers\n        hdrs = rf.readline()\n        copy_cmd = \"copy copy_test from stdin with (format csv)\"\n        curs.copy_expert(copy_cmd, rf)\n\n    def simple_copy(csvfile, db):\n        \"\"\"Row-by-row reading and writing\"\"\"\n        ins_sql = db.insert_sql(\"copy_test\", csvfile)\n        curs = db.conn.cursor()\n        rdr = csvfile.reader()\n        for line in rdr:\n            curs.execute(ins_sql, clean_line(line))\n        db.conn.commit()\n\n    def buffer1_copy(csvfile, db, buflines):\n        \"\"\"Buffered reading and writing\"\"\"\n        ins_sql = db.insert_sql(\"copy_test\", csvfile)\n        curs = db.conn.cursor()\n        rdr = csvfile.reader()\n        eof = False\n        while True:\n            b = []\n            for j in range(buflines):\n                try:\n                    line = next(rdr)\n                except StopIteration:\n                    eof = True\n                else:\n                    b.append(clean_line(line))\n            if len(b) &gt; 0:\n                curs.executemany(ins_sql, b)\n            if eof:\n                break\n        db.conn.commit()"
  },
  {
    "objectID": "content/development/code/sql.html",
    "href": "content/development/code/sql.html",
    "title": "SQL",
    "section": "",
    "text": "The code snippets on this page are primarily geared towards the PostgreSQL dialect."
  },
  {
    "objectID": "content/development/code/sql.html#fundamentals",
    "href": "content/development/code/sql.html#fundamentals",
    "title": "SQL",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nPrimary Key Relationships\nColumns\n\ntable_schema: PK schema name\ntable_name: PK table name\nconstraint_name: PK constraint name\nposition: index of column in table (1, 2, …). 2 or higher means key is composite (contains more than one column)\nkey_column: PK column name\n\nRows\n\nOne row represents one primary key column\nScope of rows: columns of all PK constraints in a database\nOrdered by table schema, table name, column position\n\nselect * from (\n -- Main query. Returns all tables\n select kcu.table_schema,\n     kcu.table_name,\n     tco.constraint_name,\n     kcu.ordinal_position as position,\n     kcu.column_name as key_column\n from information_schema.table_constraints tco\n join information_schema.key_column_usage kcu \n   on kcu.constraint_name = tco.constraint_name\n   and kcu.constraint_schema = tco.constraint_schema\n   and kcu.constraint_name = tco.constraint_name\n where tco.constraint_type = 'PRIMARY KEY'\n order by kcu.table_schema,\n    kcu.table_name,\n    position\n) main\nwhere table_name = 'd_location'\n\n\nForeign Key Relationships\nColumns\n\nforeign_table: foreign table schema and name\nrel: relationship symbol implicating direction\nprimary_table: primary (rerefenced) table schema and name\nfk_columns: list of FK colum names, separated with “,”\nconstraint_name: foreign key constraint name\n\nRows\n\nOne row represents one foreign key.\nIf foreign key consists of multiple columns (composite key) it is still represented as one row.\nScope of rows: all foregin keys in a database.\nOrdered by foreign table schema name and table name.\n\n\n\nOrdering Tables by FK\ndrop table if exists dependencies cascade;\ncreate temporary table dependencies as\nselect \n  tc.table_name as child,\n  tu.table_name as parent\nfrom \n  information_schema.table_constraints as tc\n  inner join information_schema.constraint_table_usage as tu\n    on tu.constraint_name = tc.constraint_name\nwhere \n  tc.constraint_type = 'FOREIGN KEY'\n  and tc.table_name &lt;&gt; tu.table_name;\n\nwith recursive dep_depth as (\nselect\n  dep.child, dep.parent, 1 as lvl\nfrom\n  dependencies as dep\nunion all\nselect\n  dep.child, dep.parent, dd.lvl + 1 as lvl\nfrom\n  dep_depth as dd\n  inner join dependencies as dep on dep.parent = dd.child\n)\nselect\n  table_name, table_order\nfrom (\n  select\n    dd.parent as table_name, max(lvl) as table_order\n  from\n    dep_depth as dd\n  group by\n    table_name\n  union\n  select\n    dd.child as table_name, max(lvl) + 1 as level\n  from\n    dep_depth as dd\n    left join dependencies as dp on dp.parent = dd.child\n  where\n    dp.parent is null\n  group by dd.child\n) as all_levels\norder by table_order;\n\n\nReturn FK relationships for all tables\nselect * from (\n -- Main query. Returns FK relationships for all tables\n select \n   kcu.table_schema as table_schema,\n   kcu.table_name as foreign_table,\n     '&gt;-' as relationship,\n     rel_tco.table_name as primary_table,\n     string_agg(kcu.column_name, ', ') as fk_columns,\n     kcu.constraint_name\n from information_schema.table_constraints tco\n join information_schema.key_column_usage kcu\n     on tco.constraint_schema = kcu.constraint_schema\n     and tco.constraint_name = kcu.constraint_name\n join information_schema.referential_constraints rco\n     on tco.constraint_schema = rco.constraint_schema\n     and tco.constraint_name = rco.constraint_name\n join information_schema.table_constraints rel_tco\n     on rco.unique_constraint_schema = rel_tco.constraint_schema\n     and rco.unique_constraint_name = rel_tco.constraint_name\n where tco.constraint_type = 'FOREIGN KEY'\n group by kcu.table_schema,\n    kcu.table_name,\n    rel_tco.table_name,\n    rel_tco.table_schema,\n    kcu.constraint_name\n order by kcu.table_schema,\n    kcu.table_name\n) main\nwhere primary_table = 'd_location'\n\n\nMost Table Relationships\nList tables with most relationships.\nselect * from\n(select relations.table_name as table_name, -- schema name and table name\n       count(relations.table_name) as relationships, -- number of table relationships\n       count(relations.referenced_tables) as foreign_keys, -- number of foreign keys in a table\n       count(relations.referencing_tables) as references, -- number of foreign keys that are refering to this table\n       count(distinct related_table) as related_tables, -- number of related tables\n       count(distinct relations.referenced_tables) as referenced_tables, -- number of different tables referenced with FKs (multiple FKs can refer to one table, so number of FKs might be different than number of referenced tables)\n       count(distinct relations.referencing_tables) as referencing_tables -- number of different tables that are refering to this table (similar to referenced_tables)\nfrom(\n     select pk_tco.table_schema || '.' || pk_tco.table_name as table_name,\n            fk_tco.table_schema || '.' || fk_tco.table_name as related_table,\n            fk_tco.table_name as referencing_tables,\n            null::varchar(100) as referenced_tables\n     from information_schema.referential_constraints rco\n     join information_schema.table_constraints fk_tco\n          on rco.constraint_name = fk_tco.constraint_name\n          and rco.constraint_schema = fk_tco.table_schema\n     join information_schema.table_constraints pk_tco\n          on rco.unique_constraint_name = pk_tco.constraint_name\n          and rco.unique_constraint_schema = pk_tco.table_schema\n    union all\n    select fk_tco.table_schema || '.' || fk_tco.table_name as table_name,\n           pk_tco.table_schema || '.' || pk_tco.table_name as related_table,\n           null as referencing_tables,\n           pk_tco.table_name as referenced_tables\n    from information_schema.referential_constraints rco\n    join information_schema.table_constraints fk_tco \n         on rco.constraint_name = fk_tco.constraint_name\n         and rco.constraint_schema = fk_tco.table_schema\n    join information_schema.table_constraints pk_tco\n         on rco.unique_constraint_name = pk_tco.constraint_name\n         and rco.unique_constraint_schema = pk_tco.table_schema\n) relations\ngroup by table_name\norder by relationships asc) results\n\nwhere substring(table_name, 5, 2) = 'd_'; -- substring(string, start_position, length)\n\n\nList column definitions and order between a set of tables\nwith recursive\nmain_tables (\n    table_schema,\n    table_name\n) as (\nvalues\n    ('idb', 'd_document'),\n    ('idb', 'd_docfile'),\n    ('idb', 'd_study'),\n    ('idb', 'd_location'),\n    ('idb', 'd_studylocation'),\n    ('idb', 'd_sampcoll'),\n    ('idb', 'd_sampmain'),\n    ('idb', 'd_sampsplit'),\n    ('idb', 'd_labsample'),\n    ('idb', 'd_labpkg'),\n    ('idb', 'd_labresult')\n),\nexclude_columns (column_name) as (\n    values\n    ('gid'),\n    ('studyloc_alias'),\n    ('sampcoll_alias'),\n    ('sample_alias'),\n    ('analresult_alias'),\n    ('rev_user'),\n    ('rev_time')\n),\ndependencies as (\n    select\n        tc.table_name as child,\n        tu.table_name as parent\n    from\n        information_schema.table_constraints as tc\n        inner join information_schema.constraint_table_usage as tu\n            on tu.constraint_name = tc.constraint_name\n    where\n        tc.constraint_type = 'FOREIGN KEY'\n        and tc.table_name &lt;&gt; tu.table_name\n),\ndep_depth as (\n    select\n        dep.child, dep.parent, 1 as lvl\n    from\n        dependencies as dep\n    union all\n    select\n        dep.child, dep.parent, dd.lvl + 1 as lvl\n    from\n        dep_depth as dd\n        inner join dependencies as dep on dep.parent = dd.child\n    ),\ntable_order as (\n    select\n        all_levels.table_name, all_levels.table_order\n    from (\n        select\n            dd.parent as table_name, max(lvl) as table_order\n        from\n            dep_depth as dd\n        group by\n            table_name\n        union\n        select\n            dd.child as table_name, max(lvl) + 1 as level\n        from\n            dep_depth as dd\n            left join dependencies as dp on dp.parent = dd.child\n        where\n            dp.parent is null\n        group by dd.child\n    ) as all_levels\n    order by table_order\n),\ncols as (\n    select\n        cc.table_name,\n        -- Some column names need changing because they exist in multiple tables\n        case\n            when cc.column_name in ('comments', 'description', 'fraction')\n            then replace(cc.table_name, 'd_', '') || '_' || cc.column_name\n            else cc.column_name end as column_name,\n        data_type,\n        character_maximum_length,\n        is_nullable,\n        table_order,\n        ordinal_position,\n        row_number() over () as col_order\n    from\n        information_schema.columns as cc\n        inner join table_order on table_order.table_name=cc.table_name\n        inner join main_tables on \n            main_tables.table_schema=cc.table_schema \n            and main_tables.table_name=cc.table_name\n        left join exclude_columns on cc.column_name=exclude_columns.column_name\n    where exclude_columns.column_name is null\n    order by table_order.table_order, table_order.table_name, cc.ordinal_position\n)\nselect distinct\n    cols.column_name,\n    cols.column_name\n        || ' '\n        || data_type\n        || case\n                when character_maximum_length is null\n                then ''\n                else '(' || character_maximum_length || ')'\n                end\n     -- || case when not is_nullable::boolean then ' NOT NULL' else '' end as col_def,\n        as col_def,\n    cols.table_order, cols.table_name, cols.ordinal_position, row_number() over () as col_order\nfrom cols\norder by cols.table_order, cols.table_name, cols.ordinal_position\n\n\nCommon Functions\n\nLENGTH(string): Returns the length of the provided string\nPOSITION(string IN substring): Returns the position of the substring within the specified string.\nCAST(expression AS datatype): Converts an expression into the specified data type.\n`NOW: Returns the current date, including time.\nCEIL(input_val): Returns the smallest integer greater than the provided number.\nFLOOR(input_val): Returns the largest integer less than the provided number.\nROUND(input_val, [round_to]): Rounds a number to a specified number of decimal places.\nTRUNC(input_value, num_decimals): Truncates a number to a number of decimals.\nREPLACE(whole_string, string_to_replace, replacement_string): Replaces one string inside the whole string with another string.\nSUBSTRING(string, [start_pos], [length]): Returns part of a value, based on a position and length.\n\n\n\nAdd Role\n1create user &lt;username&gt; with password 'password';\ngrant connect on database &lt;db_name&gt; to &lt;username&gt;;\ngrant usage on schema &lt;schema_name&gt; to &lt;username&gt;;\ngrant select on all tables in schema &lt;schema_name&gt; to &lt;username&gt;;\nalter default privileges in schema &lt;schema_name&gt; grant select on tables to &lt;username&gt;;\n\n2grant create on database &lt;db_name&gt; to &lt;username&gt;;\n3grant insert on database &lt;db_name&gt; to &lt;username&gt;;\n4grant update on database &lt;db_name&gt; to &lt;username&gt;;\n5grant update on database &lt;db_name&gt; to &lt;username&gt;;\n\n1\n\nCreate a read-only user.\n\n2\n\nAllow user to create database objects.\n\n3\n\nAllow user to insert rows to any schema and table.\n\n4\n\nAllow user to update rows in any schema and table.\n\n5\n\nAllow user to delete rows in any schema and table.\n\n\n\n\nCreate read only user - shorthand\nOnce already creating a specific user role, you can user the `pg_read_all_data` to grant read only access to all tables.\nGRANT pg_read_all_data TO username;\n\n\nFinding Temporary Objects\nSELECT\n n.nspname as SchemaName,\n c.relname as RelationName,\n CASE c.relkind\n  WHEN 'r' THEN 'table'\n  WHEN 'v' THEN 'view'\n  WHEN 'i' THEN 'index'\n  WHEN 'S' THEN 'sequence'\n  WHEN 's' THEN 'special'\n  END as RelationType,\n pg_catalog.pg_get_userbyid(c.relowner) as RelationOwner,             \n pg_size_pretty(pg_relation_size(n.nspname ||'.'|| c.relname)) as RelationSize\nFROM pg_catalog.pg_class c\nLEFT JOIN pg_catalog.pg_namespace AS n ON n.oid = c.relnamespace\n WHERE  c.relkind IN ('r','s') \n AND  (n.nspname !~ '^pg_toast' and nspname like 'pg_temp%')\nORDER BY pg_relation_size(n.nspname ||'.'|| c.relname) DESC;"
  },
  {
    "objectID": "content/development/code/sql.html#sql",
    "href": "content/development/code/sql.html#sql",
    "title": "SQL",
    "section": "SQL",
    "text": "SQL\n\nTerminate backends of a particular user (“test”)\nThis query will kill every backend user “test” is connected to.\nWITH pids AS (\n  SELECT pid\n  FROM pg_stat_activity\n  WHERE usename='test'\n)\n\nSELECT pg_terminate_backend(pid)\nFROM pids;\n\n\nCancel every running SQL commands from a particular user (“test”)\nThis query will cancel every running query issued by the particular user “test”.\nWITH pids AS ( \n  SELECT pid \n  FROM pg_stat_activity \n  WHERE username='test' \n) \nSELECT pg_cancel_backend(pid) \nFROM pids;\n\n\nList tables in database\nSELECT table_schema, table_name \nFROM information_schema.tables \nORDER BY table_schema,table_name;\n\n\nList columns in table\nSELECT column_name\nFROM   information_schema.columns\nWHERE  table_schema = 'schema'\nAND    table_name = 'table';\n\n\nCreate a read-only user\ngrant connect on database db_name to user;\ngrant usage on schema schema_name to user;\ngrant select on all tables in schema schema_name to user;\nalter default privileges in schema schema_name grant select on tables to user;\n\n\nCreate DB\ncreate database &lt;new_db_name&gt; owner &lt;user_or_group&gt; template &lt;name_of_db_to_use_as_template&gt;;\n-- show search_path;\nset search_path to &lt;default_schema&gt;,public;\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\n-- Database Creation\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncreate database &lt;new_db_name&gt; owner &lt;user_or_group&gt; template &lt;name_of_db_to_use_as_template&gt;;\n-- show search_path;\nset search_path to idb, public;\n\ngrant connect, temporary on database &lt;new_db_name&gt; to public;\ngrant all on database &lt;new_db_name&gt; to &lt;user&gt;;\ngrant all on database &lt;new_db_name&gt; to &lt;group&gt;;\n\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\ncreate schema staging;\n\n-- Add a unique constraint to e_analyte.full_name and e_analyte.cas_rn so that\n-- no full name or cas_rn can be used for more than one analyte\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nalter table e_analyte\n  add constraint uc_fullname unique(full_name),\n  add constraint uc_casrn unique(cas_rn);\n\n\nDBLink\nselect\n  a.*, b.*\nfrom\n  table1 as a\n  left join (\n    select * from dblink(\n      'dbname=&lt;database&gt;',\n      'select col1, col2, col3 from &lt;table&gt;'\n    ) as d (\n      col1 text, col2 text, col3 text\n    )\n  ) as b\n  on a.col1 = b.col2\n\n\nPartitioning\nselect * from (\n    select *, row_number() over(\n        partition by\n            col1, col2, col3\n        order by col1 desc\n    ) rowid\n    from sometable\n) someid\nwhere rowid &gt; 1;\n\n\nCurrent Database\nselect * from pg_catalog.current_database()\n\n\nCurrent user/role\nselect * from current_role\nselect * from current_user\n\n\nProcess ID\nselect * from pg_catalog.pg_backend_pid()\n\n\nList functions/defs/args\nselect \n pg_get_userbyid(p.proowner) as owner,\n n.nspname as function_schema,\n p.proname as function_name,\n l.lanname as function_language,\n case when l.lanname = 'internal' then p.prosrc\n  else pg_get_functiondef(p.oid)\n  end as definition,\n pg_get_function_arguments(p.oid) as function_arguments,\n t.typname as return_type\nfrom pg_proc p\n left join pg_namespace n on p.pronamespace = n.oid\n left join pg_language l on p.prolang = l.oid\n left join pg_type t on t.oid = p.prorettype \nwhere n.nspname not in ('pg_catalog', 'information_schema')\nand n.nspname = 'idb'\norder by function_schema, function_name;\n\n\nWhos logged in\nselect * from pg_stat_activity\nwhere usename != '' and usename != 'postgres'\norder by usename, pid\n\n\nAggregate Functions\npg_aggregate catalog\n-- pg_proc contains data for aggregate functions as well as plain functions\nselect * from pg_proc\n-- pg_aggregate is an extension of pg_proc.\nselect * from pg_aggregate\n\n\nList users\nSELECT rolname FROM pg_roles;\n\n\nUpdate From\nUPDATE tablename\nSET columnname = someothervalue\nFROM ...\nWHERE ...\n\n\nMaterialized View\nReference\nCREATE MATERIALIZED VIEW view_name\nAS\nquery\nWITH [NO] DATA;\nWhen you refresh data for a materialized view, PostgreSQL locks the entire table therefore you cannot query data against it. To avoid this, you can use the CONCURRENTLY option.\nWith CONCURRENTLY option, PostgreSQL creates a temporary updated version of the materialized view, compares two versions, and performs INSERT and UPDATE only the differences.\nREFRESH MATERIALIZED VIEW CONCURRENTLY view_name;\n\n\nConstants\nWITH myconstants (analyte_search) as (\n   values ('%Hexachlorocyclopentadiene%')\n)\n\nSELECT *\nFROM e_analyte, myconstants\nWHERE analyte ilike analyte_search\n   OR full_name ilike analyte_search\n   OR aliases ilike analyte_search;\n\n\nSequential Keys\nseq_key bigint NOT NULL DEFAULT nextval('seq_key'::regclass)\n\nALTER SEQUENCE seq_key RESTART WITH 3;\n\n\nCross-Database Search\nThese could be refined further by creating a function.\nwith\nconst (param) as (\n    values ('%solid%')\n),\ndbrows as (\n   select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte\n  union\n    select * from dblink(\n        'dbname=database1',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=database2',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n     'dbname=database3',\n     'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n)\nselect \n analyte, full_name, chem_class, cas_rn, aliases, \n count(*) as num_instances, string_agg(db, '; ') as db\nfrom dbrows, const\nwhere analyte ilike param\n   or full_name ilike param\n   or aliases ilike param\ngroup by analyte, full_name, chem_class, cas_rn, aliases\norder by chem_class, analyte;\n\n\nLogging\n\nLog slow queries by setting log_min_duration_statement\nALTER database postgres SET log_min_duration_statement = '250ms';\n\n\nControl which statement types get logged\nControl the types of statements that are logged for your database.\nALTER DATABASE postgres SET log_statement = 'all';\nValid values include all, ddl, none, mod\n\n\nLog when waiting on a lock\nLog when database is waiting on a lock.\nALTER DATABASE postgres SET log_lock_waits = 'on';\n\n\n\nPerformance\n\nUse statement timeouts to control runaway queries\nSetting a statement timeout prevents queries from running longer than the specified time. You can set a statement timeout on the database, user, or session level. We recommend you set a global timeout on Postgres and then override that one specific users or sessions that need a longer allowed time to run.\nALTER DATABASE mydatabase SET statement_timeout = '60s';\n\n\nUse pg_stat_statements to find the queries and processes that use the most resources\nSELECT\n total_exec_time,\n mean_exec_time as avg_ms,\n calls,\n query\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n\n\nMonitor connections in Postgres\nThis query will provide the number of connection based on type.\nSELECT count(*),\n    state\nFROM pg_stat_activity\nGROUP BY state;\nIf you see idle connections is above 20, it is recommended to explore using PgBouncer.\n\n\nQuery size of specific table\nWill give you the size of the specific relation you pass in.\nSELECT pg_relation_size('table_name');\n\n-- For prettier formatting you can wrap with:\n\nSELECT pg_size_pretty(pg_relation_size('table_name'));\n\n\nQuery all relation sizes\nWill report on all table sizes in descending order\nSELECT relname AS relation,\n       pg_size_pretty (\n         pg_total_relation_size (C .oid)\n       ) AS total_size\nFROM pg_class C\nLEFT JOIN pg_namespace N ON (N.oid = C .relnamespace)\nWHERE nspname NOT IN (\n        'pg_catalog',\n        'information_schema'\n      )\n  AND C .relkind &lt;&gt; 'i'\n  AND nspname !~ '^pg_toast'\n  ORDER BY pg_total_relation_size (C .oid) DESC\n\n\nCheck for unused indexes\nWill return the unused indexes in descending order of size. Keep in mind you want to also check replicas before dropping indexes.\nSELECT schemaname || '.' || relname AS table,\n       indexrelname AS index,\n       pg_size_pretty(pg_relation_size(i.indexrelid)) AS \"index size\",\n       idx_scan as \"index scans\"\nFROM pg_stat_user_indexes ui\nJOIN pg_index i ON ui.indexrelid = i.indexrelid\nWHERE NOT indisunique\n  AND idx_scan &lt; 50\n  AND pg_relation_size(relid) &gt; 5 * 8192\nORDER BY \n  pg_relation_size(i.indexrelid) / nullif(idx_scan, 0) DESC NULLS FIRST,\n  pg_relation_size(i.indexrelid) DESC;\n\n\nGet approximate counts for a table\nWill return the approximate count for a table based on PostgreSQL internal statistics. Useful for large tables where performing a `SELECT count(*)` is costly on performance.\nSELECT reltuples::numeric as count\nFROM pg_class\nWHERE relname='table_name';\n\n\nNon-blocking index creation\nAdding `CONCURRENTLY` during index creation, while not permitted in a transaction, will not hold a lock on the table while creating your index.\nCREATE INDEX CONCURRENTLY foobar ON foo (bar);\n\n\n\nReplace nulls with other value\nCoalesce will use the value and if the value is null display your specified string.\nSELECT id, \n       coalesce(ip, 'no IP') \nFROM logs;\nYou can supply two columns as well prior to your replacement value and the function will use first not null value.\n\n\nImport Schema with Mapping a Foreign Data Wrapper (FDW)\nImport foreign schema creates foreign tables representing those from the foreign server.\nIMPORT FOREIGN SCHEMA \"public\";\nYou can IMPORT FOREIGN SCHEMA when mapping a foreign data wrapper to save you from building a new one\n\n\nGenerate data with generate_series\nGenerates values from the start to the end values supplied based on the interval. Values can be numbers or timestamps. Can be used in a FROM or JOIN clause or CTE. Commonly used when building charts and reports that require all dates to be filled.\nSELECT * FROM\ngenerate_series(now() - '3 month'::interval, now(), '1 day');\n\n\nRound dates with date_trunc\nWill truncate the date to the specified level of precision. Some example precision levels include: month, week, day, hour, minute.\nSELECT date_trunc('day', now());\n\n\nPerform time math with intervals\nYou can add or subtract specific amounts of time of a timestamp by casting the value you want as an interval.\nSELECT now() - '1 month'::interval;\n\n\nMake your session rest a bit\nThis function will make your session sleep for 2.5 seconds. Useful in any testing tool executing a script in a given loop where you want to pause a bit between iterations, as an example.\nselect pg_sleep(2.5);"
  },
  {
    "objectID": "content/development/code/sql.html#plpgsql",
    "href": "content/development/code/sql.html#plpgsql",
    "title": "SQL",
    "section": "PL/pgSQL",
    "text": "PL/pgSQL\n\nWipe Schema Tables\n-- DROP FUNCTION IF EXISTS idb.wipe_staging();\nCREATE OR REPLACE FUNCTION idb.wipe_staging()\nRETURNS TABLE(staging_schema text, deleted_tables integer) \nLANGUAGE 'plpgsql'\nCOST 100\nVOLATILE PARALLEL UNSAFE\nROWS 1000\nAS $BODY$\n#variable_conflict use_column\nDECLARE\n  staging_schema TEXT;\n  table_name TEXT;\n  deleted_tables INTEGER := 0;\nBEGIN\nstaging_schema = (select 'stg_' || user)::text;\nFOR table_name IN (\n    SELECT table_name \n    FROM information_schema.tables\n    WHERE table_schema = staging_schema\n)\nLOOP\nEXECUTE format('DROP TABLE %I.%I CASCADE', staging_schema, table_name );\ndeleted_tables := deleted_tables + 1;\nEND LOOP;\nRETURN query select staging_schema, deleted_tables;\nEND;\n$BODY$;\nALTER FUNCTION idb.wipe_staging()\n    OWNER TO envdb_dm;"
  },
  {
    "objectID": "content/development/code/sql.html#psql",
    "href": "content/development/code/sql.html#psql",
    "title": "SQL",
    "section": "psql",
    "text": "psql\nCheat sheet\n\n\nCode\nimport pandas as pd\n\nf = \"../../../static/development/commands.xlsx\"\n\nt4 = pd.read_excel(f, sheet_name=\"psql-commands\")\nt4.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\nDetail\n\n\n\n\n\n\n\n\n\n\n\nsudo -s postgres psql\n\n\n\nConnect to PostgreSQL as admin\n\n\n\n\n\n\n\npostgres=# \\l\n\n\n\nList all databases\n\n\n\n\n\n\n\npostgres=# \\c postgres\n\n\n\nConnect to the database named postgres\n\n\n\n\n\n\n\npostgres=# \\q\n\n\n\nDisconnect\n\n\n\n\n\n\n\npostgres=# -d mydb\n\n\n\nConnecting to database\n\n\n\n\n\n\n\npostgres=# -U john mydb\n\n\n\nConnecting as a specific user\n\n\n\n\n\n\n\npostgres=# -h localhost -p 5432 mydb\n\n\n\nConnecting to a host/port\n\n\n\n\n\n\n\npostgres=# -U admin -h 192.168.1.5 -p 2506 -d mydb\n\n\n\nConnect remote PostgreSQL\n\n\n\n\n\n\n\npostgres=# -W mydb\n\n\n\nForce password\n\n\n\n\n\n\n\npostgres=# -c '\\c postgres' -c '\\dt'\n\n\n\nExecute a SQL query or command\n\n\n\n\n\n\n\npostgres=# -c “\\l+” -H postgres &gt; database.html\n\n\n\nGenerate HTML report\n\n\n\n\n\n\n\npostgres=# -l\n\n\n\nList all databases\n\n\n\n\n\n\n\npostgres=# \\dt\n\n\n\nShow all tables in a database\n\n\n\n\n\n\n\npostgres=# mydb -f file.sql\n\n\n\nExecute commands from a file\n\n\n\n\n\n\n\npostgres=# -V\n\n\n\nPrint the psql version\n\n\n\n\n\n\n\npostgres=# \\df &lt;schema&gt;\n\n\n\nList functions in schema\n\n\n\n\n\n\n\npostgres=# \\du\n\n\n\nShow current user permission\n\n\n\n\n\n\n\npostgres=# \\d &lt;table&gt;\n\n\n\nDescribe table\n\n\n\n\n\n\n\npostgres=# \\d+ &lt;table&gt;\n\n\n\nDescribe table with details\n\n\n\n\n\n\n\npostgres=# \\di\n\n\n\nList indexes\n\n\n\n\n\n\n\npostgres=# \\du\n\n\n\nList roles\n\n\n\n\n\n\n\npostgres=# \\ds\n\n\n\nList sequences\n\n\n\n\n\n\n\npostgres=# \\copy …\n\n\n\nImport/export table\n\n\n\n\n\n\n\npostgres=# \\echo [string]\n\n\n\nPrint string\n\n\n\n\n\n\n\npostgres=# \\i [file]\n\n\n\nExecute file\n\n\n\n\n\n\n\npostgres=# \\o [file]\n\n\n\nExport all results to file\n\n\n\n\n\n\n\n\n\n\n\nExport table to CSV\n\n\\copy table TO '&lt;path&gt;' CSV\n\\copy table(col1,col1) TO '&lt;path&gt;' CSV\n\\copy (SELECT...) TO '&lt;path&gt;' CSV\n\n\n\nBackup\nUse pg_dumpall to backup all databases\npg_dumpall -U postgres &gt; all.sql\nUse pg_dump to backup a database\npg_dump -d mydb -f mydb_backup.sql\n\n-a   Dump only the data, not the schema\n-s   Dump only the schema, no data\n-c   Drop database before recreating\n-C   Create database before restoring\n-t   Dump the named table(s) only\n-F   Format (c: custom, d: directory, t: tar)\n\nUse pg_dump -? to get the full list of options\n\n\nRestore\npsql -U user mydb &lt; mydb_backup.sql\n\npg_restore\npg_restore -d mydb mydb_backup.sql -c\n\n-U   Specify a database user\n-c   Drop database before recreating\n-C   Create database before restoring\n-e   Exit if an error has encountered\n-F   Format (c: custom, d: directory, t: tar, p: plain text sql(default))\n\nUse pg_restore -? to get the full list of options\n\n\n\nAutomatically log query time in psql\nWill automatically print the time it took to run a query from within psql. *Of note this is the round trip time not simply query execution time.*\n\\timing\nYou can save this in your .psqlrc to be a default setting\n\n\nAutoformat query results in psql\nWill automatically reorganize the query output based on your terminal window for better readability.\n\\x auto\nYou can save this in your .psqlrc to be a default setting\n\n\nEdit your psql queries in editor of your choice\nWill automatically open your last run query in your default $EDITOR. When you save and close it will execute that query.\n\\e\n\n\nSet a value for nulls\nWill render the nulls as whatever character you specify. Handy for easier parsing of nulls vs. blank text.\n\\pset null 👻\nYou can save this in your .psqlrc to be a default setting\n\n\nSave your query history per database locally\nWill automatically save a history file for each **DBNAME**.\n\\set HISTFILE ~/.psql_history- :DBNAME\nYou can save this in your .psqlrc to be a default setting\n\n\nShow queries issued by internal psql commands\nAdd “-E” (or –echo-hidden) option to psql in the command line. This option will display queries that internal psql commands generate (like “\\dt mytable”). This is a cool way to learn more about system catalogs, or reuse queries issued by psql in your own tool.\npsql -E\n\n\nGet data back, and only the data\nAdd “-qtA” options to psql in the command line. Those options will have psql run in quiet mode (“-q”), return tuples only (“-t”) in an unaligned fashion (“-A”). Combined with “-c” option to send a single query, it can be useful for your scripts if you want the data and only that back from Postgres. Returns one line per row.\npsql -qtA\n\n\nGet results as an HTML table\nAdd “-qtH” options to psql in the command line. Those options will have psql run in quiet mode (“-q”), return tuples only (“-t”) in an HTML table (“-H”). Combined with “-c” option to send a single query, can be a fast way to embed the result of a query in an HTML page.\npsql -qtH\n\n\nClear your psql screen\nWill clear your screen in current psql session\n\\! clear\n\n\nContinually run a query with watch\nWill automatically run the last query every 2 seconds and display the output. You can also specify the query that will run after watch as well.\n\\watch\n\n\nRollback to previous statement on error when in interactive mode\nWhen you encounter an error when in interactive mode this will automatically rollback to just before the previous command, allowing you to continue working as you would expect.\n\\set ON_ERROR_ROLLBACK interactive\n\n\nExport a CSV from directly in psql\nWhen providing the --csv value with a query, this command will run the specific query and return CSV to STDOUT.\npsql &lt;connection-string&gt; --csv -c 'select * from test;'\n\n\nRun a query from a file in psql\nWill execute the specified file when inside psql.\n\\i filename\n\n\nProvide clean border within psql\nWill give you a border around the output of your queries when in psql\n\\pset border 2\nYou can save this in your .psqlrc to be a default setting\n\n\nSet linestyle to unicode\nChanges the linestyle to unicode, which when combined with above tip leads to much cleaner formatting\n\\pset linestyle unicode\nYou can save this in your .psqlrc to be a default setting"
  },
  {
    "objectID": "content/development/server/index.html",
    "href": "content/development/server/index.html",
    "title": "Server",
    "section": "",
    "text": "Initial Linux server setup steps:\n\nLogin to server as root: ssh root@&lt;ip&gt;\nCreate new admin user: sudo adduser &lt;user&gt;\nAdd user to sudo group: sudo usermod -aG sudo &lt;user&gt;\nDisable root login\n\nOpen SSH configuration: sudo vi /etc/ssh/sshd_config\nSet PermitRootLogin to no\n\nConfigure Uncomplicated Firewall (UFW)\n\nAllow SSH connections: sudo ufw allow OpenSSH\nOpen port 80 and 443 for Apache: sudo ufw allow 'Apache Full'\nTurn on firewall: sudo ufw enable\nCheck: sudo ufw status verbose\n\nReboot the server: sudo reboot\nLogin as new user: ssh &lt;user&gt;@&lt;ip&gt;\nCheck for package updates apt list --upgradable 1, Upgrade packages sudo apt upgrade\nShow system info: uname -a\nEven though the kernel was updated, need to reboot to take effect: sudo reboot\nJust to be sure: sudo apt update && sudo apt upgrade\nShutdown for backup/snapshot sudo shutdown -h now\nAfter snapshot, navigate to provider website and restart machine"
  },
  {
    "objectID": "content/development/server/index.html#server-initialization",
    "href": "content/development/server/index.html#server-initialization",
    "title": "Server",
    "section": "",
    "text": "Initial Linux server setup steps:\n\nLogin to server as root: ssh root@&lt;ip&gt;\nCreate new admin user: sudo adduser &lt;user&gt;\nAdd user to sudo group: sudo usermod -aG sudo &lt;user&gt;\nDisable root login\n\nOpen SSH configuration: sudo vi /etc/ssh/sshd_config\nSet PermitRootLogin to no\n\nConfigure Uncomplicated Firewall (UFW)\n\nAllow SSH connections: sudo ufw allow OpenSSH\nOpen port 80 and 443 for Apache: sudo ufw allow 'Apache Full'\nTurn on firewall: sudo ufw enable\nCheck: sudo ufw status verbose\n\nReboot the server: sudo reboot\nLogin as new user: ssh &lt;user&gt;@&lt;ip&gt;\nCheck for package updates apt list --upgradable 1, Upgrade packages sudo apt upgrade\nShow system info: uname -a\nEven though the kernel was updated, need to reboot to take effect: sudo reboot\nJust to be sure: sudo apt update && sudo apt upgrade\nShutdown for backup/snapshot sudo shutdown -h now\nAfter snapshot, navigate to provider website and restart machine"
  },
  {
    "objectID": "content/development/server/index.html#permissions",
    "href": "content/development/server/index.html#permissions",
    "title": "Server",
    "section": "Permissions",
    "text": "Permissions\n\nSyntax\nGeneral: _rwxrwxrwx 1 owner group\n_ | rwx | rwx | rwx = Special | Owner | Group | All Users\n\nShorthand\n\n4 | r = Read\n2 | w = Write\n1 | x = Execute\n\n\n\nDetailed\n\n0 = ---\n1 = --x\n2 = -w-\n3 = -wx\n4 = r-\n5 = r-x\n6 = rw-\n7 = rwx\n\n\n\n\nCommands\nchgrp = Change group\nExample: sudo chgrp -R &lt;group&gt; &lt;folder&gt;\nchown = Change ownership\nExample: sudo chown -R &lt;user&gt;:&lt;group&gt; &lt;file/folder&gt;\nchmod = Change permissions\nExample: sudo chmod -R 774 &lt;file/folder&gt;\nMake new files inherit the group: sudo chmod g+s &lt;folder&gt;\n\n\nExample\nCreate a shared directory for a group.\n\nCreate a shared directory for users to access: /share\nAssign users to a common group (staff): sudo usermod -a -G staff &lt;user&gt;\nVerify user groups: groups &lt;user&gt;\nCreate shared directory and assign permissions:\nsudo mkdir /share && \\\nsudo chgrp -R staff /share && \\  # assign group\nsudo chmod -R g+w /share && \\  # permissions\nsudo chmod -R +s /share  # inherit permissions for newly created files/folders"
  },
  {
    "objectID": "content/development/server/python-web-app.html",
    "href": "content/development/server/python-web-app.html",
    "title": "Python Web App",
    "section": "",
    "text": "Clone repository:\nsudo mkdir /etc/local/webs && \\\ncd /etc/local/webs && \\\nsudo git clone https://github.com/app.git && \\\nsudo chown -R &lt;user&gt;:&lt;user&gt; app && \\\ncd app\nCreate python virtual environment & install dependencies:\npython -m venv .venv && \\\nsource .venv/bin/activate && \\\npython -m pip install -r requirements.txt\nConfigure WSGI - /usr/local/webs/app/app.wsgi:\n#!/usr/bin/python\nimport sys\nimport logging\nimport os\n\nAPP_DIR = \"/usr/local/webs/app\"\nos.environ[\"APP_DIR\"] = APP_DIR\n\nlogging.basicConfig(stream=sys.stderr)\nsys.path.insert(0, APP_DIR)\n\nfrom app import app as application\nUpdate owner:group\ncd /usr/local/web && \\\nsudo chown -R www-data:www-data cmat"
  },
  {
    "objectID": "content/development/server/python-web-app.html#flask-app-configuration",
    "href": "content/development/server/python-web-app.html#flask-app-configuration",
    "title": "Python Web App",
    "section": "",
    "text": "Clone repository:\nsudo mkdir /etc/local/webs && \\\ncd /etc/local/webs && \\\nsudo git clone https://github.com/app.git && \\\nsudo chown -R &lt;user&gt;:&lt;user&gt; app && \\\ncd app\nCreate python virtual environment & install dependencies:\npython -m venv .venv && \\\nsource .venv/bin/activate && \\\npython -m pip install -r requirements.txt\nConfigure WSGI - /usr/local/webs/app/app.wsgi:\n#!/usr/bin/python\nimport sys\nimport logging\nimport os\n\nAPP_DIR = \"/usr/local/webs/app\"\nos.environ[\"APP_DIR\"] = APP_DIR\n\nlogging.basicConfig(stream=sys.stderr)\nsys.path.insert(0, APP_DIR)\n\nfrom app import app as application\nUpdate owner:group\ncd /usr/local/web && \\\nsudo chown -R www-data:www-data cmat"
  },
  {
    "objectID": "content/development/server/python-web-app.html#apache-configuration",
    "href": "content/development/server/python-web-app.html#apache-configuration",
    "title": "Python Web App",
    "section": "Apache Configuration",
    "text": "Apache Configuration\n\nCreate custom Apache log directory:\nsudo mkdir /usr/local/webs/apache-logs && \\\nsudo touch /usr/local/webs/apache-logs/app/error.log && \\\nsudo touch /usr/local/webs/apache-logs/app/access.log && \\\nsudo chown -R www-data:www-data /usr/local/webs/apache-logs\nCreate configuration file:\ncd /etc/apache2/sites-available && \\\nsudo vi app.conf\nConfiguration - app.conf:\n&lt;VirtualHost *:80&gt;\n    ServerName {DNS}\n\n    ServerSignature Off\n\n    RewriteEngine On\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n&lt;/VirtualHost&gt;\n\n&lt;VirtualHost *:443&gt;\n    ServerAdmin webmaster@localhost\n    ServerName {DNS}\n\n    DocumentRoot /usr/local/webs/app\n\n    WSGIDaemonProcess web-app threads=5 python-home=/usr/local/webs/app/.venv\n    WSGIProcessGroup web-app\n    WSGIScriptAlias / /usr/local/webs/app/app.wsgi\n    WSGIPassAuthorization On\n    &lt;Directory /usr/local/webs/app&gt;\n            Order allow,deny\n            Allow from all\n    &lt;/Directory&gt;\n\n    &lt;Location /&gt;\n            Require all granted\n    &lt;/Location&gt;\n\n    ErrorLog /usr/local/webs/apache-logs/app/error.log\n    CustomLog /usr/local/webs/apache-logs/app/access.log combined\n\n    SSLEngine on\n    SSLCertificateFile /etc/letsencrypt/live/{DNS}/fullchain.pem\n    SSLCertificateKeyFile /etc/letsencrypt/live/{DNS}/privkey.pem\n    Include /etc/letsencrypt/options-ssl-apache.conf\n&lt;/VirtualHost&gt;\nTest configuration:\nsudo apache2ctl configtest\nEnable the site\nsudo a2ensite app.conf\nRestart Apache service\nsudo systemctl restart apache2"
  },
  {
    "objectID": "content/development/tools/docker.html",
    "href": "content/development/tools/docker.html",
    "title": "Docker",
    "section": "",
    "text": "Steps are pulled from a Digital Ocean tutorial.\n\nUpdate list of packages and install prerequisites.\nsudo apt update && \\\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\nAdd the GPG key for the official Docker repository.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nAdd the Docker repository to APT sources and update package list from the new repo.\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"\nMake sure install is pulling from Docker repo instead of default Ubuntu.\napt-cache policy docker-ce\nInstall Docker.\nsudo apt install docker-ce\nCheck that Docker is running.\nsudo systemctl status docker\nExecute Docker commands without sudo.\nsudo usermod -aG docker ${USER} && \\\nsu - ${USER}\nVerify user is now aded to the docker group.\ngroups"
  },
  {
    "objectID": "content/development/tools/docker.html#installation",
    "href": "content/development/tools/docker.html#installation",
    "title": "Docker",
    "section": "",
    "text": "Steps are pulled from a Digital Ocean tutorial.\n\nUpdate list of packages and install prerequisites.\nsudo apt update && \\\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\nAdd the GPG key for the official Docker repository.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nAdd the Docker repository to APT sources and update package list from the new repo.\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"\nMake sure install is pulling from Docker repo instead of default Ubuntu.\napt-cache policy docker-ce\nInstall Docker.\nsudo apt install docker-ce\nCheck that Docker is running.\nsudo systemctl status docker\nExecute Docker commands without sudo.\nsudo usermod -aG docker ${USER} && \\\nsu - ${USER}\nVerify user is now aded to the docker group.\ngroups"
  },
  {
    "objectID": "content/development/tools/docker.html#resources",
    "href": "content/development/tools/docker.html#resources",
    "title": "Docker",
    "section": "Resources",
    "text": "Resources\n\nCheatSheet\nConvert Docker command to docker-compose.yml"
  },
  {
    "objectID": "content/development/tools/docker.html#image-vs.-container",
    "href": "content/development/tools/docker.html#image-vs.-container",
    "title": "Docker",
    "section": "Image vs. Container",
    "text": "Image vs. Container\nImage - Application we want to run\nContainer - Instance of that image running as a process"
  },
  {
    "objectID": "content/development/tools/docker.html#docker-basics",
    "href": "content/development/tools/docker.html#docker-basics",
    "title": "Docker",
    "section": "Docker Basics",
    "text": "Docker Basics\n\nCreate an Nginx container\ndocker run -p 80:80 -d --name webhost nginx\n\nDownloads Nginx from Docker Hub\nStarts new container from that image\nOpened port 80 on host IP\nRoutes port 80 traffic to the container IP, port 80\nView container at http://localhost:80\n\n\n\nOther examples\ndocker run -p 80:80 -d --name nginx nginx\ndocker run -p 8080:80 -d --name httpd httpd\ndocker run -p 3306:3306 --platform linux/amd64 -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql\nCreate a JupyterLab instance and attach your current directory as a volume: docker run -it --rm -p 8888:8888 -v $(PWD):/home/jovyan jupyter/pyspark-notebook\n\n\nProcesses and configurations\nCheck processes running inside a container: docker top &lt;container&gt;\nContainer configuration: docker &lt;container&gt; inspect\nCheck container stats (memory, cpu, network): docker stats &lt;container&gt;\n\n\nGetting a shell inside containers\nStart a new container interactively: docker run -it &lt;container&gt;\nRun commands in existing container: docker exec -it &lt;container&gt;\n\nExample: Start a container interactively and launch bash within it\n\nStart container and launch bash: docker run -it --name ubuntu ubuntu bash\nRun some bash command: apt-get install -y curl\nExit the container: exit\nStart and re-enter the container: docker start -ai ubuntu\n\n\n\nExample: Launch shell in running container\ndocker exec -it &lt;container&gt; bash\n\n\n\nPull an image from docker hub\ndocker pull &lt;imagename&gt;"
  },
  {
    "objectID": "content/development/tools/docker.html#docker-networks",
    "href": "content/development/tools/docker.html#docker-networks",
    "title": "Docker",
    "section": "Docker Networks",
    "text": "Docker Networks\n\nEach container is connected to a private virtual network (called “bridge”).\nEach virtual network routes through NAT firewall on host IP.\nAll containers on a virtual network can talk to each other without -p\nBest practice: Create a new virtual network for each app.\nYou can skip virtual networks and use the host IP (--net=host).\n\nGet container IP: docker inspect --format '{{ .NetworkSettings.IPAddress }}' &lt;container&gt;\n\nPublishing (#:#)\nexample: 8080:80\nleft number: published/host port\nright number: listening/container port\nTraffic passing through port 8080 on the HOST will be directed to port 80 on the container.\n\n\nDNS\nDocker uses container names as host names.\nDont rely on IPs for inter-communication.\nBest Practice Always use custom networks.\n\nAssignment\nCheck different curl versions within current versions of Ubuntu and CentOS.\nRun “curl –version” on both operating systems.\n\nSteps\nubuntu: apt-get update && apt-get install curl\ncentos: yum update curl\nThen…\ncurl --version\nAlso:\nCheck out command docker --rm"
  },
  {
    "objectID": "content/development/tools/docker.html#dockerfiles",
    "href": "content/development/tools/docker.html#dockerfiles",
    "title": "Docker",
    "section": "Dockerfiles",
    "text": "Dockerfiles\nRecipe for creating images\nEach Dockerfile stanza such as “RUN”, “CMD”, etc. are stored as a single image layer. Docker caches each layer by giving it a unique SHA (hash), so whenever the image is (re)built, it can check to see if a layer has changed, and if not, it will use the cached layer.\nDocker builds images top down, so it is best practice to structure the Dockerfile in such a way that lines which will change the most are at the bottom, and lines that will change the least are at the top. If a line is changed (ie. source code changes) Docker will rebuild that line, and thus each line after that will also need to be rebuilt."
  },
  {
    "objectID": "content/development/tools/docker.html#keeping-the-docker-system-clean",
    "href": "content/development/tools/docker.html#keeping-the-docker-system-clean",
    "title": "Docker",
    "section": "Keeping the Docker system clean",
    "text": "Keeping the Docker system clean\ndocker system prune - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache"
  },
  {
    "objectID": "content/development/tools/docker.html#volumes-an-bind-mounts",
    "href": "content/development/tools/docker.html#volumes-an-bind-mounts",
    "title": "Docker",
    "section": "Volumes an Bind Mounts",
    "text": "Volumes an Bind Mounts\nVolumes - Special location outside of container UFS\nBind Mounts - Link container path to host path\nBuild an image and named volume (persistent): docker run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql:/var/lib/mysql --platform linux/amd64 mysql"
  },
  {
    "objectID": "content/development/tools/docker.html#rebuilding-a-compose-service",
    "href": "content/development/tools/docker.html#rebuilding-a-compose-service",
    "title": "Docker",
    "section": "Rebuilding a Compose Service",
    "text": "Rebuilding a Compose Service\ndocker compose up -d --no-deps --build &lt;service_name&gt;"
  },
  {
    "objectID": "content/development/tools/github.html",
    "href": "content/development/tools/github.html",
    "title": "GitHub",
    "section": "",
    "text": "Action to push app/site to remote host: https://claritydev.net/blog/automate-deployment-workflow-nextjs-digitalocean-github-actions"
  },
  {
    "objectID": "content/recipes/bloody-mary.html",
    "href": "content/recipes/bloody-mary.html",
    "title": "Bloody Mary",
    "section": "",
    "text": "V8\nCelery salt\nHorseradish\nWorcestershire sauce\nBuffalo sauce"
  },
  {
    "objectID": "content/recipes/buffalo-cauliflower.html",
    "href": "content/recipes/buffalo-cauliflower.html",
    "title": "Buffalo Cauliflower",
    "section": "",
    "text": "Preheat oven to 450 F\nBatter:\n\n3/4 cup flour\n1 tsp paprika\n2 tsp garlic powder\n1 tsp salt\n1/2 tsp black pepper\n3/4 cup milk\n\nSauce:\n\n1/4 cup buffalo sauce\n1 tbsp honey\n1 tbsp butter\n1/2 tsp black pepper\n\nSplit cauliflower into florets and mix in batter\nBake for 20 minutes, flip after 10.\nBrush sauce onto cauliflower, bake 10 minutes.\nFlip cauliflower and brush again with sauce, bake 10 minutes."
  },
  {
    "objectID": "content/recipes/ceasar-dressing.html",
    "href": "content/recipes/ceasar-dressing.html",
    "title": "Ceasar Dressing",
    "section": "",
    "text": "Egg yolk\nOlive oil\n1/4 lemon\nJohnnys seasoning salt\nBlack pepper\nTobasco/hot sauce\nDijon mustard\nAnchovi paste\nRed wine vinegar\nWorcestershire\nGarlic\n\nSee also"
  },
  {
    "objectID": "content/recipes/day-island-iced-tea.html",
    "href": "content/recipes/day-island-iced-tea.html",
    "title": "Day Island Iced Tea",
    "section": "",
    "text": "Equal parts:\n\nIce tea flavored vodka\nRum\nGin\nTriple sec\nLemonade\nIced tea"
  },
  {
    "objectID": "content/recipes/index.html",
    "href": "content/recipes/index.html",
    "title": "Recipes",
    "section": "",
    "text": "Title\n\n\nDescription\n\n\nCook Time\n\n\nCategories\n\n\n\n\n\n\nBloody Mary\n\n\nBloody Mary\n\n\n5 minutes\n\n\nDrink,Alcohol\n\n\n\n\nBolognese\n\n\nBolognese Sauce\n\n\n1 hour\n\n\nSauce\n\n\n\n\nBuffalo Cauliflower\n\n\nBuffalo cauliflower bites\n\n\n45 minutes\n\n\nSnack\n\n\n\n\nCauliflower Pizza Crust\n\n\nCauliflower pizza crust\n\n\n45 minutes\n\n\nMeal\n\n\n\n\nCeasar Dressing\n\n\nCeasar Dressing\n\n\n10 minutes\n\n\nDressing\n\n\n\n\nCougar Gold Mac & Cheese\n\n\nRich and filling Cougar Gold mac & cheese\n\n\n1 hour\n\n\nMeal\n\n\n\n\nDay Island Iced Tea\n\n\nDay Island Iced Tea\n\n\n1 minute\n\n\nDrink,Alcohol\n\n\n\n\nEgg Protein Bars\n\n\nEgg protein bars\n\n\n1 hour\n\n\nProtein,Snack\n\n\n\n\nGreek Yogurt Protein Smoothie\n\n\nHigh protein smoothie.\n\n\n2 minutes\n\n\nProtein,Snack\n\n\n\n\nItalian Dressing\n\n\nItalian dressing\n\n\n5 minutes\n\n\nDressing\n\n\n\n\nLemon Dill Sauce\n\n\nLemon dill sauce.\n\n\n5 minutes\n\n\nDressing\n\n\n\n\nOvernight Protein Oats\n\n\nOvernight protein oats\n\n\n12 hours\n\n\nProtein,Snack\n\n\n\n\nQuinoa & Cucumber Salad\n\n\nRefreshing quinoa and cucumber salad with avacado dressing\n\n\n30 minutes\n\n\nMeal\n\n\n\n\nRavioli\n\n\nJohn’s famous ravioli\n\n\n2 hours\n\n\nMeal\n\n\n\n\nRib Marinade\n\n\nRib Marinade\n\n\n5 minutes\n\n\nMarinade\n\n\n\n\nRice Pilaf\n\n\nRice pilaf\n\n\n45 minutes\n\n\nSide\n\n\n\n\nSourdough Bread\n\n\nTypical schedule for baking sourdough bread\n\n\n24 hours\n\n\nBread\n\n\n\n\nTurkey Meatballs\n\n\nGround turkey meatballs\n\n\n30 minutes\n\n\nProtein,Snack\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/recipes/lemon-dill-sauce.html",
    "href": "content/recipes/lemon-dill-sauce.html",
    "title": "Lemon Dill Sauce",
    "section": "",
    "text": "1/2 lemon\n1 tsp dill\n4 tbsp olive oil\n1 tsp honey\n1-2 tsp dijon mustard\n1 clove garlic finely minced\npaprika & ground black pepper to taste"
  },
  {
    "objectID": "content/recipes/quinoa-cucumber-salad.html",
    "href": "content/recipes/quinoa-cucumber-salad.html",
    "title": "Quinoa & Cucumber Salad",
    "section": "",
    "text": "Salad:\n\n2 cups cucumber, spiralized or julienned\n2 cups chopped tomatoes\n2 large avocados, diced\n1 red onion, sliced\n2 cups cooked quinoa\n1 handful chopped parsley (or cilantro)\n\nDressing - blend the following:\n\n1 ripe avocado\n1/4 cup white wine vinegar\nJuice of one lime\nSalt and fresh cracked pepper, to taste\n3/4 cup olive oil"
  },
  {
    "objectID": "content/recipes/rib-marinade.html",
    "href": "content/recipes/rib-marinade.html",
    "title": "Rib Marinade",
    "section": "",
    "text": "Yellow mustard\nBrown sugar\nGarlic\nWorcestershire sauce\nMontreal steak seasoning"
  },
  {
    "objectID": "content/recipes/sourdough-bread.html",
    "href": "content/recipes/sourdough-bread.html",
    "title": "Sourdough Bread",
    "section": "",
    "text": "Timing could change based on room temperature.\n\n\n\n\n\n\n\n\nTime\nStep\nNotes\n\n\n\n\n8:00am\nFeed starter\nDiscard all but at least 2 tbsp\n\n\n12:00pm\nAutolyse\n\n\n\n1:00pm\nAdd starter & salt\n\n\n\n1:30pm\nStretch & fold\nEvery 30 min for 2-2.5 hours\n\n\n3:30pm\nBulk rise\n1-3 hours, should rise 30-50%\n\n\n8:30pm\nPre shape & bench rest\n\n\n\n9:00pm\nFinal shaping & proof\nPlace in proofing baskets, cover, place in fridge overnight\n\n\n9:00am\nBake\n500 F in dutch oven, 20 min lid on, lower to 450, 15-30 min lid off"
  },
  {
    "objectID": "content/recipes/yogurt-protein-smoothie.html",
    "href": "content/recipes/yogurt-protein-smoothie.html",
    "title": "Greek Yogurt Protein Smoothie",
    "section": "",
    "text": "2 scoops protein powder\n1/2 cup greek yogurt\n1 banana\n1/3 cup berries\n1/2 tbsp flaxseed\n1/3 tbsp chia seed\n1 tbsp honey\nAdd water for desired consistency\n\nBlend until smooth."
  },
  {
    "objectID": "content/workouts/introductory.html",
    "href": "content/workouts/introductory.html",
    "title": "Intro Workouts",
    "section": "",
    "text": "Lift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nDumbbell Bench Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbell Incline Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Flys\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nTricep Push Down\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\nFailure\n\n\n\nView\n\n\n\n\n\n\n\nSit-ups\n\n\n\n3\n\n\n\n12\n\n\n\nView\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nCable Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFarmer Carry\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nKettlebell Deadlift\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\nView\n\n\n\n\n\n\n\nHalf Squat\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n3\n\n\n\n12\n\n\n\nView\n\n\n\n\n\n\n\n1 leg balance\n\n\n\n3\n\n\n\n30 sec\n\n\n\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nBird Dog\n\n\n\n3\n\n\n\n10\n\n\n\nView"
  },
  {
    "objectID": "content/workouts/introductory.html#introductory-routines",
    "href": "content/workouts/introductory.html#introductory-routines",
    "title": "Intro Workouts",
    "section": "",
    "text": "Lift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nDumbbell Bench Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbell Incline Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Flys\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nTricep Push Down\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\nFailure\n\n\n\nView\n\n\n\n\n\n\n\nSit-ups\n\n\n\n3\n\n\n\n12\n\n\n\nView\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nCable Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFarmer Carry\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nKettlebell Deadlift\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\nView\n\n\n\n\n\n\n\nHalf Squat\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n3\n\n\n\n12\n\n\n\nView\n\n\n\n\n\n\n\n1 leg balance\n\n\n\n3\n\n\n\n30 sec\n\n\n\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nBird Dog\n\n\n\n3\n\n\n\n10\n\n\n\nView"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html",
    "href": "content/development/code/notebooks/data-science.html",
    "title": "Data Science",
    "section": "",
    "text": "Convert Jupyter Notebook to static HTML: $ jupyter nbconvert --to html NOTEBOOK-NAME.ipynb\nimport os\nimport pandas as pd\ndata_dir = \"./data/titanic\"\nd_test = os.path.join(data_dir, \"test.csv\")\nd_train = os.path.join(data_dir, \"train.csv\")\ndf = pd.read_csv(d_train)\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#question",
    "href": "content/development/code/notebooks/data-science.html#question",
    "title": "Data Science",
    "section": "Question",
    "text": "Question\nPredict who would survive and who won’t"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#explore-the-data",
    "href": "content/development/code/notebooks/data-science.html#explore-the-data",
    "title": "Data Science",
    "section": "Explore the data",
    "text": "Explore the data\n\n# rows, columns\ndf.shape\n\n(891, 12)\n\n\n\n# bits of data (rows x columns)\ndf.size\n\n10692\n\n\n\n# shows summary of numerical data types\ndf.describe()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\ndf.info()\n\n# dtype of \"object\" means column is incomplete (missing value) and datatype cannot be determined\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\ndf.sample()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n368\n369\n1\n3\nJermyn, Miss. Annie\nfemale\nNaN\n0\n0\n14313\n7.75\nNaN\nQ\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#accessing-pandas-dataframe-columns",
    "href": "content/development/code/notebooks/data-science.html#accessing-pandas-dataframe-columns",
    "title": "Data Science",
    "section": "Accessing pandas dataframe columns",
    "text": "Accessing pandas dataframe columns\n\ndf[\"Fare\"] = 0\ndf.sample()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n250\n251\n0\n3\nReed, Mr. James George\nmale\nNaN\n0\n0\n362316\n0\nNaN\nS"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#delete-a-column",
    "href": "content/development/code/notebooks/data-science.html#delete-a-column",
    "title": "Data Science",
    "section": "Delete a column",
    "text": "Delete a column\n\ndel df[\"Cabin\"]\ndf.sample()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nEmbarked\n\n\n\n\n699\n700\n0\n3\nHumblen, Mr. Adolf Mathias Nicolai Olsen\nmale\n42.0\n0\n0\n348121\n0\nS\n\n\n\n\n\n\n\n\n# reset dataframe\ndf = pd.read_csv(d_train)"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#variable-types",
    "href": "content/development/code/notebooks/data-science.html#variable-types",
    "title": "Data Science",
    "section": "Variable Types",
    "text": "Variable Types\nDependent vs Independent variable types\n\nCategorical\n\nNominal - Any number of categories, order not important (eg male or female)\nOrdinal - Order is important (eg educational background)\n\nNumerical\n\nDiscrete - Count, dice throwing, etc.\nContinuous - Height of a person, weight, height of a tree, speed of a car."
  },
  {
    "objectID": "content/development/code/notebooks/logging.html",
    "href": "content/development/code/notebooks/logging.html",
    "title": "Logging",
    "section": "",
    "text": ".\n├── script.py\n└── my_library\n    ├── __init__.py\n    ├── module.py\n    └── submodule.py"
  },
  {
    "objectID": "content/development/code/notebooks/logging.html#script.py",
    "href": "content/development/code/notebooks/logging.html#script.py",
    "title": "Logging",
    "section": "script.py",
    "text": "script.py\n\nimport os\nimport logging\nfrom datetime import datetime\n\nfrom my_library import module\n\n# Do not specify __name__ to use root log level\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\n    \"%(asctime)s : %(msecs)04d : %(name)s : %(levelname)s : %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlog_file = f\"log_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.log\"\nstream_handler = logging.StreamHandler()\nstream_handler.setFormatter(formatter)\nfile_handler = logging.FileHandler(filename=log_file)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(stream_handler)\nlogger.addHandler(file_handler)\nlogger.info(\"Begin my test module\")\n\nmodule.main()\n\nos.remove(log_file)\n\n2022-12-17 06:03:54 : 0441 : __main__ : INFO : Begin my test module\n2022-12-17 06:03:54 : 0443 : __main__ : INFO : Log message from main function in module.py\n2022-12-17 06:03:54 : 0444 : __main__ : INFO : Hello from submodule.py in function bar()"
  },
  {
    "objectID": "content/development/code/notebooks/logging.html#module.py",
    "href": "content/development/code/notebooks/logging.html#module.py",
    "title": "Logging",
    "section": "module.py",
    "text": "module.py\nimport logging\nfrom my_library import submodule\n\n\ndef main():\n    logger = logging.getLogger(\"__main__\")\n    logger.info(\"Log message from main function in module.py\")\n    submodule.bar()\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "content/development/code/notebooks/logging.html#submodule.py",
    "href": "content/development/code/notebooks/logging.html#submodule.py",
    "title": "Logging",
    "section": "submodule.py",
    "text": "submodule.py\nimport logging\n\n\ndef bar():\n    logger = logging.getLogger()\n    logger.info(\"Hello from submodule.py in function bar()\")"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html",
    "href": "content/development/code/notebooks/ml-train-and-test.html",
    "title": "ML Training",
    "section": "",
    "text": "PROTIP - type function name with empty paranthesis and press shift+tab inside paranthesis to see documentation\nimport os\nimport pandas as pd\ndata_dir = \"./data/titanic\"\ndf = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ndf.sample(5)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n301\n302\n1\n3\nMcCoy, Mr. Bernard\nmale\nNaN\n2\n0\n367226\n23.25\nNaN\nQ\n\n\n179\n180\n0\n3\nLeonard, Mr. Lionel\nmale\n36.0\n0\n0\nLINE\n0.00\nNaN\nS\n\n\n865\n866\n1\n2\nBystrom, Mrs. (Karolina)\nfemale\n42.0\n0\n0\n236852\n13.00\nNaN\nS\n\n\n112\n113\n0\n3\nBarton, Mr. David John\nmale\n22.0\n0\n0\n324669\n8.05\nNaN\nS\n\n\n530\n531\n1\n2\nQuick, Miss. Phyllis May\nfemale\n2.0\n1\n1\n26360\n26.00\nNaN\nS\ny= dependent variable\nx = independent variable\ny = df[\"Survived\"]\nx = df.drop([\"Survived\", \"PassengerId\"], axis=1)\nx.sample()\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n732\n2\nKnight, Mr. Robert J\nmale\nNaN\n0\n0\n239855\n0.0\nNaN\nS\nfrom sklearn.model_selection import train_test_split\n# Order matters (train, test)\n\n# test_size = what percent of training data goes into test model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\nx_train.shape\n\n(801, 10)\nx_test.shape\n\n(90, 10)\nDecision Trees - nodes, branches, leafs\nProne to overfitting. Overcome by using random forests and using multiple iterations"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html#first-ml-model",
    "href": "content/development/code/notebooks/ml-train-and-test.html#first-ml-model",
    "title": "ML Training",
    "section": "First ML Model",
    "text": "First ML Model\n\nGetting started\n\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata_dir = \"./data/titanic\"\ndf = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ndf.sample(5)\n\ny = df[\"Survived\"]\nx = df.drop([\"Survived\", \"PassengerId\"], axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\n# Count number of null values per column\ndf.isnull().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\n# List all of peoples tables\ndef get_title(name):\n    if \".\" in name:\n        return name.split(\",\")[1].split(\".\")[0].strip()\n    else:\n        return \"Unknown\"\n\n\ntitles = sorted(set([x for x in df.Name.map(lambda x: get_title(x))]))\nprint(\"Different titles found in the dataset: \")\nprint(len(titles), \":\", titles)\n\nDifferent titles found in the dataset: \n17 : ['Capt', 'Col', 'Don', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Master', 'Miss', 'Mlle', 'Mme', 'Mr', 'Mrs', 'Ms', 'Rev', 'Sir', 'the Countess']\n\n\n\n# Normalize the titles\ndef replace_titles(x):\n    title = x[\"Title\"]\n    if title in [\"Capt\", \"Col\", \"Major\"]:\n        return \"Officer\"\n    elif title in [\"Jonkheer\", \"Don\", \"the Countess\", \"Dona\", \"Lady\", \"Sir\"]:\n        return \"Royalty\"\n    elif title in [\"Mme\"]:\n        return \"Mrs\"\n    elif title in [\"Mlle\", \"Ms\"]:\n        return \"Miss\"\n    else:\n        return title\n\n\ndf[\"Title\"] = df[\"Name\"].map(lambda x: get_title(x))\ndf[\"Title\"] = df.apply(replace_titles, axis=1)\nprint(df.Title.value_counts())\n\nMr         517\nMiss       185\nMrs        126\nMaster      40\nDr           7\nRev          6\nOfficer      5\nRoyalty      5\nName: Title, dtype: int64\n\n\n\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\ndf[\"Embarked\"].fillna(\"S\", inplace=True)\ndf.drop(\"Cabin\", axis=1, inplace=True)\ndf.drop(\"Ticket\", axis=1, inplace=True)\ndf.drop(\"Name\", axis=1, inplace=True)\ndf.Sex.replace((\"male\", \"female\"), (0, 1), inplace=True)\ndf.Embarked.replace((\"S\", \"C\", \"Q\"), (0, 1, 2), inplace=True)\ndf.Title.replace(\n    (\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Office\", \"Royalty\"),\n    (0, 1, 2, 3, 4, 5, 6, 7),\n    inplace=True,\n)\n\nprint(df.isnull().sum())\nprint(df[\"Sex\"].sample(5))\nprint(df.columns)\n\nPassengerId    0\nSurvived       0\nPclass         0\nSex            0\nAge            0\nSibSp          0\nParch          0\nFare           0\nEmbarked       0\nTitle          0\ndtype: int64\n791    0\n161    1\n284    0\n323    1\n127    0\nName: Sex, dtype: int64\nIndex(['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Fare', 'Embarked', 'Title'],\n      dtype='object')\n\n\n\n# Correlate 2 columns\ncorr = df.corr()\ncorr.Survived\n\nPassengerId   -0.005007\nSurvived       1.000000\nPclass        -0.338481\nSex            0.543351\nAge           -0.064910\nSibSp         -0.035322\nParch          0.081629\nFare           0.257307\nEmbarked       0.106811\nName: Survived, dtype: float64"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html#machine-learning-model---putting-it-all-together",
    "href": "content/development/code/notebooks/ml-train-and-test.html#machine-learning-model---putting-it-all-together",
    "title": "ML Training",
    "section": "Machine Learning Model - Putting it all together",
    "text": "Machine Learning Model - Putting it all together\n\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata_dir = \"../10_Data Science/data/titanic\"\ndf = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ndf.sample(5)\n\n\n# List all of peoples tables\ndef get_title(name):\n    if \".\" in name:\n        return name.split(\",\")[1].split(\".\")[0].strip()\n    else:\n        return \"Unknown\"\n\n\ntitles = sorted(set([x for x in df.Name.map(lambda x: get_title(x))]))\n\n\n# Normalize the titles\ndef replace_titles(x):\n    title = x[\"Title\"]\n    if title in [\"Capt\", \"Col\", \"Major\"]:\n        return \"Officer\"\n    elif title in [\"Jonkheer\", \"Don\", \"the Countess\", \"Dona\", \"Lady\", \"Sir\"]:\n        return \"Royalty\"\n    elif title in [\"Mme\"]:\n        return \"Mrs\"\n    elif title in [\"Mlle\", \"Ms\"]:\n        return \"Miss\"\n    else:\n        return title\n\n\ndf[\"Title\"] = df[\"Name\"].map(lambda x: get_title(x))\ndf[\"Title\"] = df.apply(replace_titles, axis=1)\n\n# Normalize data\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\ndf[\"Embarked\"].fillna(\"S\", inplace=True)\ndf.drop(\"Cabin\", axis=1, inplace=True)\ndf.drop(\"Ticket\", axis=1, inplace=True)\ndf.drop(\"Name\", axis=1, inplace=True)\ndf.Sex.replace((\"male\", \"female\"), (0, 1), inplace=True)\ndf.Embarked.replace((\"S\", \"C\", \"Q\"), (0, 1, 2), inplace=True)\ndf.Title.replace(\n    (\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Officer\", \"Royalty\"),\n    (0, 1, 2, 3, 4, 5, 6, 7),\n    inplace=True,\n)\n\ny = df[\"Survived\"]\nx = df.drop([\"Survived\", \"PassengerId\"], axis=1)\n\n# print(x.sample())\n# print(y.sample())\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1)\n\n\n# Saving the model\nimport pickle\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrandomforest = RandomForestClassifier()  # initiate random forest classification\nrandomforest.fit(x_train, y_train)  # train the model\ny_pred = randomforest.predict(x_val)  # Make some predicions using the x validation\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy: {}\".format(acc_randomforest))\n\npickle.dump(randomforest, open(\"titanic_model.sav\", \"wb\"))\n\nAccuracy: 83.33"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html#make-some-ml-predictions",
    "href": "content/development/code/notebooks/ml-train-and-test.html#make-some-ml-predictions",
    "title": "ML Training",
    "section": "Make some ML predictions",
    "text": "Make some ML predictions\n\ndf_test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n\ndf_test[\"Title\"] = df_test[\"Name\"].map(lambda x: get_title(x))\ndf_test[\"Title\"] = df_test.apply(replace_titles, axis=1)\n\nids = df_test[\"PassengerId\"]\n\ndf_test[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf_test[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\ndf_test[\"Embarked\"].fillna(\"S\", inplace=True)\ndf_test.drop(\"Cabin\", axis=1, inplace=True)\ndf_test.drop(\"Ticket\", axis=1, inplace=True)\ndf_test.drop(\"Name\", axis=1, inplace=True)\ndf_test.drop(\"PassengerId\", axis=1, inplace=True)\ndf_test.Sex.replace((\"male\", \"female\"), (0, 1), inplace=True)\ndf_test.Embarked.replace((\"S\", \"C\", \"Q\"), (0, 1, 2), inplace=True)\ndf_test.Title.replace(\n    (\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Officer\", \"Royalty\"),\n    (0, 1, 2, 3, 4, 5, 6, 7),\n    inplace=True,\n)\n\ndf_test.sample()\n\n\n\n\n\n\n\n\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\nTitle\n\n\n\n\n110\n2\n0\n41.0\n0\n0\n15.0458\n1\n0\n\n\n\n\n\n\n\n\npredictions = randomforest.predict(df_test)\noutput = pd.DataFrame({\"PassengerId\": ids, \"Survived\": predictions})\noutput.to_csv(\"submission.csv\", index=False)\n\n\nimport numpy as np\n\nx = [1, 2, 3, 4, 5]\ny = [5, 7, 9, 13, 23]\nm, b = np.polyfit(x, y, 1)\nprint(m, b)\n\n4.2 -1.2000000000000026"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html#ml-compiled-predictor",
    "href": "content/development/code/notebooks/ml-train-and-test.html#ml-compiled-predictor",
    "title": "ML Training",
    "section": "ML Compiled Predictor",
    "text": "ML Compiled Predictor\nCompiled machine learning predictor.\n\nimport os\nimport pickle\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndata_dir = \"./data/titanic\"\ndf = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n\n\n# List all of peoples tables\ndef get_title(name):\n    if \".\" in name:\n        return name.split(\",\")[1].split(\".\")[0].strip()\n    else:\n        return \"Unknown\"\n\n\n# Normalize the titles\ndef replace_titles(x):\n    title = x[\"Title\"]\n    if title in [\"Capt\", \"Col\", \"Major\"]:\n        return \"Officer\"\n    elif title in [\"Jonkheer\", \"Don\", \"the Countess\", \"Dona\", \"Lady\", \"Sir\"]:\n        return \"Royalty\"\n    elif title in [\"the Countess\", \"Mme\", \"Lady\"]:\n        return \"Mrs\"\n    elif title in [\"Mlle\", \"Ms\"]:\n        return \"Miss\"\n    else:\n        return title\n\n\ndf[\"Title\"] = df[\"Name\"].map(lambda x: get_title(x))\ndf[\"Title\"] = df.apply(replace_titles, axis=1)\n\n# Normalize data\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\ndf[\"Embarked\"].fillna(\"S\", inplace=True)\ndf.drop(\"Cabin\", axis=1, inplace=True)\ndf.drop(\"Ticket\", axis=1, inplace=True)\ndf.drop(\"Name\", axis=1, inplace=True)\ndf.Sex.replace((\"male\", \"female\"), (0, 1), inplace=True)\ndf.Embarked.replace((\"S\", \"C\", \"Q\"), (0, 1, 2), inplace=True)\ndf.Title.replace(\n    (\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Officer\", \"Royalty\"),\n    (0, 1, 2, 3, 4, 5, 6, 7),\n    inplace=True,\n)\n\ny = df[\"Survived\"]\nx = df.drop([\"Survived\", \"PassengerId\"], axis=1)\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1)\n\nrandomforest = RandomForestClassifier()  # initiate random forest classification\nrandomforest.fit(x_train, y_train)  # train the model\n\npickle.dump(randomforest, open(\"titanic_model.sav\", \"wb\"))\n\n\ndef prediction_model(pclass, sex, age, sibsp, parch, fare, embarked, title):\n    import pickle\n\n    x = [[pclass, sex, age, sibsp, parch, fare, embarked, title]]\n    randomforest = pickle.load(open(\"titanic_model.sav\", \"rb\"))\n    predictions = randomforest.predict(x)\n    print(predictions)\n\n\nprediction_model(1, 1, 11, 1, 1, 19, 1, 1)"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html",
    "href": "content/development/code/notebooks/python-fundamentals.html",
    "title": "Python Fundamentals",
    "section": "",
    "text": "Concepts and methods on the fundamentals of Python."
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#creating-a-class",
    "href": "content/development/code/notebooks/python-fundamentals.html#creating-a-class",
    "title": "Python Fundamentals",
    "section": "Creating a class",
    "text": "Creating a class\n\nclass MyClass:\n    x = 55\n    name = \"Allie Trent\"\n    attr_list = [\"Bones\", \"Food\", \"Frisbee\"]\n\n\nMyClass.name\n\n'Allie Trent'"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#methods-in-a-class",
    "href": "content/development/code/notebooks/python-fundamentals.html#methods-in-a-class",
    "title": "Python Fundamentals",
    "section": "Methods in a class",
    "text": "Methods in a class\n\nclass Methods_Class:\n    x = \"Hello World\"\n\n    def my_method():\n        print(\"Contents of my_method\")\n\n    def subtractor(num_one, num_two):\n        product = num_one - num_two\n        print(\"{} - {} = {}\".format(num_two, num_one, product))\n\n    def print_stuff(a, b):\n        print(a, b)\n\n\nprint(Methods_Class)\nprint(Methods_Class.x)\nprint(Methods_Class.my_method)\nprint(Methods_Class.my_method())\nprint(Methods_Class.subtractor(20, 5))\nprint(Methods_Class.print_stuff(3, \"Hello\"))\n\n&lt;class '__main__.Methods_Class'&gt;\nHello World\n&lt;function Methods_Class.my_method at 0xffffab79b400&gt;\nContents of my_method\nNone\n5 - 20 = 15\nNone\n3 Hello\nNone"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#init__",
    "href": "content/development/code/notebooks/python-fundamentals.html#init__",
    "title": "Python Fundamentals",
    "section": "__init__",
    "text": "__init__\nFunction defined at the start of a class\n\nclass Animals:\n    def __init__(self, size, noise, color, num_of_legs):\n        self.size = size\n        self.noise = noise\n        self.color = color\n        self.num_of_legs = num_of_legs\n\n\ndog = Animals(\"Large\", \"Woof\", \"Liver & White\", 4)\ndog\n\n&lt;__main__.Animals at 0xffffab81e590&gt;\n\n\n\ndog.noise\n\n'Woof'"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#using-attributes-in-a-method",
    "href": "content/development/code/notebooks/python-fundamentals.html#using-attributes-in-a-method",
    "title": "Python Fundamentals",
    "section": "Using attributes in a method",
    "text": "Using attributes in a method\n\nclass Animals:\n    def __init__(self, species, name, noise, color):\n        self.species = species\n        self.name = name\n        self.noise = noise\n        self.color = color\n\n    def describe(self):\n        print(\n            \"{} is {} which makes a {} noise\".format(self.name, self.color, self.noise)\n        )\n\n\ndog = Animals(\"Dog\", \"Allie\", \"Woof\", \"Liver & White\")\ncat = Animals(\"Cat\", \"Misty\", \"Meow\", \"Black\")\n\n\ndog.describe()\n\nAllie is Liver & White which makes a Woof noise"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#changing-variable-names-in-a-class-object",
    "href": "content/development/code/notebooks/python-fundamentals.html#changing-variable-names-in-a-class-object",
    "title": "Python Fundamentals",
    "section": "Changing variable names in a class object",
    "text": "Changing variable names in a class object\n\nclass Ages:\n    def __init__(self, age):\n        self.age = age\n\n    def plus_year(self):\n        self.age += 1\n\n    def show_age(self):\n        print(\"Your age is {}\".format(self.age))\n\n\nme = Ages(27)\n\n\nme.plus_year()\nme.show_age()\n\nYour age is 28\n\n\n\nclass Warrior:\n    def __init__(self, name, strength, health):\n        self.name = name\n        self.strength = strength\n        self.health = health\n\n    def report(self):\n        print(\n            \"Hi {}, your strength is {} and health is {}\".format(\n                self.name, self.strength, self.health\n            )\n        )\n\n    def heal(self):\n        self.health += 1\n\n    def damage(self):\n        self.health -= 1\n\n    def workout(self):\n        self.strength += 1\n\n\ncharacter = Warrior(\"Geocoug\", 60, 100)\n\n\n[character.damage() for i in range(8)]\ncharacter.report()\n[character.heal() for i in range(5)]\ncharacter.report()\ncharacter.workout()\ncharacter.report()\n\nHi Geocoug, your strength is 60 and health is 92\nHi Geocoug, your strength is 60 and health is 97\nHi Geocoug, your strength is 61 and health is 97"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#practical-example---payfriend",
    "href": "content/development/code/notebooks/python-fundamentals.html#practical-example---payfriend",
    "title": "Python Fundamentals",
    "section": "Practical Example - PayFriend",
    "text": "Practical Example - PayFriend\nCreate an online bank where: - User can create a new account that includes: Account type (currnet, savings, etc), name of account holder, account balance, etc. - User can withdraw or deposit money, or check balance\n\nclass Banking:\n    def __init__(self, name, account, balance):\n        self.name = name\n        self.account = account\n        self.balance = balance\n\n    def deposit(self, amount):\n        self.balance += amount\n\n    def withdraw(self, amount):\n        self.balance -= amount\n\n    def report(self):\n        print(\n            \"{}, the balance of {} is ${}\".format(self.name, self.account, self.balance)\n        )\n\n\nmy_account = Banking(\"Geocoug\", \"Savings\", 1000)\n\n\nmy_account.deposit(50)\nmy_account.report()\nmy_account.withdraw(500)\nmy_account.report()\n\nGeocoug, the balance of Savings is $1050\nGeocoug, the balance of Savings is $550"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#lambda",
    "href": "content/development/code/notebooks/python-fundamentals.html#lambda",
    "title": "Python Fundamentals",
    "section": "Lambda",
    "text": "Lambda\nQuicker way of creating functions\n\ndef some_func(x):\n    y = x + 2\n\n\nlambda x: x + 2\n\n&lt;function __main__.&lt;lambda&gt;(x)&gt;\n\n\n\nlambda_func = lambda x: x + 2\nlambda_func(6)\n\n8\n\n\n\nother_func = lambda name: \"Hello there, {}\".format(name)\nother_func(\"Geocoug\")\n\n'Hello there, Geocoug'\n\n\n\nanother_func = lambda x, y: x + y\nanother_func(3, 4)\n\n7"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#map",
    "href": "content/development/code/notebooks/python-fundamentals.html#map",
    "title": "Python Fundamentals",
    "section": "Map",
    "text": "Map\nApply the same function/operation on all elements in a list\nmap(name of function, name of list)\n\n# using map as a normal function\nsomelist = [1, 10, 20, 15]\n\n\ndef divider(x):\n    y = x / 5\n    return y\n\n\nnewlist = list(map(divider, somelist))\nnewlist\n\n[0.2, 2.0, 4.0, 3.0]\n\n\n\nsomelist = [1, 10, 20, 15]\nnewlist = list(map(lambda x: x / 5, somelist))\nnewlist\n\n[0.2, 2.0, 4.0, 3.0]\n\n\n\nnum_tuple = (1, 10, 20, 15)\nnew_tuple = tuple(map(lambda x: x / 5, num_tuple))\nnew_tuple\n\n(0.2, 2.0, 4.0, 3.0)\n\n\n\nlist_one = [1, 2, 3, 4]\nlist_two = [90, 80, 70, 60]\n\nnew_list = map(lambda x, y: x + y, list_one, list_two)\nlist(new_list)\n\n[91, 82, 73, 64]"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#filter",
    "href": "content/development/code/notebooks/python-fundamentals.html#filter",
    "title": "Python Fundamentals",
    "section": "Filter",
    "text": "Filter\n\nmy_list = [1, 14, 53, 72, 22, 99]\nfiltered_list = filter(lambda x: x &gt;= 50, my_list)\nlist(filtered_list)\n\n[53, 72, 99]"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#generators",
    "href": "content/development/code/notebooks/python-fundamentals.html#generators",
    "title": "Python Fundamentals",
    "section": "Generators",
    "text": "Generators\nGenerators - yield instead of return. Return terminates local variables, yield ‘pauses’.\nSee - How do python generators work?\n\ndef a():\n    x = 5\n    yield x\n\n\na()\n\n&lt;generator object a at 0xffffab9637d0&gt;\n\n\n\ndef a():\n    x = 5\n    yield x\n    x += 1\n    yield x\n\n\nexample = a()\nprint(next(example))\nprint(next(example))\nprint(next(example))\n\n5\n6\n\n\nStopIteration: \n\n\n\ndef generator_fn(x):\n    for i in range(x):\n        yield i\n\n\nimport sys\n\nsys.getsizeof(example)  # bytes\n\n104\n\n\n\ng = generator_fn(100_000_000)\n\n\ndir(g)\n\n['__class__',\n '__del__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__lt__',\n '__name__',\n '__ne__',\n '__new__',\n '__next__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'close',\n 'gi_code',\n 'gi_frame',\n 'gi_running',\n 'gi_yieldfrom',\n 'send',\n 'throw']\n\n\n\nnext(g)\n\n0\n\n\n\nnext(g)\n\n1\n\n\n\nnext(g)\n\n2\n\n\n\nhelp(g)\n\nHelp on generator object:\n\ngenerator_fn = class generator(object)\n |  Methods defined here:\n |  \n |  __del__(...)\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __next__(self, /)\n |      Implement next(self).\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  close(...)\n |      close() -&gt; raise GeneratorExit inside generator.\n |  \n |  send(...)\n |      send(arg) -&gt; send 'arg' into generator,\n |      return next yielded value or raise StopIteration.\n |  \n |  throw(...)\n |      throw(value)\n |      throw(type[,value[,tb]])\n |      \n |      Raise exception in generator, return next yielded value or raise\n |      StopIteration.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  gi_code\n |  \n |  gi_frame\n |  \n |  gi_running\n |  \n |  gi_yieldfrom\n |      object being iterated by yield from, or None\n\n\n\n\nNested Generators\n\ndef inner():\n    print(\"We're inside\")\n    value = yield 2\n    print(\"Received\", value)\n    return 4\n\n\ndef outer():\n    yield 1\n    retval = yield from inner()\n    print(\"Returned\", retval)\n    yield 5\n\n\ng = outer()\n\n\nnext(g)\n\n1\n\n\n\nnext(g)\n\nWe're inside\n\n\n2\n\n\n\ng.send(3)\n\nReceived 3\nReturned 4\n\n\n5\n\n\n\n\nGenerator vs. List Comprehension\n\nimport time\n\n\nn = 100_000_000\n\n\nstart = time.time()\nx = [i * i for i in range(n)]\nend = time.time()\nprint(type(x))\nprint(\"{:.4f}\".format(end - start))\n\n&lt;class 'list'&gt;\n4.3685\n\n\n\nstart = time.time()\ng = (i * i for i in range(n))\nend = time.time()\nprint(type(g))\nprint(\"{:.4f}\".format(end - start))\n\n&lt;class 'generator'&gt;\n0.0001"
  },
  {
    "objectID": "content/development/code/notebooks/visualization-with-seaborn.html",
    "href": "content/development/code/notebooks/visualization-with-seaborn.html",
    "title": "Visualization with Seaborn",
    "section": "",
    "text": "Matplotlib has proven to be an incredibly useful and popular visualization tool, but even avid users will admit it often leaves much to be desired. There are several valid complaints about Matplotlib that often come up:\nAn answer to these problems is Seaborn. Seaborn provides an API on top of Matplotlib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality provided by Pandas DataFrames.\nTo be fair, the Matplotlib team is addressing this: it has recently added the plt.style tools discussed in Customizing Matplotlib: Configurations and Style Sheets, and is starting to handle Pandas data more seamlessly. The 2.0 release of the library will include a new default stylesheet that will improve on the current status quo. But for all the reasons just discussed, Seaborn remains an extremely useful addon."
  },
  {
    "objectID": "content/development/code/notebooks/visualization-with-seaborn.html#seaborn-versus-matplotlib",
    "href": "content/development/code/notebooks/visualization-with-seaborn.html#seaborn-versus-matplotlib",
    "title": "Visualization with Seaborn",
    "section": "Seaborn Versus Matplotlib",
    "text": "Seaborn Versus Matplotlib\nHere is an example of a simple random-walk plot in Matplotlib, using its classic plot formatting and colors. We start with the typical imports:\n\nimport matplotlib.pyplot as plt\nplt.style.use('classic')\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\nNow we create some random walk data:\n\n# Create some data\nrng = np.random.RandomState(0)\nx = np.linspace(0, 10, 500)\ny = np.cumsum(rng.randn(500, 6), 0)\n\nAnd do a simple plot:\n\n# Plot the data with Matplotlib defaults\nplt.plot(x, y)\nplt.legend(\"ABCDEF\", ncol=2, loc=\"upper left\")\n\n\n\n\nAlthough the result contains all the information we’d like it to convey, it does so in a way that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of 21st-century data visualization.\nNow let’s take a look at how it works with Seaborn. As we will see, Seaborn has many of its own high-level plotting routines, but it can also overwrite Matplotlib’s default parameters and in turn get even simple Matplotlib scripts to produce vastly superior output. We can set the style by calling Seaborn’s set() method. By convention, Seaborn is imported as sns:\n\nimport seaborn as sns\n\nsns.set()\n\nNow let’s rerun the same two lines as before:\n\n# same plotting code as above!\nplt.plot(x, y)\nplt.legend(\"ABCDEF\", ncol=2, loc=\"upper left\")\n\n\n\n\nAh, much better!"
  },
  {
    "objectID": "content/development/code/notebooks/visualization-with-seaborn.html#exploring-seaborn-plots",
    "href": "content/development/code/notebooks/visualization-with-seaborn.html#exploring-seaborn-plots",
    "title": "Visualization with Seaborn",
    "section": "Exploring Seaborn Plots",
    "text": "Exploring Seaborn Plots\nThe main idea of Seaborn is that it provides high-level commands to create a variety of plot types useful for statistical data exploration, and even some statistical model fitting.\nLet’s take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood) but the Seaborn API is much more convenient.\n\nHistograms, KDE, and densities\nOften in statistical data visualization, all you want is to plot histograms and joint distributions of variables. We have seen that this is relatively straightforward in Matplotlib:\n\ndata = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], size=2000)\ndata = pd.DataFrame(data, columns=[\"x\", \"y\"])\n\nfor col in \"xy\":\n    plt.hist(data[col], normed=True, alpha=0.5)\n\n\n\n\nRather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with sns.kdeplot:\n\nfor col in \"xy\":\n    sns.kdeplot(data[col], shade=True)\n\n\n\n\nHistograms and KDE can be combined using distplot:\n\nsns.distplot(data[\"x\"])\nsns.distplot(data[\"y\"])\n\n\n\n\nIf we pass the full two-dimensional dataset to kdeplot, we will get a two-dimensional visualization of the data:\n\nsns.kdeplot(data)\n\n\n\n\nWe can see the joint distribution and the marginal distributions together using sns.jointplot. For this plot, we’ll set the style to a white background:\n\nwith sns.axes_style(\"white\"):\n    sns.jointplot(\"x\", \"y\", data, kind=\"kde\")\n\n\n\n\nThere are other parameters that can be passed to jointplot—for example, we can use a hexagonally based histogram instead:\n\nwith sns.axes_style(\"white\"):\n    sns.jointplot(\"x\", \"y\", data, kind=\"hex\")\n\n\n\n\n\n\nPair plots\nWhen you generalize joint plots to datasets of larger dimensions, you end up with pair plots. This is very useful for exploring correlations between multidimensional data, when you’d like to plot all pairs of values against each other.\nWe’ll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species:\n\niris = sns.load_dataset(\"iris\")\niris.head()\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nVisualizing the multidimensional relationships among the samples is as easy as calling sns.pairplot:\n\nsns.pairplot(iris, hue=\"species\", size=2.5)\n\n\n\n\n\n\nFaceted histograms\nSometimes the best way to view data is via histograms of subsets. Seaborn’s FacetGrid makes this extremely simple. We’ll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data:\n\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\ntips[\"tip_pct\"] = 100 * tips[\"tip\"] / tips[\"total_bill\"]\n\ngrid = sns.FacetGrid(tips, row=\"sex\", col=\"time\", margin_titles=True)\ngrid.map(plt.hist, \"tip_pct\", bins=np.linspace(0, 40, 15))\n\n\n\n\n\n\nFactor plots\nFactor plots can be useful for this kind of visualization as well. This allows you to view the distribution of a parameter within bins defined by any other parameter:\n\nwith sns.axes_style(style=\"ticks\"):\n    g = sns.factorplot(\"day\", \"total_bill\", \"sex\", data=tips, kind=\"box\")\n    g.set_axis_labels(\"Day\", \"Total Bill\")\n\n\n\n\n\n\nJoint distributions\nSimilar to the pairplot we saw earlier, we can use sns.jointplot to show the joint distribution between different datasets, along with the associated marginal distributions:\n\nwith sns.axes_style(\"white\"):\n    sns.jointplot(\"total_bill\", \"tip\", data=tips, kind=\"hex\")\n\n\n\n\nThe joint plot can even do some automatic kernel density estimation and regression:\n\nsns.jointplot(\"total_bill\", \"tip\", data=tips, kind=\"reg\")\n\n\n\n\n\n\nBar plots\nTime series can be plotted using sns.factorplot. In the following example, we’ll use the Planets data that we first saw in Aggregation and Grouping:\n\nplanets = sns.load_dataset(\"planets\")\nplanets.head()\n\n\n\n\n\n\n\nmethod\nnumber\norbital_period\nmass\ndistance\nyear\n\n\n\n\n0\nRadial Velocity\n1\n269.300\n7.10\n77.40\n2006\n\n\n1\nRadial Velocity\n1\n874.774\n2.21\n56.95\n2008\n\n\n2\nRadial Velocity\n1\n763.000\n2.60\n19.84\n2011\n\n\n3\nRadial Velocity\n1\n326.030\n19.40\n110.62\n2007\n\n\n4\nRadial Velocity\n1\n516.220\n10.50\n119.47\n2009\n\n\n\n\n\n\n\n\nwith sns.axes_style(\"white\"):\n    g = sns.factorplot(\"year\", data=planets, aspect=2, kind=\"count\", color=\"steelblue\")\n    g.set_xticklabels(step=5)\n\n\n\n\nWe can learn more by looking at the method of discovery of each of these planets:\n\nwith sns.axes_style(\"white\"):\n    g = sns.factorplot(\n        \"year\",\n        data=planets,\n        aspect=4.0,\n        kind=\"count\",\n        hue=\"method\",\n        order=range(2001, 2015),\n    )\n    g.set_ylabels(\"Number of Planets Discovered\")\n\n\n\n\nFor more information on plotting with Seaborn, see the Seaborn documentation, a tutorial, and the Seaborn gallery."
  },
  {
    "objectID": "content/development/code/notebooks/visualization-with-seaborn.html#example-exploring-marathon-finishing-times",
    "href": "content/development/code/notebooks/visualization-with-seaborn.html#example-exploring-marathon-finishing-times",
    "title": "Visualization with Seaborn",
    "section": "Example: Exploring Marathon Finishing Times",
    "text": "Example: Exploring Marathon Finishing Times\nHere we’ll look at using Seaborn to help visualize and understand finishing results from a marathon. I’ve scraped the data from sources on the Web, aggregated it and removed any identifying information, and put it on GitHub where it can be downloaded (if you are interested in using Python for web scraping, I would recommend Web Scraping with Python by Ryan Mitchell). We will start by downloading the data from the Web, and loading it into Pandas:\n\n# !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv\n\n\ndata = pd.read_csv(\"marathon-data.csv\")\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n\n\n1\n32\nM\n01:06:26\n02:09:28\n\n\n2\n31\nM\n01:06:49\n02:10:42\n\n\n3\n38\nM\n01:06:16\n02:13:45\n\n\n4\n31\nM\n01:06:32\n02:13:59\n\n\n\n\n\n\n\nBy default, Pandas loaded the time columns as Python strings (type object); we can see this by looking at the dtypes attribute of the DataFrame:\n\ndata.dtypes\n\nage        int64\ngender    object\nsplit     object\nfinal     object\ndtype: object\n\n\nLet’s fix this by providing a converter for the times:\n\nimport datetime\n\n\ndef convert_time(s):\n    h, m, s = map(int, s.split(\":\"))\n    return datetime.timedelta(hours=h, minutes=m, seconds=s)\n\n\ndata = pd.read_csv(\n    \"marathon-data.csv\", converters={\"split\": convert_time, \"final\": convert_time}\n)\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n\n\n1\n32\nM\n01:06:26\n02:09:28\n\n\n2\n31\nM\n01:06:49\n02:10:42\n\n\n3\n38\nM\n01:06:16\n02:13:45\n\n\n4\n31\nM\n01:06:32\n02:13:59\n\n\n\n\n\n\n\n\ndata.dtypes\n\nage                 int64\ngender             object\nsplit     timedelta64[ns]\nfinal     timedelta64[ns]\ndtype: object\n\n\nThat looks much better. For the purpose of our Seaborn plotting utilities, let’s next add columns that give the times in seconds:\n\ndata[\"split_sec\"] = data[\"split\"].astype(int) / 1e9\ndata[\"final_sec\"] = data[\"final\"].astype(int) / 1e9\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\nsplit_sec\nfinal_sec\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n3938.0\n7731.0\n\n\n1\n32\nM\n01:06:26\n02:09:28\n3986.0\n7768.0\n\n\n2\n31\nM\n01:06:49\n02:10:42\n4009.0\n7842.0\n\n\n3\n38\nM\n01:06:16\n02:13:45\n3976.0\n8025.0\n\n\n4\n31\nM\n01:06:32\n02:13:59\n3992.0\n8039.0\n\n\n\n\n\n\n\nTo get an idea of what the data looks like, we can plot a jointplot over the data:\n\nwith sns.axes_style(\"white\"):\n    g = sns.jointplot(\"split_sec\", \"final_sec\", data, kind=\"hex\")\n    g.ax_joint.plot(np.linspace(4000, 16000), np.linspace(8000, 32000), \":k\")\n\n\n\n\nThe dotted line shows where someone’s time would lie if they ran the marathon at a perfectly steady pace. The fact that the distribution lies above this indicates (as you might expect) that most people slow down over the course of the marathon. If you have run competitively, you’ll know that those who do the opposite—run faster during the second half of the race—are said to have “negative-split” the race.\nLet’s create another column in the data, the split fraction, which measures the degree to which each runner negative-splits or positive-splits the race:\n\ndata[\"split_frac\"] = 1 - 2 * data[\"split_sec\"] / data[\"final_sec\"]\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\nsplit_sec\nfinal_sec\nsplit_frac\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n3938.0\n7731.0\n-0.018756\n\n\n1\n32\nM\n01:06:26\n02:09:28\n3986.0\n7768.0\n-0.026262\n\n\n2\n31\nM\n01:06:49\n02:10:42\n4009.0\n7842.0\n-0.022443\n\n\n3\n38\nM\n01:06:16\n02:13:45\n3976.0\n8025.0\n0.009097\n\n\n4\n31\nM\n01:06:32\n02:13:59\n3992.0\n8039.0\n0.006842\n\n\n\n\n\n\n\nWhere this split difference is less than zero, the person negative-split the race by that fraction. Let’s do a distribution plot of this split fraction:\n\nsns.distplot(data[\"split_frac\"], kde=False)\nplt.axvline(0, color=\"k\", linestyle=\"--\")\n\n\n\n\n\nsum(data.split_frac &lt; 0)\n\n251\n\n\nOut of nearly 40,000 participants, there were only 250 people who negative-split their marathon.\nLet’s see whether there is any correlation between this split fraction and other variables. We’ll do this using a pairgrid, which draws plots of all these correlations:\n\ng = sns.PairGrid(\n    data,\n    vars=[\"age\", \"split_sec\", \"final_sec\", \"split_frac\"],\n    hue=\"gender\",\n    palette=\"RdBu_r\",\n)\ng.map(plt.scatter, alpha=0.8)\ng.add_legend()\n\n\n\n\nIt looks like the split fraction does not correlate particularly with age, but does correlate with the final time: faster runners tend to have closer to even splits on their marathon time. (We see here that Seaborn is no panacea for Matplotlib’s ills when it comes to plot styles: in particular, the x-axis labels overlap. Because the output is a simple Matplotlib plot, however, the methods in Customizing Ticks can be used to adjust such things if desired.)\nThe difference between men and women here is interesting. Let’s look at the histogram of split fractions for these two groups:\n\nsns.kdeplot(data.split_frac[data.gender == \"M\"], label=\"men\", shade=True)\nsns.kdeplot(data.split_frac[data.gender == \"W\"], label=\"women\", shade=True)\nplt.xlabel(\"split_frac\")\n\n\n\n\nThe interesting thing here is that there are many more men than women who are running close to an even split! This almost looks like some kind of bimodal distribution among the men and women. Let’s see if we can suss-out what’s going on by looking at the distributions as a function of age.\nA nice way to compare distributions is to use a violin plot\n\nsns.violinplot(\"gender\", \"split_frac\", data=data, palette=[\"lightblue\", \"lightpink\"])\n\n\n\n\nThis is yet another way to compare the distributions between men and women.\nLet’s look a little deeper, and compare these violin plots as a function of age. We’ll start by creating a new column in the array that specifies the decade of age that each person is in:\n\ndata[\"age_dec\"] = data.age.map(lambda age: 10 * (age // 10))\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\nsplit_sec\nfinal_sec\nsplit_frac\nage_dec\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n3938.0\n7731.0\n-0.018756\n30\n\n\n1\n32\nM\n01:06:26\n02:09:28\n3986.0\n7768.0\n-0.026262\n30\n\n\n2\n31\nM\n01:06:49\n02:10:42\n4009.0\n7842.0\n-0.022443\n30\n\n\n3\n38\nM\n01:06:16\n02:13:45\n3976.0\n8025.0\n0.009097\n30\n\n\n4\n31\nM\n01:06:32\n02:13:59\n3992.0\n8039.0\n0.006842\n30\n\n\n\n\n\n\n\n\nmen = data.gender == \"M\"\nwomen = data.gender == \"W\"\n\nwith sns.axes_style(style=None):\n    sns.violinplot(\n        \"age_dec\",\n        \"split_frac\",\n        hue=\"gender\",\n        data=data,\n        split=True,\n        inner=\"quartile\",\n        palette=[\"lightblue\", \"lightpink\"],\n    )\n\n\n\n\nLooking at this, we can see where the distributions of men and women differ: the split distributions of men in their 20s to 50s show a pronounced over-density toward lower splits when compared to women of the same age (or of any age, for that matter).\nAlso surprisingly, the 80-year-old women seem to outperform everyone in terms of their split time. This is probably due to the fact that we’re estimating the distribution from small numbers, as there are only a handful of runners in that range:\n\n(data.age &gt; 80).sum()\n\n7\n\n\nBack to the men with negative splits: who are these runners? Does this split fraction correlate with finishing quickly? We can plot this very easily. We’ll use regplot, which will automatically fit a linear regression to the data:\n\ng = sns.lmplot(\n    \"final_sec\",\n    \"split_frac\",\n    col=\"gender\",\n    data=data,\n    markers=\".\",\n    scatter_kws=dict(color=\"c\"),\n)\ng.map(plt.axhline, y=0.1, color=\"k\", ls=\":\")\n\n\n\n\nApparently the people with fast splits are the elite runners who are finishing within ~15,000 seconds, or about 4 hours. People slower than that are much less likely to have a fast second split."
  }
]