[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to geocoug’s resource toolkit. This site was built using Quarto."
  },
  {
    "objectID": "index.html#quick-links",
    "href": "index.html#quick-links",
    "title": "Welcome",
    "section": "Quick Links",
    "text": "Quick Links\n\n\n\n\n\n\n\n\n\n\nBookmarks\n\n\nBookmarks to miscellaneous sites.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Management\n\n\nData management guidelines and resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevelopment\n\n\nDevelopment resources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecipes\n\n\nRecipies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResources\n\n\nResources\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nServer\n\n\nServer setup and administration\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkouts\n\n\nWorkouts and routines\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/resources/recipes.html",
    "href": "docs/resources/recipes.html",
    "title": "Recipes",
    "section": "",
    "text": "Typical schedule for baking sourdough bread. Timing could change based on room temperature.\n\n\n\n\n\n\n\n\nTime\nStep\nNotes\n\n\n\n\n8:00am\nFeed starter\nDiscard all but at least 2 tbsp\n\n\n12:00pm\nAutolyse\n\n\n\n1:00pm\nAdd starter & salt\n\n\n\n1:30pm\nStretch & fold\nEvery 30 min for 2-2.5 hours\n\n\n3:30pm\nBulk rise\n1-3 hours, should rise 30-50%\n\n\n8:30pm\nPre shape & bench rest\n\n\n\n9:00pm\nFinal shaping & proof\nPlace in proofing baskets, cover, place in fridge overnight\n\n\n9:00am\nBake\n500 F in dutch oven, 20 min lid on, lower to 450, 15-30 min lid off"
  },
  {
    "objectID": "docs/resources/recipes.html#sourdough-bread",
    "href": "docs/resources/recipes.html#sourdough-bread",
    "title": "Recipes",
    "section": "",
    "text": "Typical schedule for baking sourdough bread. Timing could change based on room temperature.\n\n\n\n\n\n\n\n\nTime\nStep\nNotes\n\n\n\n\n8:00am\nFeed starter\nDiscard all but at least 2 tbsp\n\n\n12:00pm\nAutolyse\n\n\n\n1:00pm\nAdd starter & salt\n\n\n\n1:30pm\nStretch & fold\nEvery 30 min for 2-2.5 hours\n\n\n3:30pm\nBulk rise\n1-3 hours, should rise 30-50%\n\n\n8:30pm\nPre shape & bench rest\n\n\n\n9:00pm\nFinal shaping & proof\nPlace in proofing baskets, cover, place in fridge overnight\n\n\n9:00am\nBake\n500 F in dutch oven, 20 min lid on, lower to 450, 15-30 min lid off"
  },
  {
    "objectID": "docs/resources/recipes.html#greek-yogurt-protein-smoothie",
    "href": "docs/resources/recipes.html#greek-yogurt-protein-smoothie",
    "title": "Recipes",
    "section": "Greek Yogurt Protein Smoothie",
    "text": "Greek Yogurt Protein Smoothie\n\n1-2 scoops protein powder\n1/2 cup greek yogurt\n1 banana\n1/3 cup berries\n1/2 tbsp flaxseed\n1/3 tbsp chia seed\n1 tbsp honey\nice & water for desired consistency"
  },
  {
    "objectID": "docs/resources/recipes.html#overnight-protein-oats",
    "href": "docs/resources/recipes.html#overnight-protein-oats",
    "title": "Recipes",
    "section": "Overnight Protein Oats",
    "text": "Overnight Protein Oats\n\n1-2 scoops protein powder\n90 grams oats\n90 grams greek yogurt\n140 grams milk\n5 grams chia and/or flaxseed\n15 grams honey\n\nStir or shake to combine. Refrigerate overnight."
  },
  {
    "objectID": "docs/resources/recipes.html#buffalo-cauliflower",
    "href": "docs/resources/recipes.html#buffalo-cauliflower",
    "title": "Recipes",
    "section": "Buffalo Cauliflower",
    "text": "Buffalo Cauliflower\n\nPreheat oven to 450 F\nBatter:\n\n3/4 cup flour\n1 tsp paprika\n2 tsp garlic powder\n1 tsp salt\n1/2 tsp black pepper\n3/4 cup milk\n\nSauce:\n\n1/4 cup buffalo sauce\n1 tbsp Honey\n1/2 tsp black pepper\n\nSplit cauliflower into florets and mix in batter\nBake for 20 minutes, flip after 10.\nBrush sauce onto cauliflower, bake 10 minutes.\nFlip cauliflower and brush again with sauce, bake 10 minutes."
  },
  {
    "objectID": "docs/resources/recipes.html#quinoa-cucumber-salad",
    "href": "docs/resources/recipes.html#quinoa-cucumber-salad",
    "title": "Recipes",
    "section": "Quinoa & Cucumber Salad",
    "text": "Quinoa & Cucumber Salad\n\nSalad:\n\n2 cups cucumber, spiralized or julienned\n2 cups chopped tomatoes\n2 large avocados, diced\n1 red onion, sliced\n2 cups cooked quinoa\n1 handful chopped parsley (or cilantro)\n\nDressing - blend the following:\n\n1 ripe avocado\n1/4 cup white wine vinegar\nJuice of one lime\nSalt and fresh cracked pepper, to taste\n3/4 cup olive oil"
  },
  {
    "objectID": "docs/resources/recipes.html#cougar-gold-mac-cheese",
    "href": "docs/resources/recipes.html#cougar-gold-mac-cheese",
    "title": "Recipes",
    "section": "Cougar Gold Mac & Cheese",
    "text": "Cougar Gold Mac & Cheese\n\n6 cups water\n2 1/2 cups pasta\n2 tbsp butter\n2 tbsp all-purpose flour\n1 1/3 cups milk\n1 1/3 cups heavy cream\n1 1/2 cups Cougar Gold, grated\n1 1/2 cups Crimson Fire, grated\n1/2 cup ricotta\n1/2 tbsp chicken bouillon\nground black pepper, paprika to taste\n1/2 cup fresh breadcrumbs (optional)\n\nPreheat oven to 350° F.\nBring the water, salt and pasta to a rolling boil in a medium saucepan. Cook just until tender. Drain pasta, put into prepared baking dish and set aside. Meanwhile prepare the sauce. In a saucepan over medium-low heat, melt 2 tablespoons of butter. While whisking, gradually add the flour. Whisk for about 2 minutes or until golden and bubbling. Very slowly add the milk, whisking constantly to avoid developing lumps. Simmer for 15 minutes until thickened (alfredo sauce consistency), stirring often to prevent mixture from burning. Remove from heat and stir in Cougar cheese, ricotta, chicken bouillon, paprika and black pepper to taste. Pour sauce onto cooked pasta (do not stir). Optionally top with breadcrumbs tossed in melted butter. Bake for 30 minutes until browned and bubbling."
  },
  {
    "objectID": "docs/resources/recipes.html#lemon-dill-sauce",
    "href": "docs/resources/recipes.html#lemon-dill-sauce",
    "title": "Recipes",
    "section": "Lemon Dill Sauce",
    "text": "Lemon Dill Sauce\n\n1/2 lemon\n1 tsp dill\n4 tbsp olive oil\n1 tsp honey\n1-2 tsp dijon mustard\n1 clove garlic finely minced\npaprika & ground black pepper to taste"
  },
  {
    "objectID": "docs/resources/recipes.html#salad-dressing",
    "href": "docs/resources/recipes.html#salad-dressing",
    "title": "Recipes",
    "section": "Salad Dressing",
    "text": "Salad Dressing\n\n1/2 cup extra virgin olive oil\n1/3 cup apple cider vinegar\n1/4 cup honey, use maple syrup for vegan\n2 teaspoons balsamic vinegar\n1 teaspoon dijon mustard\n2 cloves garlic finely minced\npinch of sea salt"
  },
  {
    "objectID": "docs/resources/recipes.html#rice-pilaf",
    "href": "docs/resources/recipes.html#rice-pilaf",
    "title": "Recipes",
    "section": "Rice Pilaf",
    "text": "Rice Pilaf\n\n1 cup rice\n1/8 cup spaghetti noodle broken into 1/4 inch pieces\n1.5 tbsp butter\n1 tsp chicken bouillon\n2 cups water\n\nHeat the butter in a sauce pan then add rice and noodle. Add boillon to the water and mix to combine then add to sauce pan. Cook on high for 1 minute then cover and simmer for 40 minutes."
  },
  {
    "objectID": "docs/resources/recipes.html#egg-protein-bars",
    "href": "docs/resources/recipes.html#egg-protein-bars",
    "title": "Recipes",
    "section": "Egg Protein Bars",
    "text": "Egg Protein Bars\n\n1.5 lbs sausage\n15 eggs\n10 button mushrooms\n20 cherry tomatoes\n5 ounce container baby spinach\n2 cups cheddar cheese\nSalt and pepper\n\n\nPreheat your oven to 350 degrees.\nHeat a large sautee pan on high heat and brown the sausage. Add the mushrooms and sautee until browned, about 3 minutes. Add the spinach and cook for an additional minute, until slightly browned, then add sliced cherry tomatoes, stir, and take off the heat.\nWhisk eggs, salt and pepper, and cheese together, then toss in your sausage and vegetables and whisk.\nLine a deep baking dish with parchment paper, then pour egg mixture on top, and bake at 350 degrees for 40 minutes, until cooked.\nRemove from oven, allow to cool for 20 minutes, then turn over, cut into 5 portions, and wrap."
  },
  {
    "objectID": "docs/resources/recipes.html#buffalo-chicken-meatballs",
    "href": "docs/resources/recipes.html#buffalo-chicken-meatballs",
    "title": "Recipes",
    "section": "Buffalo Chicken Meatballs",
    "text": "Buffalo Chicken Meatballs\n\n3 lbs ground turkey\n0.75 cup breadcrumbs\n1 tsp salt\n6 ounces crumbled blue cheese\n0.5 cup buffalo sauce\n1 bunch scallions\n3 eggs\n3 garlic cloves\nAmoroso hoagie roll\nShrettuce\nSliced tomato\nPickles\nRanch\n\n\nAdd ground turkey, blue cheese, salt, and breadcrumbs to a large bowl.\nAdd eggs, buffalo sauce, scallions, and garlic to a blender and blend. Add the wet ingredients to the meat and mix.\nUse an ice cream scoop to scoop out the meatballs onto a parchment lined baking sheet, and bake at 550 degrees for 12 minutes.\nDouse in buffalo sauce and toss.\nSlice a hoagie roll in half, add ranch, lettuce, tomato, and meatballs, and eat."
  },
  {
    "objectID": "docs/resources/recipes.html#others",
    "href": "docs/resources/recipes.html#others",
    "title": "Recipes",
    "section": "Others",
    "text": "Others\n\nSteak Diane\nCilantro Lime Shrimp with Zucchini Noodles"
  },
  {
    "objectID": "docs/resources/data-management.html",
    "href": "docs/resources/data-management.html",
    "title": "Data Management",
    "section": "",
    "text": "EPA Guidance on Systematic Planning Using the Data Quality Objectives Process\n\n\n\n\n\n\nCode\nimport pandas as pd\n\ndqo = \"../../static/resources/DQO.xlsx\"\nelements = pd.read_excel(dqo, sheet_name=0)\nfactors = pd.read_excel(dqo, sheet_name=1)\n\n\n\n\n\n\n\n\nCode\nelements.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nElement\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nOrganization\n\n\n\nIdentification and involvement of the project manager, sponsoring organization and responsible official, project personnel, stakeholders, scientific experts, etc. (e.g., all customers and suppliers).\n\n\n\n\n\n\n\nProject Goal\n\n\n\nDescription of the project goal, objectives, and study questions and issues.\n\n\n\n\n\n\n\nSchedule\n\n\n\nIdentification of project schedule, resources (including budget), milestones, and any applicable requirements (e.g., regulatory requirements, contractual requirements).\n\n\n\n\n\n\n\nData Needs\n\n\n\nIdentification of the type of data needed and how the data will be used to support the project’s objectives.\n\n\n\n\n\n\n\nCriteria\n\n\n\nDetermination of the quantity of data needed and specification of performance criteria for measuring quality.\n\n\n\n\n\n\n\nData Collection\n\n\n\nDescription of how and where the data will be obtained (including existing data) and identification of any constraints on data collection.\n\n\n\n\n\n\n\nQuality Assurance (QA)\n\n\n\nSpecification of needed QA and quality control (QC) activities to assess the quality performance criteria (e.g., QC samples for both field and laboratory, audits, technical assessments, performance evaluations, etc.).\n\n\n\n\n\n\n\nAnalysis\n\n\n\nDescription of how the acquired data will be analyzed (either in the field or the laboratory), evaluated (i.e., QA review/verification/validation), and assessed against its intended use and the quality performance criteria.\n\n\n\n\n\n\n\n\n\n\n\nWhen specifying the project goal (element #2 in Table 1), a key activity is to determine the key questions which the study will address once data and information are properly collected and analyzed.\n\n\n\n\n\n\n\nCode\nfactors.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nFactor\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nSoundness\n\n\n\nThe extent to which the scientific and technical procedures, measures, methods or models employed to generate the information are reasonable for, and consistent with, the intended application.\n\n\n\n\n\n\n\nApplicability and Utility\n\n\n\nThe extent to which the information is relevant for the Agency’s intended use.\n\n\n\n\n\n\n\nClarity and Completeness\n\n\n\nThe degree of clarity and completeness with which the data, assumptions, methods, quality assurance, sponsoring organizations and analyses employed to generate the information are documented.\n\n\n\n\n\n\n\nUncertainty and Variability\n\n\n\nThe extent to which the variability and uncertainty (quantitative and qualitative) in the information or the procedures, measures, methods or models are evaluated and characterized.\n\n\n\n\n\n\n\nEvaluation and Review\n\n\n\nThe extent of independent verification, validation, and peer review of the information or of the procedures, measures, methods or models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchieving clarity in a project’s development becomes straightforward when using systematic planning, as almost every element of the planning process contributes to understanding how the project’s assumptions, methods, and proposed analyses will be conducted.\nPlanning for analyzing the data and information before collection clearly meets the intent of the GAFs.\nClear statements on the goals of the project developed through systematic planning leads to a better understanding of purpose and credibility of the results.\nSystematic planning leads to a clear statement of information needs and how the information will be collected, and leads to transparency in data quality."
  },
  {
    "objectID": "docs/resources/data-management.html#data-quality",
    "href": "docs/resources/data-management.html#data-quality",
    "title": "Data Management",
    "section": "",
    "text": "EPA Guidance on Systematic Planning Using the Data Quality Objectives Process\n\n\n\n\n\n\nCode\nimport pandas as pd\n\ndqo = \"../../static/resources/DQO.xlsx\"\nelements = pd.read_excel(dqo, sheet_name=0)\nfactors = pd.read_excel(dqo, sheet_name=1)\n\n\n\n\n\n\n\n\nCode\nelements.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nElement\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nOrganization\n\n\n\nIdentification and involvement of the project manager, sponsoring organization and responsible official, project personnel, stakeholders, scientific experts, etc. (e.g., all customers and suppliers).\n\n\n\n\n\n\n\nProject Goal\n\n\n\nDescription of the project goal, objectives, and study questions and issues.\n\n\n\n\n\n\n\nSchedule\n\n\n\nIdentification of project schedule, resources (including budget), milestones, and any applicable requirements (e.g., regulatory requirements, contractual requirements).\n\n\n\n\n\n\n\nData Needs\n\n\n\nIdentification of the type of data needed and how the data will be used to support the project’s objectives.\n\n\n\n\n\n\n\nCriteria\n\n\n\nDetermination of the quantity of data needed and specification of performance criteria for measuring quality.\n\n\n\n\n\n\n\nData Collection\n\n\n\nDescription of how and where the data will be obtained (including existing data) and identification of any constraints on data collection.\n\n\n\n\n\n\n\nQuality Assurance (QA)\n\n\n\nSpecification of needed QA and quality control (QC) activities to assess the quality performance criteria (e.g., QC samples for both field and laboratory, audits, technical assessments, performance evaluations, etc.).\n\n\n\n\n\n\n\nAnalysis\n\n\n\nDescription of how the acquired data will be analyzed (either in the field or the laboratory), evaluated (i.e., QA review/verification/validation), and assessed against its intended use and the quality performance criteria.\n\n\n\n\n\n\n\n\n\n\n\nWhen specifying the project goal (element #2 in Table 1), a key activity is to determine the key questions which the study will address once data and information are properly collected and analyzed.\n\n\n\n\n\n\n\nCode\nfactors.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nFactor\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nSoundness\n\n\n\nThe extent to which the scientific and technical procedures, measures, methods or models employed to generate the information are reasonable for, and consistent with, the intended application.\n\n\n\n\n\n\n\nApplicability and Utility\n\n\n\nThe extent to which the information is relevant for the Agency’s intended use.\n\n\n\n\n\n\n\nClarity and Completeness\n\n\n\nThe degree of clarity and completeness with which the data, assumptions, methods, quality assurance, sponsoring organizations and analyses employed to generate the information are documented.\n\n\n\n\n\n\n\nUncertainty and Variability\n\n\n\nThe extent to which the variability and uncertainty (quantitative and qualitative) in the information or the procedures, measures, methods or models are evaluated and characterized.\n\n\n\n\n\n\n\nEvaluation and Review\n\n\n\nThe extent of independent verification, validation, and peer review of the information or of the procedures, measures, methods or models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchieving clarity in a project’s development becomes straightforward when using systematic planning, as almost every element of the planning process contributes to understanding how the project’s assumptions, methods, and proposed analyses will be conducted.\nPlanning for analyzing the data and information before collection clearly meets the intent of the GAFs.\nClear statements on the goals of the project developed through systematic planning leads to a better understanding of purpose and credibility of the results.\nSystematic planning leads to a clear statement of information needs and how the information will be collected, and leads to transparency in data quality."
  },
  {
    "objectID": "docs/resources/data-management.html#benefits-of-using-the-dqo-process",
    "href": "docs/resources/data-management.html#benefits-of-using-the-dqo-process",
    "title": "Data Management",
    "section": "Benefits of Using the DQO Process",
    "text": "Benefits of Using the DQO Process\n\nThe interaction amongst a multidisplinary team results in a clear understanding of the problem and the options available. Organizations that have used the DQO Process have found the structured format facilitated good communicaitons, documentation, and data collection design, all of which facilitated rapid peer review and approval.\n\n\nThe structure of the DQO Process provides a convenient way to document activities and decisions and to communicate the data collection design to others.\nThe DQO Process is an effective planning tool that can save resources by making data collection operations more resource-effective.\nThe DQO Process enables data users and technical experts to participate collectively in planning and to specify their needs prior to data collection. The DQO Process helps to focus studies by encouraging data users to clarify vague objectives and document clearly how scientific theory motivating this project is applicable to the intended use of the data.\nThe DQO Process provides a method for defining performance requirements appropriate for the intended use of the data by considering the consequences of drawing incorrect conclusions and then placing tolerable limits on them.\nThe DQO Process encourages good documentation for a model-based approach to investigate the objectives of a project, with discussion on how the key parameters were estimated or derived, and the robustness of the model to small perturbations. Upon implementing the DQO Process, your environmental programs can be strengthened"
  },
  {
    "objectID": "docs/resources/data-management.html#data-handling",
    "href": "docs/resources/data-management.html#data-handling",
    "title": "Data Management",
    "section": "Data Handling",
    "text": "Data Handling\n\nResources\n\nexecSQL\nPostgreSQL Tutorial\nPostgreSQL Tips\nDevDocs.io\n\n\n\nCharacter Encoding\nWe deal with character encoding issues when importing data to databases all the time. All databases are encoded in UTF-8, and the most common non-ASCII, non-UTF-8 format is Microsoft’s custom encoding, which is CP-1252 (also known as win-1252 and a few other things). The character encoding of a file can’t necessarily be definitively determined by examination, but the Linux command-line tool file generally does a good job. If you want to change the encoding of a file rather than importing it in a known format, the Linux command-line tool iconv will do that for you. There’s a Python library on PyPI named chardet that will also diagnose file encodings.\nFor data managers, the workflow is to first guess that encoding errors on data import are due to the file being cp-1252. That covers about 90% of cases. Our import tool also automatically diagnoses instances where a file starts with a byte order mark (BOM), which covers most of the rest of the cases. For the remainders, Geany is usually the quickest way to check the file encoding.\nEverything that comes out of our databases is always in UTF-8, so I, at least, don’t ordinarily have encoding issues when importing data to R. For those who use data from other sources, it is a good idea to document a recommended workflow and set of tools."
  },
  {
    "objectID": "docs/resources/data-management.html#analytical-chemistry",
    "href": "docs/resources/data-management.html#analytical-chemistry",
    "title": "Data Management",
    "section": "Analytical Chemistry",
    "text": "Analytical Chemistry\n\nSummarization\nChemistry data frequently is summarized for use in analyses or for presentation using tables or maps. Summarization is ordinarily performed when there are multiple concentration values measured for a sample, or for a specific location, date, and depth. Multiple concentration values result from field or laboratory replications, from field splits created for quality control evaluations, and sometimes from sample reanalyses. Although field splits and laboratory replicates are created to support data quality assessments, all of the valid results that are produced are informative, are ordinarily stored in the project database, and are used to produce the most accurate possible estimate of the true concentration in a sample. When there are replicate results for a sample, the data will be averaged in a stepwise, or hierarchical, fashion. Because each level of the hierarchy represents a different source of variation, all the results at a single level are averaged together before results are averaged across levels. The different levels of replication, and the source of variation that each represents, are as follows:\n\nAverage across lab replicates\nAverage across multiple analyses (if they exist) for the same lab sample. This will average across multiple methods and reanalyses (different SDGs)\nAverage across multiple lab samples (if they exist) for the same sample number (split) and lab. Multiple lab samples may exist for the same sample number if the laboratory has created new laboratory sample IDs for reanalyses.\nAverage across multiple splits (if they exist) for the same lab and interpretive sample. Multiple sample numbers (splits) may exist for the same lab, or the same sample number may exist for different labs. Splits sent to different labs potentially have an additional source of variability in addition to variability between splits sent to the same lab.\nAverage across laboratories for the same main sample. This summarizes data by main sample (sample_id), which is typically the lowest level of data summarization to be used for data interpretation.\n\nBy default, data are summarized by successive averaging across these levels of replication, in the order given above. During the averaging process, data validation qualifiers and significant digits must be propagated. The rules for propagating the data validation qualifiers U (undetected), J (estimated), and R (rejected) are as follows:\n\nIf both detected and undetected data are to be averaged, then undetected data lower than the highest detected value will be taken at one-half the detection limit and averaged with the detected data, and the result will be identified as detected. Non-detects that are higher than the highest detected value will be omitted from the average.\nIf all data to be averaged are undetected, the result will be taken to be the lowest detection limit, and will be identified as undetected.\nIf J-qualified data are averaged with non-J-qualified data, the result will be J-qualified.\nIf R-qualified data are averaged with non-R-qualified data, the result will be R-qualified.\n\nSignificant digits are propagated so that the place (in the sense of one’s place, ten’s place, etc.) of the least significant digit of the average is equal to the highest place of the least significant digit of any of the values that are averaged.\nThese rules are built into custom aggregate functions in IDB that use the measurement_result data type.\nThese default data handling rules will be applied if no project-specific alternate rules are specified. The project manager, project technical staff, and data manager should evaluate, at the start of a project, whether an alternative approach is needed. Alternate data summarization rules should be summarized in the project plan or in the data management plan, if it exists. (Data managers: if not documented elsewhere, record this information in the Data Manager’s Manual.)\nNote that the handling of nondetects during hierarchical averaging and the presentation of nondetects in data summaries may be different. Regardless of whether nondetects are taken at half the detection limit or the full detection limit when averaging, the summarized result may be presented with either the full detection limit or half the detection limit. Reporting nondetects at the full detection limit should ordinarily be done when preparing data tables for reports or other deliverables. Data analyses to be conducted by Integral may be carried out using either half or full detection limits. The method of reporting nondetects should be specified when requesting data summaries."
  },
  {
    "objectID": "docs/resources/data-management.html#chemistry",
    "href": "docs/resources/data-management.html#chemistry",
    "title": "Data Management",
    "section": "Chemistry",
    "text": "Chemistry\n\nResources\n\nHazardous Waste Test Methods\nNational Environmental Methods Index\nSubstance Registry Service\nEIM Valid Values\nVerification and Validation\nQualifiers\nData Review\nChemical Lists\nPCBs\nWashington Water Resources Data Defs\nMeasurement Basis Conversions\nhttps://pubs.usgs.gov/wdr/WDR-WA-03-1/pdf/ADR_N.pdf\nhttp://www.eccsmobilelab.com/resources/literature/?Id=117\nConversions\n\n\n\nCode\nimport pandas as pd\n\nwb = \"../../static/resources/Chemical_Lists.xls\"\n\nchem_lists = pd.read_excel(wb, sheet_name=0)\ncwa = pd.read_excel(wb, sheet_name=1)\npfas = pd.read_excel(wb, sheet_name=2)\npesticides = pd.read_excel(wb, sheet_name=3)\npcb = pd.read_excel(wb, sheet_name=4)\npah = pd.read_excel(wb, sheet_name=5)\nfertilizers = pd.read_excel(wb, sheet_name=6)\ndioxfuran = pd.read_excel(wb, sheet_name=7)\npbde = pd.read_excel(wb, sheet_name=8)\n\n\n\n\nTEFs\n\nVan den Berg source document\nVan den Berg TEF table\n\n\n\nChemical Groups\n\nDioxin & Furans\nReference\n\n\nPCBs\nLearn about PCBs\n\nGeneral\nPCBs are a group of man-made organic chemicals consisting of carbon, hydrogen and chlorine atoms. The number of chlorine atoms and their location in a PCB molecule determine many of its physical and chemical properties. PCBs have no known taste or smell, and range in consistency from an oil to a waxy solid.\nPCBs belong to a broad family of man-made organic chemicals known as chlorinated hydrocarbons. PCBs were domestically manufactured from 1929 until manufacturing was banned in 1979. They have a range of toxicity and vary in consistency from thin, light-colored liquids to yellow or black waxy solids. Due to their non-flammability, chemical stability, high boiling point and electrical insulating properties, PCBs were used in hundreds of industrial and commercial applications including:\n\nElectrical, heat transfer and hydraulic equipment\nPlasticizers in paints, plastics and rubber products\nPigments, dyes and carbonless copy paper\nOther industrial applications\n\nCommercial Uses for PCBs\nAlthough no longer commercially produced in the United States, PCBs may be present in products and materials produced before the 1979 PCB ban. Products that may contain PCBs include:\n\nTransformers and capacitors\nElectrical equipment including voltage regulators, switches, re-closers, bushings, and electromagnets\nOil used in motors and hydraulic systems\nOld electrical devices or appliances containing PCB capacitors\nFluorescent light ballasts\nCable insulation\nThermal insulation material including fiberglass, felt, foam, and cork\nAdhesives and tapes\nOil-based paint\nCaulking\nPlastics\nCarbonless copy paper\nFloor finish\n\nThe PCBs used in these products were chemical mixtures made up of a variety of individual chlorinated biphenyl components known as congeners. Most commercial PCB mixtures are known in the United States by their industrial trade names, the most common being Arochlor.\nRelease and Exposure of PCBs\nToday, PCBs can still be released into the environment from:\n\nPoorly maintained hazardous waste sites that contain PCBs\nIllegal or improper dumping of PCB wastes\nLeaks or releases from electrical transformers containing PCBs\nDisposal of PCB-containing consumer products into municipal or other landfills not designed to handle hazardous waste\nBurning some wastes in municipal and industrial incinerators\n\nPCBs do not readily break down once in the environment. They can remain for long periods cycling between air, water and soil. PCBs can be carried long distances and have been found in snow and sea water in areas far from where they were released into the environment. As a consequence, they are found all over the world. In general, the lighter the form of PCB, the further it can be transported from the source of contamination.\nPCBs can accumulate in the leaves and above-ground parts of plants and food crops. They are also taken up into the bodies of small organisms and fish. As a result, people who ingest fish may be exposed to PCBs that have bioaccumulated in the fish they are ingesting.\nThe National Center for Health Statistics, a division of the Centers for Disease Control and Prevention, conducts the National Health and Nutrition Examination Surveys (NHANES). NHANES is a series of U.S. national surveys on the health and nutrition status of the noninstitutionalized civilian population, which includes data collection on selected chemicals. Interviews and physical examinations are conducted with approximately 10,000 people in each two-year survey cycle. PCBs are one of the chemicals where data are available from the NHANES surveys.\n\n\nPCB Congeners\nA PCB congener is any single, unique well-defined chemical compound in the PCB category. The name of a congener specifies the total number of chlorine substituents, and the position of each chlorine. For example: 4,4’-Dichlorobiphenyl is a congener comprising the biphenyl structure with two chlorine substituents - one on each of the #4 carbons of the two rings. In 1980, a numbering system was developed which assigned a sequential number to each of the 209 PCB congeners.\n\n\nPCB Homologs\nHomologs are subcategories of PCB congeners that have equal numbers of chlorine substituents. For example, the tetrachlorobiphenyls are all PCB congeners with exactly 4 chlorine substituents that can be in any arrangement.\n\n\nPCB Aroclor\nAroclor is a PCB mixture produced from approximately 1930 to 1979. It is one of the most commonly known trade names for PCB mixtures. There are many types of Aroclors and each has a distinguishing suffix number that indicates the degree of chlorination. The numbering standard for the different Aroclors is as follows:\n\nThe first two digits usually refer to the number of carbon atoms in the phenyl rings (for PCBs this is 12)\nThe second two numbers indicate the percentage of chlorine by mass in the mixture. For example, the name Aroclor 1254 means that the mixture contains approximately 54% chlorine by weight.\n\n\n\n\n\nQualifiers\n\nLabs may apply whatever flags they want to a result. Some data qualifiers are defined by EPA’s Functional Guidelines documents, which describe how data validation is to be conducted, and the use and interpretation of U, J, and R qualifiers is pretty universal (but older standards for Puget Sound data used E instead of J). Because the U, J, and R qualifiers are pretty universal and have implications for data usability, they are the only ones that are represented as Boolean fields in the meas_value column. All lab flags are put into the lab_flags column, and there is no lookup table for them, and there is no defined use for them. Similarly, the validator qualifiers (U, J, R, and possibly others) are put in the validator_flags column. If any of those three common qualifiers is in the validator_flags column, then the corresponding flags in meas_value should be set. The lab_conc_qual column is something of a relic, left over from the days when data were commonly provided in EPA’s Contract Laboratory Program (CLP) data format, which had a corresponding column. The lab_conc_qual column was meant to either contain “U” or be null. We don’t ordinarily use that column any more. Of the qualifiers you listed above, other than U, J, and R, N is commonly used to flag a tentatively identified compound, which means that the analyte code itself is uncertain. The d_labresult.tic column is meant to hold that information. The tic column is not part of the measurement_result data type because it is not used in any way during data aggregation (averaging or summing). I see that Jerry added other flags and qualifiers to the e_concqual table, but needn’t—really shouldn’t—be there. The e_concqual dictionary should have only “U” defined. It may seem odd to define a lookup table for only one value when a check constraint on the concentration qualifier columns could be used instead, but it’s easier to check relational constraints than to check check constraints programmatically.\n\n\n\nDuplicates\n\n“Duplicate” is a somewhat ambiguous term, but in practice it most commonly refers to field duplicates, which we ordinarily refer to as splits to avoid that ambiguity. Some QC data, particularly spikes, are frequently duplicated, so when we have lab QC data we may have values for spikes and spike duplicates. When we receive lab results in one big flat table that includes both analytical results for natural samples and results of lab QC samples, the word or code “DUP” in a column header or table cell could mean a couple of different things. Without seeing the original data source, I’m not sure where the “DUP” code in the “labqc_samp” column of your “d_labsample” table came from. I’m going to assume that it refers to a field duplicate, and not a spike duplicate.\n\n\nIdeally, samples are submitted to the laboratory “blind” so that the laboratory does not know which field samples are duplicates of one another. This is to prevent them from seeing that there’s a lot of variation between some pair of duplicates and deciding to re-run one or both of them. If the lab is producing highly variable data, we don’t want them to be able to hide it. Unfortunately, many field sampling programs use a suffix of “-D” or “-DUP” or something like that on the sample ID, so the lab knows which samples are field duplicates. If they know, they may pass that information back in their EDD.\n\n\nAlthough field duplicates are used as a QC check on laboratory performance, they are not lab QC samples themselves. They are just normal field samples (which have been split), and don’t need to have a laboratory QC sample ID assigned to them. Thus, field duplicates should not be listed in the “d_labqcsamp” table, so that table looks fine as it appears below. The same is true for the “d_labresult” table.\n\nThere are a couple of things to be changed about the “d_labsample” table as shown below:\n\nThe values in the “labqc_samp” column should be identifiers that appear in the “d_labqcsamp” table, not codes. The codes for the lab QC type should be in the d_labqcsamp.qc_type column, and neither “Natural” nor “DUP” should be used there.\nThe “d_labsample” table should have values in the “study_id” and “sample_no” columns, or a value in the “labqc_samp” column, but not both. There are other invalid combinations of columns also. The “d_labsample” table may have any one of the following tables as a parent: d_sampsplit, d_fldqcsplit, d_labqcsamp, d_bioaccum_samp, d_samptreatsplit, or d_bioasrepsamp. The “ck_one_sample” check constraint on the table enforces this rule. Check constraints like this are not run by the upsert scripts, so a set of staged data may pass all the checks performed by the upsert script and yet the INSERT into d_labsample will fail.\n\n\n\nMeasurement Basis\n\nR tool\nOrgMassSpecR\n\n\nGeneral\n\nData for soil and sediment are almost always reported on a dry-weight basis. If there’s anything to indicate a different basis, that deserves a closer look. Almost the only legitimate reason for a different basis for soil or sediment samples is when a leaching procedure has been applied (e.g., the Toxicity Characteristic Leaching Procedure, or TCLP); in those cases the data may be reported as the concentration in the leachate, so the basis may be “Wet” or “Whole” or “Unfiltered” – anything indicating an unfractionated liquid sample.\nData for tissues should ordinarily be reported in wet weight. Organisms’ homeostasis means that they maintain a nearly constant moisture content in their tissues, whereas the same is not true of materials like soil or sediment. If tissue data are reported in dry weight, check it carefully: labs can be sloppy about that.\nWater data are where things can be complex, because often water samples are filtered or centrifuged to remove particulates, which results in the water samples have a ‘dissolved’ basis. If the particulates are analyzed, and the results are then expressed in terms of the volume of the original sample, then the data will have a ‘particulate’ basis. Unfiltered, or whole, water, should have a basis of ‘Unfilt’, ‘Whole’, or sometimes ‘Wet’. Either of the first two of these are preferred, “Wet” is better used as a counterpart to “Dry” for soil, sediment, or tissue samples.\nThere are variations in the way things have been done in different databases. You may find that the measurement basis code for whole water samples differs from one to another, as in the third bullet above.\nIDB v.8 now has the “fraction” code, which is intended to be used to distinguish dissolved, particulate, and whole fractions of a water sample. In IDB v.8, the measurement basis for water samples will almost always be ‘Whole’. Sometimes sediment or soil samples are fractionated too, e.g., by sieving, and the fraction code should be used in those cases too, so the measurement basis will always be ‘dry’ in those cases.\nThere is an implicit association between measurement bases and units. For example, if the measurement basis is “Dry”, the units should not be “mg/L” because “…/L” implies a liquid, not a solid.\nThe measurement basis refers to the form of the sample material, which is represented in the denominator of concentration units. So codes of “Sediment” “Arsenic”, “mg/kg”, “Dry” should be read as “mg of arsenic per kg of dry sediment.”\n\n\n\nWet Weight\n\nWet weight (or as-is) basis means no calculation has been made to compensate for the moisture content of a sample. Wet weight refers to the weight of animal tissue or other substance including its contained water. (See also “Dry weight”)\n\n\n\nDry Weight\n\nDry weight basis means the lab has measured moisture content of a sample and calculated concentrations based on the percent solids present. Dry weight refers to the weight of animal tissue after it has been dried in an oven at 65°C until a constant weight is achieved. Dry weight represents total organic and inorganic matter in the tissue. (See also “Wet weight”).\n\n\n\nLipid\n\nLipid is any one of a family of compounds that are insoluble in water and that make up one of the principal components of living cells. Lipids include fats, oils, waxes, and steroids. Many environmental contaminants such as organochlorine pesticides are lipophilic.\n\n\n\n\n\nConversions\n\nWet to Dry\n\\[DryWt = \\frac{WetWt}{Percent Solids} * 100\\]\n\n\nDry to Wet\n\\[WetWt = DryWt * \\frac{PercentSolids}{100}\\]\n\n\nOrganic Carbon Normalization\n\\[OCnorm = \\frac{DryWt}{\\frac{PercentTOC}{100}}\\]\n\nDryWt & WetWt = concentration PercentSolids = percentage (no decimal)\n\n\n\nResource\n\n\n\n\nCalcs\n\n\n\n\n\n\n\nAnalytical Blanks\n\n\nTrip Blank\nThe trip blank is designed to identify levels of contamination from the exposure of the reagent or sorbent bed to the same atmospheres exposed to the analyte reagent or sorbent bed. The trip blank is prepared in the laboratory with the other reagents or adsorbents prior to shipping to the field. However, the trip blank is never exposed to the field atmospheres. It is simply sent along with the field samples to and from the site. The trip blank identified areas of exposure such as shipping temperatures and pressures, laboratory preparation of field samples and laboratory preparation of field samples for analysis.\n\nField Blank\nThe field blank is similar to the trip blank in that it is also prepared during the preparation of the field reagents or adsorbents. However, the field blank is exposed to the same atmospheres in the field as the field samples. This means that the field blank is opened during the charging of impingers or sorbents in the sample train. The field blank is also exposed during the exchanging of cartridges in SW-846, Method 0030 or when field reagents are being exchanged during a test run. In summary, field blanks consist of additional sample collection media (e.g., sorbent tubes, reagents, filters) which are transported to the monitoring site, exposed briefly at the site when the samples are exposed (but no stack gas is actually pulled through these blanks), and transported back to the laboratory for analysis, similar to a field sample. At least one field blank should be collected and analyzed for each test series.\n\n\nLaboratory Blank\nThe laboratory blank is a sample of the reagents or sorbents used during the sample train reagent preparation or recovery. The laboratory blank is a sample of the extraction solvent, the rinses used during sample recovery, or a sample from the batch of sorbent used to preparing sampling cartridges. Laboratory blanks include both method blanks and instrument blanks. Method blanks are carried through all steps of the measurement process (from extraction through analysis). A method blank is typically analyzed with each sample batch. Instrument blanks are used to demonstrate that an instrument system is free of contamination. Instrument blanks are typically analyzed prior to sample analysis and following the analysis of highly contaminated samples.\n\n\nReagent Blank\nThe reagent blank is a sample of the solvents used during recovery of the sample train after the test is completed. You recall, reagent blanks for both multi-metal and chromium +6 require that the reagent blank be the same volume as the renses used to recover the samples, from probe to impinger. This is because the blank value is substracted from the sample to obtain a final concentration.\n\n\nDiagram\n\n\n\n\n\nDetection Limits\nPresentation\n\nWhat affects detection limits?\n\nSample size\nConcentration of other constituents\nSample clean-up\nMethodology\nLab Performance\n\nExperience\nExtraction technique\nInstrument type and maintenance\n\n\ndetection_limit - the lowest possible value an instrument/method can sense a compound is present (think of it like a whisper - you can barely hear it, but know its there). This is better known as the “method detection limit”\nquantification_limit - the limit in which an instrument/method can actually start to quantify the amount of something which is present. If the result is between the detection_limit and the quantification_limit, the result is estimated, because the instrument/method cant confidently identify the amount of something until it reaches the quantification_limit.\nreporting_limit - usually project or dataset specific. this limit is used for data analysis/statistics. the reporting_limit is equal to either the detection_limit or quantification_limit. This is better known as the “reporting detection limit”.\n\n\nMethod Detection Limit (MDL)\n\nStatistically determined\nThe minimum concentration that can be measured with 99% confidence that the concentration is greater than zero\nConcentrations near MDL are estimates\nLaboratory, instrument, matrix, method, and analyte specific\nConcentrations at MDL expected to be a false positive 1% of the time, but false negatives 50% of the time\n\n\n\nMethod Reporting Limit (MRL)\n\nMay also be referred to as QL (quantitation limit), sample quantitation limit, or just RL (reporting limit)​\nDetermined by the lowest point of the calibration​\nNot as specific as MDL, labs can adust​\nConcentrations at MRL can be reliably quantified​\nMRL &gt; MDL​\nAlso laboratory, instrument, matrix, method, & analyte specific\n\n\n\nMDL & MRL Relationship\n\n\n\nOther Detection Limits\n\nPQL\n\nConsidered to be lowest concentration that can be reliably quantified by a method\nLimit of Detection (LOD); Lowest concentration that can be detected with a 1% false negative rate.\n\nGenerally 2x to 3X MDL\n\nLimit of Quantitation (LOQ); similar to MRL\n\n\n\nPCDD/F & PCB specific\n2.5 times signal to noise\n\nEQL: Estimated Quantitation Limit\nEDL: Estimated Detection Limit\nSDL: Sample Detection Limit\nEMPC\n\nEstimated Maximum Possible Concentration (EMPC)\nPeak present but not all of the identification criteria is met\nAlways greater than MDL, may be greater than MRL\nGenerally treated a non-detect in TEQ calculations\nEMPCs can present data management difficulties and need to be reviewed in QC checks"
  },
  {
    "objectID": "docs/development/tools.html#movie-gif",
    "href": "docs/development/tools.html#movie-gif",
    "title": "Tools",
    "section": "Movie > Gif",
    "text": "Movie &gt; Gif\nffmpeg -i file.mov -s 600x400 -pix_fmt rgb24 -r 20 -f gif - | gifsicle --optimize=3 --delay=3 &gt; file.gif"
  },
  {
    "objectID": "docs/development/tools.html#resources",
    "href": "docs/development/tools.html#resources",
    "title": "Tools",
    "section": "Resources",
    "text": "Resources\n\nCheatSheet\nConvert Docker command to docker-compose.yml"
  },
  {
    "objectID": "docs/development/tools.html#image-vs.-container",
    "href": "docs/development/tools.html#image-vs.-container",
    "title": "Tools",
    "section": "Image vs. Container",
    "text": "Image vs. Container\nImage - Application we want to run\nContainer - Instance of that image running as a process"
  },
  {
    "objectID": "docs/development/tools.html#docker-basics",
    "href": "docs/development/tools.html#docker-basics",
    "title": "Tools",
    "section": "Docker Basics",
    "text": "Docker Basics\n\nCreate an Nginx container\ndocker run -p 80:80 -d --name webhost nginx\n\nDownloads Nginx from Docker Hub\nStarts new container from that image\nOpened port 80 on host IP\nRoutes port 80 traffic to the container IP, port 80\nView container at http://localhost:80\n\n\n\nOther examples\ndocker run -p 80:80 -d --name nginx nginx\ndocker run -p 8080:80 -d --name httpd httpd\ndocker run -p 3306:3306 --platform linux/amd64 -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql\nCreate a JupyterLab instance and attach your current directory as a volume: docker run -it --rm -p 8888:8888 -v $(PWD):/home/jovyan jupyter/pyspark-notebook\n\n\nProcesses and configurations\nCheck processes running inside a container: docker top &lt;container&gt;\nContainer configuration: docker &lt;container&gt; inspect\nCheck container stats (memory, cpu, network): docker stats &lt;container&gt;\n\n\nGetting a shell inside containers\nStart a new container interactively: docker run -it &lt;container&gt;\nRun commands in existing container: docker exec -it &lt;container&gt;\n\nExample: Start a container interactively and launch bash within it\n\nStart container and launch bash: docker run -it --name ubuntu ubuntu bash\nRun some bash command: apt-get install -y curl\nExit the container: exit\nStart and re-enter the container: docker start -ai ubuntu\n\n\n\nExample: Launch shell in running container\ndocker exec -it &lt;container&gt; bash\n\n\n\nPull an image from docker hub\ndocker pull &lt;imagename&gt;"
  },
  {
    "objectID": "docs/development/tools.html#docker-networks",
    "href": "docs/development/tools.html#docker-networks",
    "title": "Tools",
    "section": "Docker Networks",
    "text": "Docker Networks\n\nEach container is connected to a private virtual network (called “bridge”).\nEach virtual network routes through NAT firewall on host IP.\nAll containers on a virtual network can talk to each other without -p\nBest practice: Create a new virtual network for each app.\nYou can skip virtual networks and use the host IP (--net=host).\n\nGet container IP: docker inspect --format '{{ .NetworkSettings.IPAddress }}' &lt;container&gt;\n\nPublishing (#:#)\nexample: 8080:80\nleft number: published/host port\nright number: listening/container port\nTraffic passing through port 8080 on the HOST will be directed to port 80 on the container.\n\n\nDNS\nDocker uses container names as host names.\nDont rely on IPs for inter-communication.\nBest Practice Always use custom networks.\n\nAssignment\nCheck different curl versions within current versions of Ubuntu and CentOS.\nRun “curl –version” on both operating systems.\n\nSteps\nubuntu: apt-get update && apt-get install curl\ncentos: yum update curl\nThen…\ncurl --version\nAlso:\nCheck out command docker --rm"
  },
  {
    "objectID": "docs/development/tools.html#dockerfiles",
    "href": "docs/development/tools.html#dockerfiles",
    "title": "Tools",
    "section": "Dockerfiles",
    "text": "Dockerfiles\nRecipe for creating images\nEach Dockerfile stanza such as “RUN”, “CMD”, etc. are stored as a single image layer. Docker caches each layer by giving it a unique SHA (hash), so whenever the image is (re)built, it can check to see if a layer has changed, and if not, it will use the cached layer.\nDocker builds images top down, so it is best practice to structure the Dockerfile in such a way that lines which will change the most are at the bottom, and lines that will change the least are at the top. If a line is changed (ie. source code changes) Docker will rebuild that line, and thus each line after that will also need to be rebuilt."
  },
  {
    "objectID": "docs/development/tools.html#keeping-the-docker-system-clean",
    "href": "docs/development/tools.html#keeping-the-docker-system-clean",
    "title": "Tools",
    "section": "Keeping the Docker system clean",
    "text": "Keeping the Docker system clean\ndocker system prune - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache"
  },
  {
    "objectID": "docs/development/tools.html#volumes-an-bind-mounts",
    "href": "docs/development/tools.html#volumes-an-bind-mounts",
    "title": "Tools",
    "section": "Volumes an Bind Mounts",
    "text": "Volumes an Bind Mounts\nVolumes - Special location outside of container UFS\nBind Mounts - Link container path to host path\nBuild an image and named volume (persistent): docker run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql:/var/lib/mysql --platform linux/amd64 mysql"
  },
  {
    "objectID": "docs/development/tools.html#initialize",
    "href": "docs/development/tools.html#initialize",
    "title": "Tools",
    "section": "Initialize",
    "text": "Initialize\n\nLaunch Git Bash\nNavigate to project directory\ninitialize git repository in the folder root: git init\ncreate new file in directory: touch filename.extension\nlist files in root: ls\ncheck which files git recognizes: git status"
  },
  {
    "objectID": "docs/development/tools.html#staging",
    "href": "docs/development/tools.html#staging",
    "title": "Tools",
    "section": "Staging",
    "text": "Staging\nA commit is a record of what files you have changed since the last time you made a commit. Essentially, you make changes to your repo (for example, adding a file or modifying one) and then tell git to put those files into a commit. Commits make up the essence of your project and allow you to go back to the state of a project at any point.\nSo, how do you tell git which files to put into a commit? This is where the staging environment or index come in. When you make changes to your repo, git notices that a file has changed but won’t do anything with it (like adding it in a commit).\nTo add a file to a commit, you first need to add it to the staging environment. To do this, you can use the git add &lt;filename&gt; command.\nOnce you’ve used the git add command to add all the files you want to the staging environment, you can then tell git to package them into a commit using the git commit command. Note: The staging environment, also called ‘staging’, is the new preferred term for this, but you can also see it referred to as the ‘index’.\n\nAdd files to the staging environment: git add filename.extension\nCheck staging environment for new files: git status"
  },
  {
    "objectID": "docs/development/tools.html#commit-locally",
    "href": "docs/development/tools.html#commit-locally",
    "title": "Tools",
    "section": "Commit Locally",
    "text": "Commit Locally\ngit commit -m \"Your message about the commit\""
  },
  {
    "objectID": "docs/development/tools.html#branches",
    "href": "docs/development/tools.html#branches",
    "title": "Tools",
    "section": "Branches",
    "text": "Branches\nSay you want to make a new feature but are worried about making changes to the main project while developing the feature. This is where git branches come in.\nBranches allow you to move back and forth between ‘states’ of a project. For instance, if you want to add a new page to your website you can create a new branch just for that page without affecting the main part of the project. Once you’re done with the page, you can merge your changes from your branch into the master branch. When you create a new branch, Git keeps track of which commit your branch ‘branched’ off of, so it knows the history behind all the files.\n\ngit checkout -b &lt;my branch name&gt;\nShow list of branches: git branch"
  },
  {
    "objectID": "docs/development/tools.html#commit-to-github",
    "href": "docs/development/tools.html#commit-to-github",
    "title": "Tools",
    "section": "Commit to Github",
    "text": "Commit to Github\n\nCreate new repo on GitHub\ngit remote add origin &lt;url produced on github for new repo&gt;\ngit push -u origin [master/main]"
  },
  {
    "objectID": "docs/development/tools.html#push-a-branch-to-github",
    "href": "docs/development/tools.html#push-a-branch-to-github",
    "title": "Tools",
    "section": "Push a Branch to Github",
    "text": "Push a Branch to Github\ngit push origin &lt;my-new-branch&gt;\nYou might be wondering what that “origin” word means in the command above. What happens is that when you clone a remote repository to your local machine, git creates an alias for you. In nearly all cases this alias is called “origin.” It’s essentially shorthand for the remote repository’s URL. So, to push your changes to the remote repository, you could’ve used either the command: git push git@github.com:git/git.git yourbranchname or git push origin yourbranchname"
  },
  {
    "objectID": "docs/development/tools.html#pull-request",
    "href": "docs/development/tools.html#pull-request",
    "title": "Tools",
    "section": "Pull Request",
    "text": "Pull Request\nA pull request (or PR) is a way to alert a repo’s owners that you want to make some changes to their code. It allows them to review the code and make sure it looks good before putting your changes on the master branch."
  },
  {
    "objectID": "docs/development/tools.html#get-changes-on-github",
    "href": "docs/development/tools.html#get-changes-on-github",
    "title": "Tools",
    "section": "Get Changes on Github",
    "text": "Get Changes on Github\ngit pull origin master\ncheck all new commits: git log"
  },
  {
    "objectID": "docs/development/tools.html#view-differences",
    "href": "docs/development/tools.html#view-differences",
    "title": "Tools",
    "section": "View Differences",
    "text": "View Differences\n\nrun: git diff"
  },
  {
    "objectID": "docs/development/tools.html#remove-a-branch",
    "href": "docs/development/tools.html#remove-a-branch",
    "title": "Tools",
    "section": "Remove a Branch",
    "text": "Remove a Branch\n\nLocally\ngit branch -d &lt;branch_name&gt;\n\n\nRemote\ngit push &lt;remote_name&gt; --delete &lt;branch_name&gt;"
  },
  {
    "objectID": "docs/development/tools.html#remove-tracked-filedirectory",
    "href": "docs/development/tools.html#remove-tracked-filedirectory",
    "title": "Tools",
    "section": "Remove tracked file/directory",
    "text": "Remove tracked file/directory\n\nFile\ngit rm --cached &lt;file&gt;\n\n\nDirectory\ngit rm --cahced -r dir/"
  },
  {
    "objectID": "docs/development/tools.html#pre-commit",
    "href": "docs/development/tools.html#pre-commit",
    "title": "Tools",
    "section": "pre-commit",
    "text": "pre-commit\nPlease make sure to install our pre-commit hooks into your Git workflow. Pre-commit will help keep our code clean and make sure we are following best practices.\n\nInstall pre-commit hooks: python -m pre_commit install --install-hooks\nRun hooks on the entire codebase: python -m pre_commit run --all-files\n\nHooks will run on the current commit snapshot when executing a git commit. Pre-commit hooks allow us to check for potential issues and make sure we are applying standards to our code before pushing to GitHub.\nSee an example .pre-commit-config.yml"
  },
  {
    "objectID": "docs/development/tools.html#merge",
    "href": "docs/development/tools.html#merge",
    "title": "Tools",
    "section": "Merge",
    "text": "Merge\nThe steps below can be used to merge two branches on your local machine. The braches used in this example are:\n\nmain: The authoritative or “production” code lives in this branch.\ndev: This branch is split from the main branch and a new feature or update is coded with the intent to merge changes back into the main branch.\n\n\nPull main and dev branches so local repo is up to date with the remote.\n\ngit checkout main\ngit pull origin main\ngit checkout dev\ngit pull origin dev\n\nCheckout the main branch so we can merge the dev branch into main\n\ngit checkout main\ngit merge dev\n\n\nCheck the branch status: git status\n\nEvaluate the two files with a conflict (ie. .gitignore and requirements.txt) and reconcile issues, then git add when ready.\nCommit the changes: git commit -m \"merge @tnelson-integral dev branch with main\"\nPush changes to the remote on GitHub: git push origin main\nCheck out the dev branch locally and pull the main branch changes into it so dev can be up-to-date with main\n\ngit checkout dev\ngit pull origin main\ngit push origin dev"
  },
  {
    "objectID": "docs/development/tools.html#export-table-to-csv",
    "href": "docs/development/tools.html#export-table-to-csv",
    "title": "Tools",
    "section": "Export table to CSV",
    "text": "Export table to CSV\n\n\\copy table TO '&lt;path&gt;' CSV\n\\copy table(col1,col1) TO '&lt;path&gt;' CSV\n\\copy (SELECT...) TO '&lt;path&gt;' CSV"
  },
  {
    "objectID": "docs/development/tools.html#backup",
    "href": "docs/development/tools.html#backup",
    "title": "Tools",
    "section": "Backup",
    "text": "Backup\nUse pg_dumpall to backup all databases\n$ pg_dumpall -U postgres &gt; all.sql\nUse pg_dump to backup a database\n$ pg_dump -d mydb -f mydb_backup.sql\n\n  -a   Dump only the data, not the schema\n  -s   Dump only the schema, no data\n  -c   Drop database before recreating\n  -C   Create database before restoring\n  -t   Dump the named table(s) only\n  -F   Format (c: custom, d: directory, t: tar)\n\nUse pg_dump -? to get the full list of options"
  },
  {
    "objectID": "docs/development/tools.html#restore",
    "href": "docs/development/tools.html#restore",
    "title": "Tools",
    "section": "Restore",
    "text": "Restore\n\npsql\n$ psql -U user mydb &lt; mydb_backup.sql\n\n\npg_restore\n$ pg_restore -d mydb mydb_backup.sql -c\n\n-U   Specify a database user\n-c   Drop database before recreating\n-C   Create database before restoring\n-e   Exit if an error has encountered\n-F   Format (c: custom, d: directory, t: tar, p: plain text sql(default))\n\nUse pg_restore -? to get the full list of options"
  },
  {
    "objectID": "docs/development/server/python-web-app.html",
    "href": "docs/development/server/python-web-app.html",
    "title": "Python Web App",
    "section": "",
    "text": "Clone repository:\nsudo mkdir /etc/local/webs && \\\ncd /etc/local/webs && \\\nsudo git clone https://github.com/app.git && \\\nsudo chown -R &lt;user&gt;:&lt;user&gt; app && \\\ncd app\nCreate python virtual environment & install dependencies:\npython -m venv .venv && \\\nsource .venv/bin/activate && \\\npython -m pip install -r requirements.txt\nConfigure WSGI - /usr/local/webs/app/app.wsgi:\n#!/usr/bin/python\nimport sys\nimport logging\nimport os\n\nAPP_DIR = \"/usr/local/webs/app\"\nos.environ[\"APP_DIR\"] = APP_DIR\n\nlogging.basicConfig(stream=sys.stderr)\nsys.path.insert(0, APP_DIR)\n\nfrom app import app as application\nUpdate owner:group\ncd /usr/local/web && \\\nsudo chown -R www-data:www-data cmat"
  },
  {
    "objectID": "docs/development/server/python-web-app.html#flask-app-configuration",
    "href": "docs/development/server/python-web-app.html#flask-app-configuration",
    "title": "Python Web App",
    "section": "",
    "text": "Clone repository:\nsudo mkdir /etc/local/webs && \\\ncd /etc/local/webs && \\\nsudo git clone https://github.com/app.git && \\\nsudo chown -R &lt;user&gt;:&lt;user&gt; app && \\\ncd app\nCreate python virtual environment & install dependencies:\npython -m venv .venv && \\\nsource .venv/bin/activate && \\\npython -m pip install -r requirements.txt\nConfigure WSGI - /usr/local/webs/app/app.wsgi:\n#!/usr/bin/python\nimport sys\nimport logging\nimport os\n\nAPP_DIR = \"/usr/local/webs/app\"\nos.environ[\"APP_DIR\"] = APP_DIR\n\nlogging.basicConfig(stream=sys.stderr)\nsys.path.insert(0, APP_DIR)\n\nfrom app import app as application\nUpdate owner:group\ncd /usr/local/web && \\\nsudo chown -R www-data:www-data cmat"
  },
  {
    "objectID": "docs/development/server/python-web-app.html#apache-configuration",
    "href": "docs/development/server/python-web-app.html#apache-configuration",
    "title": "Python Web App",
    "section": "Apache Configuration",
    "text": "Apache Configuration\n\nCreate custom Apache log directory:\nsudo mkdir /usr/local/webs/apache-logs && \\\nsudo touch /usr/local/webs/apache-logs/app/error.log && \\\nsudo touch /usr/local/webs/apache-logs/app/access.log && \\\nsudo chown -R www-data:www-data /usr/local/webs/apache-logs\nCreate configuration file:\ncd /etc/apache2/sites-available && \\\nsudo vi app.conf\nConfiguration - app.conf:\n&lt;VirtualHost *:80&gt;\n    ServerName {DNS}\n\n    ServerSignature Off\n\n    RewriteEngine On\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n&lt;/VirtualHost&gt;\n\n&lt;VirtualHost *:443&gt;\n    ServerAdmin webmaster@localhost\n    ServerName {DNS}\n\n    DocumentRoot /usr/local/webs/app\n\n    WSGIDaemonProcess web-app threads=5 python-home=/usr/local/webs/app/.venv\n    WSGIProcessGroup web-app\n    WSGIScriptAlias / /usr/local/webs/app/app.wsgi\n    WSGIPassAuthorization On\n    &lt;Directory /usr/local/webs/app&gt;\n            Order allow,deny\n            Allow from all\n    &lt;/Directory&gt;\n\n    &lt;Location /&gt;\n            Require all granted\n    &lt;/Location&gt;\n\n    ErrorLog /usr/local/webs/apache-logs/app/error.log\n    CustomLog /usr/local/webs/apache-logs/app/access.log combined\n\n    SSLEngine on\n    SSLCertificateFile /etc/letsencrypt/live/{DNS}/fullchain.pem\n    SSLCertificateKeyFile /etc/letsencrypt/live/{DNS}/privkey.pem\n    Include /etc/letsencrypt/options-ssl-apache.conf\n&lt;/VirtualHost&gt;\nTest configuration:\nsudo apache2ctl configtest\nEnable the site\nsudo a2ensite app.conf\nRestart Apache service\nsudo systemctl restart apache2"
  },
  {
    "objectID": "docs/development/server/index.html",
    "href": "docs/development/server/index.html",
    "title": "Server",
    "section": "",
    "text": "Initial Linux server setup steps:\n\nLogin to server as root: ssh root@&lt;ip&gt;\nCreate new admin user: sudo adduser &lt;user&gt;\nAdd user to sudo group: sudo usermod -aG sudo &lt;user&gt;\nDisable root login\n\nOpen SSH configuration: sudo vi /etc/ssh/sshd_config\nSet PermitRootLogin to no\n\nConfigure Uncomplicated Firewall (UFW)\n\nAllow SSH connections: sudo ufw allow OpenSSH\nOpen port 80 and 443 for Apache: sudo ufw allow 'Apache Full'\nTurn on firewall: sudo ufw enable\nCheck: sudo ufw status verbose\n\nReboot the server: sudo reboot\nLogin as new user: ssh &lt;user&gt;@&lt;ip&gt;\nCheck for package updates apt list --upgradable 1, Upgrade packages sudo apt upgrade\nShow system info: uname -a\nEven though the kernel was updated, need to reboot to take effect: sudo reboot\nJust to be sure: sudo apt update && sudo apt upgrade\nShutdown for backup/snapshot sudo shutdown -h now\nAfter snapshot, navigate to provider website and restart machine"
  },
  {
    "objectID": "docs/development/server/index.html#server-initialization",
    "href": "docs/development/server/index.html#server-initialization",
    "title": "Server",
    "section": "",
    "text": "Initial Linux server setup steps:\n\nLogin to server as root: ssh root@&lt;ip&gt;\nCreate new admin user: sudo adduser &lt;user&gt;\nAdd user to sudo group: sudo usermod -aG sudo &lt;user&gt;\nDisable root login\n\nOpen SSH configuration: sudo vi /etc/ssh/sshd_config\nSet PermitRootLogin to no\n\nConfigure Uncomplicated Firewall (UFW)\n\nAllow SSH connections: sudo ufw allow OpenSSH\nOpen port 80 and 443 for Apache: sudo ufw allow 'Apache Full'\nTurn on firewall: sudo ufw enable\nCheck: sudo ufw status verbose\n\nReboot the server: sudo reboot\nLogin as new user: ssh &lt;user&gt;@&lt;ip&gt;\nCheck for package updates apt list --upgradable 1, Upgrade packages sudo apt upgrade\nShow system info: uname -a\nEven though the kernel was updated, need to reboot to take effect: sudo reboot\nJust to be sure: sudo apt update && sudo apt upgrade\nShutdown for backup/snapshot sudo shutdown -h now\nAfter snapshot, navigate to provider website and restart machine"
  },
  {
    "objectID": "docs/development/server/index.html#permissions",
    "href": "docs/development/server/index.html#permissions",
    "title": "Server",
    "section": "Permissions",
    "text": "Permissions\n\nSyntax\nGeneral: _rwxrwxrwx 1 owner group\n_ | rwx | rwx | rwx = Special | Owner | Group | All Users\n\nShorthand\n\n4 | r = Read\n2 | w = Write\n1 | x = Execute\n\n\n\nDetailed\n\n0 = ---\n1 = --x\n2 = -w-\n3 = -wx\n4 = r-\n5 = r-x\n6 = rw-\n7 = rwx\n\n\n\n\nCommands\nchgrp = Change group\nExample: sudo chgrp -R &lt;group&gt; &lt;folder&gt;\nchown = Change ownership\nExample: sudo chown -R &lt;user&gt;:&lt;group&gt; &lt;file/folder&gt;\nchmod = Change permissions\nExample: sudo chmod -R 774 &lt;file/folder&gt;\nMake new files inherit the group: sudo chmod g+s &lt;folder&gt;\n\n\nExample\nCreate a shared directory for a group.\n\nCreate a shared directory for users to access: /share\nAssign users to a common group (staff): sudo usermod -a -G staff &lt;user&gt;\nVerify user groups: groups &lt;user&gt;\nCreate shared directory and assign permissions:\nsudo mkdir /share && \\\nsudo chgrp -R staff /share && \\  # assign group\nsudo chmod -R g+w /share && \\  # permissions\nsudo chmod -R +s /share  # inherit permissions for newly created files/folders"
  },
  {
    "objectID": "docs/development/index.html",
    "href": "docs/development/index.html",
    "title": "Development",
    "section": "",
    "text": "Note\n\n\n\nTBD\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/development/code/rust.html",
    "href": "docs/development/code/rust.html",
    "title": "Rust",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "docs/development/code/r.html",
    "href": "docs/development/code/r.html",
    "title": "R",
    "section": "",
    "text": "Code\nif (!require(librarian)){\n  remotes::install_github(\"DesiQuintans/librarian\")\n  library(librarian)\n}\nshelf(\n  # database\n  DBI, RPostgres,\n  # spatial\n  ggmap, leaflet, ggplot2, plotly,\n  r-spatial/mapview,\n  sf, sp,\n  lwgeom,\n  # tidyverse\n  dplyr, purrr, readr, tibble, tidyr,\n  reticulate,\n  # todo: use these\n  # googledrive,\n  # report\n  DT, gt, htmltools, htmlwidgets, kableExtra, knitr, \n  markdown, shiny, webshot,\n  # utility\n  fs, glue, here, png, scales, stringr, urltools)\n\nhere &lt;- here::here"
  },
  {
    "objectID": "docs/development/code/r.html#r",
    "href": "docs/development/code/r.html#r",
    "title": "R",
    "section": "R",
    "text": "R\n\n\nCode\nx &lt;- 5 * 5\nx\n\n\n[1] 25"
  },
  {
    "objectID": "docs/development/code/r.html#sql",
    "href": "docs/development/code/r.html#sql",
    "title": "R",
    "section": "SQL",
    "text": "SQL\nThere are a couple ways to connect to a database instance. These examples show how to connect to a local PostgreSQL instance, and how to use the connection information to query a database.\n\nOption 1\nCreate connection chunk, then call connection in each subsequent chunk. ```{.r connSQL, class.source=‘fold-show’} library(DBI)\npass &lt;- readLines(“../../pwd.txt”) db = dbConnect( RPostgres::Postgres(), dbname = “personal”, host = “localhost”, port = 5432, user = “cgrant”, password = pass )\n\n```{.sql, connection=db, eval=F}\nselect * from workouts limit 10;\n\n\nOption 2\nSet default connection in the setup chunk (pretend its the setup chunk). ```{.r setup-v2, eval=F, class.source=‘fold-show’} library(DBI)\ndb = dbConnect( RPostgres::Postgres(), dbname = “personal”, host = “localhost”, port = 5432, user = “cgrant”, password = pass ) knitr::opts_chunk$set(connection = “db”)\n\nNow you dont have to specify the connection for each chunk.\n\n```{.sql, eval=F}\nselect * from workouts limit 10;"
  },
  {
    "objectID": "docs/development/code/r.html#python",
    "href": "docs/development/code/r.html#python",
    "title": "R",
    "section": "Python",
    "text": "Python\nTo use a Python engine, you need to call library(reticulate) link ```{.r, class.source=‘fold-show’} # install.packages(“reticulate”)\nSys.setenv(RETICULATE_PYTHON = “/Users/cgrant/venvs/dev/bin/python”) library(reticulate) py_config()\n\nNow you can run Python code\n\n::: {.cell}\n\n```{.python .fold-show .cell-code}\nx = 4 * 4\nprint(x)\n\n16\n\n:::\nYou can import libraries as normal\n\n\nCode\nimport pandas as pd\n\nvals = {\"col1\": [\"a\", \"b\", \"c\", \"d\"], \"col2\": [1, 10, 100, 1000]}\ndf = pd.DataFrame(vals)\nprint(df)\n\n\n  col1  col2\n0    a     1\n1    b    10\n2    c   100\n3    d  1000\n\n\nAccess objects created within Python chunks from R using py$&lt;var&gt;\n\n\nCode\nlibrary(DT)\ndatatable(py$df)\n\n\nError in crosstalk::is.SharedData(data): object 'py' not found"
  },
  {
    "objectID": "docs/development/code/r.html#leaflet",
    "href": "docs/development/code/r.html#leaflet",
    "title": "R",
    "section": "Leaflet",
    "text": "Leaflet\nUse the leaflet map below to explore.\n\n\nCode\nlibrary(leaflet)\nlibrary(dplyr)\n\nleaflet() %&gt;% \n  setView(lng=-122.90486, lat=47.03576, zoom=16) %&gt;% \n  addTiles() %&gt;% \n  addMarkers(lng=-122.90486, lat=47.03576, popup=\"WA State Capitol\")"
  },
  {
    "objectID": "docs/development/code/r.html#temperature",
    "href": "docs/development/code/r.html#temperature",
    "title": "R",
    "section": "Temperature",
    "text": "Temperature\nReference Source\n\n\nCode\nlibrary(leaflet)\nlibrary(dplyr)\n\nf &lt;- \"./static/NOAA-SeaTac_cleaned.csv\"\ndt &lt;- read.csv(f)\n\n\nError in file(file, \"rt\"): cannot open the connection\n\n\nCode\ndf &lt;- as.data.frame(dt)\n\n\nError in as.data.frame.default(dt): cannot coerce class '\"function\"' to a data.frame\n\n\nCode\nupdatemenus &lt;- list(\n  list(\n    active = 0,\n    x = -.125,\n    type= 'buttons',\n    buttons = list(\n      list(\n        label = \"Wet Bulb\",\n        method = \"update\",\n        args = list(list(visible = c(TRUE, \"legendonly\")))),\n      list(\n        label = \"Dry Bulb\",\n        method = \"update\",\n        args = list(list(visible = c(\"legendonly\", TRUE))))\n    )\n  )\n)\n\nplt &lt;- plot_ly(data = df) %&gt;%\n  add_markers(x=as.Date(df$Timestamp), y=df$HourlyWetBulbTemperature, name=\"Wet Bulb\") %&gt;%\n  add_markers(x=as.Date(df$Timestamp), y=df$HourlyDryBulbTemperature, name=\"Dry Bulb\", visible=\"legendonly\") %&gt;%\n  layout(title = \"SeaTac 2020 Temperature Data\", \n         showlegend=FALSE,\n         xaxis=list(zeroline = FALSE,title=\"Date\"),\n         yaxis=list(zeroline = FALSE,title=\"Temperature (F)\"),\n         updatemenus=updatemenus)\n\n\nError in layout(., title = \"SeaTac 2020 Temperature Data\", showlegend = FALSE, : unused arguments (title = \"SeaTac 2020 Temperature Data\", showlegend = FALSE, xaxis = list(zeroline = FALSE, title = \"Date\"), yaxis = list(zeroline = FALSE, title = \"Temperature (F)\"), updatemenus = updatemenus)\n\n\nCode\nplt\n\n\nError in eval(expr, envir, enclos): object 'plt' not found"
  },
  {
    "objectID": "docs/development/code/powershell.html",
    "href": "docs/development/code/powershell.html",
    "title": "PowerShell",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "docs/development/code/julia.html",
    "href": "docs/development/code/julia.html",
    "title": "Julia",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "docs/development/code/index.html",
    "href": "docs/development/code/index.html",
    "title": "Code",
    "section": "",
    "text": "Bourne Again SHell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand Prompt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperText Markup Language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJavaScript programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia Programming Language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLightweight markup language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft PowerShell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular expressions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRust programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructured Query Language\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/development/code/index.html#code-pages",
    "href": "docs/development/code/index.html#code-pages",
    "title": "Code",
    "section": "",
    "text": "Bourne Again SHell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand Prompt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperText Markup Language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJavaScript programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJulia Programming Language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLightweight markup language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMicrosoft PowerShell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular expressions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRust programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructured Query Language\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "docs/development/code/cmd.html",
    "href": "docs/development/code/cmd.html",
    "title": "CMD",
    "section": "",
    "text": "@echo off\ncls\n\n@REM Set the path to the script root (using UNC path). Note the string is not quoted.\nset SCRIPT_DIR=\\\\integral-corp.com\\data\\&lt;Project Number Range&gt;\\&lt;Project Folder&gt;\\Working_Files\\DataManagement\\IDB\\export\\script\n\n@REM Name of the script to execute. Note the string is not quoted.\nset SCRIPT_NAME=export.sql\n\n@REM Prompt the user for their Postgres password\n@REM This allows for users to run the execsql script who might not have\n@REM a custom execsql.conf file specified with their username.\nset /p \"DB_USER=Enter you PostgreSQL username: \"\n\n@REM Run the execsql script\nPowerShell -NoProfile -ExecutionPolicy Bypass -Command \"& Set-Location -Path %SCRIPT_DIR%; M:\\DataManagement\\bin\\execsql.py -u %DB_USER% %SCRIPT_NAME%\"\n\npause"
  },
  {
    "objectID": "docs/development/code/cmd.html#run-an-execsql-script",
    "href": "docs/development/code/cmd.html#run-an-execsql-script",
    "title": "CMD",
    "section": "",
    "text": "@echo off\ncls\n\n@REM Set the path to the script root (using UNC path). Note the string is not quoted.\nset SCRIPT_DIR=\\\\integral-corp.com\\data\\&lt;Project Number Range&gt;\\&lt;Project Folder&gt;\\Working_Files\\DataManagement\\IDB\\export\\script\n\n@REM Name of the script to execute. Note the string is not quoted.\nset SCRIPT_NAME=export.sql\n\n@REM Prompt the user for their Postgres password\n@REM This allows for users to run the execsql script who might not have\n@REM a custom execsql.conf file specified with their username.\nset /p \"DB_USER=Enter you PostgreSQL username: \"\n\n@REM Run the execsql script\nPowerShell -NoProfile -ExecutionPolicy Bypass -Command \"& Set-Location -Path %SCRIPT_DIR%; M:\\DataManagement\\bin\\execsql.py -u %DB_USER% %SCRIPT_NAME%\"\n\npause"
  },
  {
    "objectID": "docs/blog/posts/creating-a-post/index.html",
    "href": "docs/blog/posts/creating-a-post/index.html",
    "title": "Creating a Blog Post",
    "section": "",
    "text": "To add a post, create a copy of docs/blog/posts/_template, modify the folder name with the title of the post, then update the index.qmd with your content. New posts under docs/blog/posts will show up in the blog listings when the site is rendered.\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/blog/index.html",
    "href": "docs/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\nAuthor\n\n\nReading Time\n\n\nCategories\n\n\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nCreating a Blog Post\n\n\nThis is an example of how to create a new blog post.\n\n\nCaleb Grant\n\n\n1 min\n\n\nQuarto,Blog,Example\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nAccessing Quarto Variables\n\n\nAccess quarto variables in a document.\n\n\nCaleb Grant\n\n\n1 min\n\n\nQuarto,Variables\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "docs/blog/posts/accessing-quarto-variables/index.html",
    "href": "docs/blog/posts/accessing-quarto-variables/index.html",
    "title": "Accessing Quarto Variables",
    "section": "",
    "text": "You can access dynamic variables within documents, which can be useful for externalizing content that varies depending on context. As an example, you can reference file metadata using the syntax {&lt; meta title &gt;}, which would include the title of this article: Accessing Quarto Variables.\nIsn’t that cool? \n\n\n\n Back to top"
  },
  {
    "objectID": "docs/development/code/bash.html",
    "href": "docs/development/code/bash.html",
    "title": "Bash",
    "section": "",
    "text": "sudo mount -t cifs //10.10.145.5/SourceDir /home/&lt;user&gt;/mnt/destination -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "docs/development/code/bash.html#mount-a-share",
    "href": "docs/development/code/bash.html#mount-a-share",
    "title": "Bash",
    "section": "",
    "text": "sudo mount -t cifs //10.10.145.5/SourceDir /home/&lt;user&gt;/mnt/destination -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "docs/development/code/html.html",
    "href": "docs/development/code/html.html",
    "title": "HTML",
    "section": "",
    "text": "Below is an example of a basic document header that includes local javascript and css document resources, plus hosted Bootstrap and jQuery libraries.\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\" dir=\"ltr\"&gt;\n\n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\" /&gt;\n        &lt;title&gt;Bootstrap 5&lt;/title&gt;\n        &lt;!-- Bootstrap CSS --&gt;\n        &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\" /&gt;\n        &lt;!-- Local assets --&gt;\n        &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"assets/css/main.css\" /&gt;\n        &lt;!-- Favicon --&gt;\n        &lt;link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"\" /&gt;\n        &lt;!-- jQuery --&gt;\n        &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js\"&gt;&lt;/script&gt;\n        &lt;!-- Bootstrap JS and Popper --&gt;\n        &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n        &lt;!-- Local scripts --&gt;\n        &lt;script type=\"text/javascript\" src=\"assets/js/main.js\"&gt;&lt;/script&gt;\n    &lt;/head&gt;\n\n    &lt;body&gt;\n    &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "docs/development/code/html.html#document-header",
    "href": "docs/development/code/html.html#document-header",
    "title": "HTML",
    "section": "",
    "text": "Below is an example of a basic document header that includes local javascript and css document resources, plus hosted Bootstrap and jQuery libraries.\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\" dir=\"ltr\"&gt;\n\n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\" /&gt;\n        &lt;title&gt;Bootstrap 5&lt;/title&gt;\n        &lt;!-- Bootstrap CSS --&gt;\n        &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\" /&gt;\n        &lt;!-- Local assets --&gt;\n        &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"assets/css/main.css\" /&gt;\n        &lt;!-- Favicon --&gt;\n        &lt;link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"\" /&gt;\n        &lt;!-- jQuery --&gt;\n        &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js\"&gt;&lt;/script&gt;\n        &lt;!-- Bootstrap JS and Popper --&gt;\n        &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n        &lt;!-- Local scripts --&gt;\n        &lt;script type=\"text/javascript\" src=\"assets/js/main.js\"&gt;&lt;/script&gt;\n    &lt;/head&gt;\n\n    &lt;body&gt;\n    &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "docs/development/code/javascript.html",
    "href": "docs/development/code/javascript.html",
    "title": "JavaScript",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "docs/development/code/markdown.html",
    "href": "docs/development/code/markdown.html",
    "title": "Markdown",
    "section": "",
    "text": "Code chunks are useful when embeding sections of code directly in a markdown document. The code chunk allows for code to be syntactically highlighted to make it easier to read.\nCheck out all the supported languages.\n\nPythonJavaScript\n\n\nMarkup:\n```{py}\nprint(\"Hello World\")\n```\nOutput:\nprint(\"Hello World\")\n\n\nMarkup:\n```{js}\nconsole.log(\"Hello World\");\n```\nOutput:\nconsole.log(\"Hello World\");"
  },
  {
    "objectID": "docs/development/code/markdown.html#code-chunks",
    "href": "docs/development/code/markdown.html#code-chunks",
    "title": "Markdown",
    "section": "",
    "text": "Code chunks are useful when embeding sections of code directly in a markdown document. The code chunk allows for code to be syntactically highlighted to make it easier to read.\nCheck out all the supported languages.\n\nPythonJavaScript\n\n\nMarkup:\n```{py}\nprint(\"Hello World\")\n```\nOutput:\nprint(\"Hello World\")\n\n\nMarkup:\n```{js}\nconsole.log(\"Hello World\");\n```\nOutput:\nconsole.log(\"Hello World\");"
  },
  {
    "objectID": "docs/development/code/markdown.html#diagrams",
    "href": "docs/development/code/markdown.html#diagrams",
    "title": "Markdown",
    "section": "Diagrams",
    "text": "Diagrams\nIn addition to code chunks, Markdown supports Mermaid JS, which is a diagramming and charting tool. An example Mermaid diagram is illustrated below.\nMarkup\n```{mermaid}\nflowchart LR\n    Start --&gt; Stop\n```\nOutput\n\n\n\n\nflowchart LR\n    Start --&gt; Stop"
  },
  {
    "objectID": "docs/development/code/python.html",
    "href": "docs/development/code/python.html",
    "title": "Python",
    "section": "",
    "text": "import glob\nimport os\n\nfor filename in glob.glob(\"./**/*.ext\", recursive=True):\n    new_name = \"-\".join(filename.split(\"_\"))\n    os.rename(filename, new_name)"
  },
  {
    "objectID": "docs/development/code/python.html#batch-file-rename",
    "href": "docs/development/code/python.html#batch-file-rename",
    "title": "Python",
    "section": "",
    "text": "import glob\nimport os\n\nfor filename in glob.glob(\"./**/*.ext\", recursive=True):\n    new_name = \"-\".join(filename.split(\"_\"))\n    os.rename(filename, new_name)"
  },
  {
    "objectID": "docs/development/code/python.html#logging-basics",
    "href": "docs/development/code/python.html#logging-basics",
    "title": "Python",
    "section": "Logging Basics",
    "text": "Logging Basics\nimport argparse\nimport logging\nimport os\nfrom datetime import datetime\n\n# Do not specify __name__ to use root log level\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\n    \"%(asctime)s : %(msecs)04d : %(name)s : %(levelname)s : %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlog_file = (\n    f\"{os.path.splitext(__file__)[0]}_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.log\"\n)\nstream_handler = logging.StreamHandler()\nstream_handler.setFormatter(formatter)\n\n\ndef clparser() -&gt; argparse.ArgumentParser:\n    \"\"\"Create a parser to handle input arguments and displaying a help message.\"\"\"\n    desc_msg = \"\"\"My logging program.\"\"\"\n    parser = argparse.ArgumentParser(description=desc_msg)\n    parser.add_argument(\n        \"-l\",\n        \"--logfile\",\n        action=\"store_true\",\n        help=\"Write log messages to a file.\",\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Control the amount of information to display.\",\n    )\n    return parser\n\n\nif __name__ == \"__main__\":\n    args = clparser().parse_args()\n    if args.verbose:\n        logger.addHandler(stream_handler)\n    if args.logfile:\n        file_handler = logging.FileHandler(filename=log_file)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n    logger.info(\"Begin my test module\")"
  },
  {
    "objectID": "docs/development/code/python.html#sending-http-requests",
    "href": "docs/development/code/python.html#sending-http-requests",
    "title": "Python",
    "section": "Sending HTTP Requests",
    "text": "Sending HTTP Requests\nimport logging\n\nimport requests\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_request(request_type: str, url: str, **kwargs) -&gt; requests.Response:\n    \"\"\"Send an HTTP request.\n\n    Args:\n    ----\n        request_type (str): Accepts \"GET\" or \"POST\"\n        url (str): Request URL\n\n    Returns:\n    -------\n        requests.Response: Request response.\n    \"\"\"\n    valid_methods = (\"GET\", \"POST\")\n    if request_type.upper() not in valid_methods:\n        raise ValueError(f\"Invalid request type. Supported types: {valid_methods}\")\n    try:\n        response = requests.request(request_type.upper(), url, **kwargs)\n        response.raise_for_status()  # Raises an exception if status code &gt;= 400\n        return response\n    except requests.exceptions.RequestException as err:\n        logger.error(f\"{err}. Request type: {request_type}. URL: {url}. Args: {kwargs}\")\n        raise err\n\n# Example GET request\nresponse = send_request(\"GET\", \"https://api.publicapis.org/entries\", timeout=5)\nlogger.info(response.json())\n\n# Example POST request\nresponse = send_request(\"POST\", \"https://someurl.com\", headers={}, timeout=5)\nlogger.info(response.json())"
  },
  {
    "objectID": "docs/development/code/python.html#regular-expressions",
    "href": "docs/development/code/python.html#regular-expressions",
    "title": "Python",
    "section": "Regular Expressions",
    "text": "Regular Expressions\n\nQuickstart\nRegex101\n\n\nMetaCharacters (Need to be escaped)\n. ^ $ * + ? { } [ ] \\ | ( )\n\n\nCharacters\n. - Any Character Except New Line \\d - Digit (0-9) \\D - Not a Digit (0-9) \\w - Word Character (a-z, A-Z, 0-9, _) \\W - Not a Word Character \\s - Whitespace (space, tab, newline) \\S - Not Whitespace (space, tab, newline)\n\n\nCharacter Classes\n[] - Matches Characters in brackets [^ ] - Matches Characters NOT in brackets [a-z] - Any lowercase character between a and z [A-Z] - Any UPPERCASE character between A and Z\n\n\nQuantifiers\n* - 0 or More + - 1 or More ? - 0 or One {3} - Exact Number {3,4} - Range of Numbers (Minimum, Maximum) {3,} - At least 3\n\n\nAnchors & Boundaries\n\\b - Word Boundary \\B - Not a Word Boundary ^ - Beginning of a String $ - End of a String\n\n\nLogic\n| - Either Or ( ) - Group \\1 - Contents of group 1\n\n\nWhite-space\n\\t - Tab \\r - Carriage return \\n - New line\n\n\nExample\n\n\nCode\nimport re\n\ntext_string = \"\"\"\nHello world\n\n8001234567\n800-321-7654\n900.987.6543\n\nsome.email@email.com\nmycompany@company.net\nwierd-12-address-4@somedomain.blah\n\"\"\"\n\npattern = re.compile(r\"[0-9]{3}[.-]?[0-9]{3}[.-]?[0-9]{4}\")\nmatches = re.finditer(pattern, text_string)\n\nfor match in matches:\n    print(match)\n    print(match.span())\n    print(text_string[match.start() : match.end()])\n\n\n&lt;re.Match object; span=(14, 24), match='8001234567'&gt;\n(14, 24)\n8001234567\n&lt;re.Match object; span=(25, 37), match='800-321-7654'&gt;\n(25, 37)\n800-321-7654\n&lt;re.Match object; span=(38, 50), match='900.987.6543'&gt;\n(38, 50)\n900.987.6543"
  },
  {
    "objectID": "docs/development/code/python.html#oop-basics",
    "href": "docs/development/code/python.html#oop-basics",
    "title": "Python",
    "section": "OOP Basics",
    "text": "OOP Basics\n\n\nCode\nclass Employee:\n    \"\"\"Create an employee object with relevant attributes.\"\"\"\n\n    annual_raise_pct = 0.04\n    employee_no = 1\n\n    def __init__(self, first_name, last_name, position, years_employed=1):\n        \"\"\"Function called when new object is initiated.\"\"\"\n        self.first_name = first_name\n        self.last_name = last_name\n        self.position = position\n        self.years_employed = years_employed\n        self.employee_number = Employee.employee_no\n        self.email = self.first_name + \".\" + self.last_name + \"@company.com\"\n        self.salary = self.calculate_salary()\n        Employee.employee_no += 1\n\n    def starting_salary(self):\n        \"\"\"Return the starting salary for each position at company\"\"\"\n        if self.position == \"HR\":\n            return 25000\n        elif self.position == \"Management\":\n            return 50000\n        elif self.position == \"Developer\":\n            return 100000\n        elif self.position == \"CEO\":\n            return 200000\n        else:\n            return None\n\n    def calculate_salary(self):\n        salary = self.starting_salary()\n        for year in range(self.years_employed):\n            salary = int(salary + (salary * Employee.annual_raise_pct))\n        return salary\n\n    def __repr__(self):\n        \"\"\"Create representational object string\"\"\"\n        return f\"\"\"Employee(first_name = {self.first_name}, last_name = {self.last_name}, position = {self.position}, years_employed = {self.years_employed})\"\"\"\n\n\ncurrent_employees = [\n    Employee(\"Geo\", \"Coug\", \"Developer\", 6),\n    Employee(\"Jane\", \"Doe\", \"HR\", 4),\n    Employee(\"John\", \"Doe\", \"Management\", 15),\n    Employee(\"Bob\", \"Loblaw\", \"CEO\", 24),\n]\n\nfor e in current_employees:\n    print(f\"Employee Number: {e.employee_number}\")\n    print(e)\n    print(\"Email:\", e.email)\n    print(\"Salary: ${:,}\\n\".format(e.salary))\n\n\nEmployee Number: 1\nEmployee(first_name = Geo, last_name = Coug, position = Developer, years_employed = 6)\nEmail: Geo.Coug@company.com\nSalary: $126,530\n\nEmployee Number: 2\nEmployee(first_name = Jane, last_name = Doe, position = HR, years_employed = 4)\nEmail: Jane.Doe@company.com\nSalary: $29,245\n\nEmployee Number: 3\nEmployee(first_name = John, last_name = Doe, position = Management, years_employed = 15)\nEmail: John.Doe@company.com\nSalary: $90,039\n\nEmployee Number: 4\nEmployee(first_name = Bob, last_name = Loblaw, position = CEO, years_employed = 24)\nEmail: Bob.Loblaw@company.com\nSalary: $512,645"
  },
  {
    "objectID": "docs/development/code/python.html#dbms-data-types",
    "href": "docs/development/code/python.html#dbms-data-types",
    "title": "Python",
    "section": "DBMS Data Types",
    "text": "DBMS Data Types\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"../../../static/development/data-types.csv\")\ndf.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nData Type\n\n\n\nPostgres\n\n\n\nMariaDB\n\n\n\nSQL Server\n\n\n\nFirebird\n\n\n\nMS-Access\n\n\n\nSQLite\n\n\n\n\n\n\n\n\n\n\n\nTimestamp with time zone\n\n\n\n1184.0\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nTimestamp\n\n\n\n1184.0\n\n\n\n7\n\n\n\nNaN\n\n\n\ntype ‘datetime.datetime’\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nDatetime\n\n\n\nNaN\n\n\n\n12\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\n\n\n\n\nDate\n\n\n\n1082.0\n\n\n\n10\n\n\n\nclass ‘datetime.date’\n\n\n\ntype ‘datetime.date’\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\n\n\n\n\nTime\n\n\n\n1083.0\n\n\n\n11\n\n\n\nclass ‘datetime.time’\n\n\n\ntype ‘datetime.time’\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\n\n\n\n\nBoolean\n\n\n\n16.0\n\n\n\n16\n\n\n\nclass ‘bool’\n\n\n\nNaN\n\n\n\nclass ‘bool’\n\n\n\nNaN\n\n\n\n\n\n\n\nSmall integer\n\n\n\n21.0\n\n\n\n1\n\n\n\nclass ‘int’\n\n\n\nNaN\n\n\n\nclass ‘int’\n\n\n\nNaN\n\n\n\n\n\n\n\nInteger\n\n\n\n23.0\n\n\n\n2\n\n\n\nclass ‘int’\n\n\n\ntype ‘int’\n\n\n\nclass ‘int’\n\n\n\nNaN\n\n\n\n\n\n\n\nLong integer\n\n\n\n20.0\n\n\n\n3\n\n\n\nclass ‘int’\n\n\n\ntype ‘long’\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nSingle\n\n\n\n701.0\n\n\n\n4\n\n\n\nclass ‘float’\n\n\n\ntype ‘float’\n\n\n\nclass ‘float’\n\n\n\nNaN\n\n\n\n\n\n\n\nDouble precision\n\n\n\n701.0\n\n\n\n5\n\n\n\nclass ‘float’\n\n\n\ntype ‘float’\n\n\n\nclass ‘float’\n\n\n\nNaN\n\n\n\n\n\n\n\nDecimal\n\n\n\n1700.0\n\n\n\n0\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nCurrency\n\n\n\n790.0\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nNaN\n\n\n\n\n\n\n\nCharacter\n\n\n\n1042.0\n\n\n\nNaN\n\n\n\nclass ‘str’\n\n\n\ntype ‘str’\n\n\n\nclass ‘str’\n\n\n\nNaN\n\n\n\n\n\n\n\nCharacter varying\n\n\n\n1043.0\n\n\n\n15\n\n\n\nclass ‘str’\n\n\n\ntype ‘str’\n\n\n\nclass ‘str’\n\n\n\nNaN\n\n\n\n\n\n\n\nText\n\n\n\n25.0\n\n\n\nNaN\n\n\n\nclass ‘str’\n\n\n\ntype ‘str’\n\n\n\nclass ‘str’\n\n\n\nNaN\n\n\n\n\n\n\n\nBinary / BLOB\n\n\n\n17.0\n\n\n\n249,250,251,252\n\n\n\nclass ‘bytearray’\n\n\n\ntype ‘str’\n\n\n\ntype ‘bytearray’\n\n\n\nNaN"
  },
  {
    "objectID": "docs/development/code/python.html#dbms-libraries",
    "href": "docs/development/code/python.html#dbms-libraries",
    "title": "Python",
    "section": "DBMS Libraries",
    "text": "DBMS Libraries\n\nPostgres: psycopg2\nMariaDB: pymysql\nSQL Server: pyodbc\nFirebird: fdb\nMS-Access: pyodbc\nSQLite: sqlite3"
  },
  {
    "objectID": "docs/development/code/python.html#db-table-dependencies",
    "href": "docs/development/code/python.html#db-table-dependencies",
    "title": "Python",
    "section": "DB Table Dependencies",
    "text": "DB Table Dependencies\ndef dependency_order(dep_list):\n    rem_tables = list(set([t[0] for t in dep_list] + [t[1] for t in dep_list]))\n    rem_dep = copy.copy(dep_list)\n    sortkey = 1\n    ret_list = []\n    while len(rem_dep) &gt; 0:\n        tbls = [tbl for tbl in rem_tables if tbl not in [dep[0] for dep in rem_dep]]\n        ret_list.extend([(tb, sortkey) for tb in tbls])\n        rem_tables = [tbl for tbl in rem_tables if tbl not in tbls]\n        rem_dep = [dep for dep in rem_dep if dep[1] not in tbls]\n        sortkey += 1\n    if len(rem_tables) &gt; 0:\n        ret_list.extend([(tb, sortkey) for tb in rem_tables])\n    ret_list.sort(cmp=lambda x, y: cmp(x[1], y[1]))\n    return [item[0] for item in ret_list]"
  },
  {
    "objectID": "docs/development/code/python.html#csv-sniffer",
    "href": "docs/development/code/python.html#csv-sniffer",
    "title": "Python",
    "section": "CSV Sniffer",
    "text": "CSV Sniffer\nimport re\n\nclass CsvDiagError(Exception):\n    def __init__(self, msg):\n        self.value = msg\n\n    def __str__(self):\n        return self.value\n\nclass CsvLine:\n    escchar = \"\\\\\"\n\n    def __init__(self, line_text):\n        self.text = line_text\n        self.delim_counts = {}\n        self.item_errors = []  # A list of error messages.\n\n    def __str__(self):\n        return \"; \".join(\n            [\n                \"Text: &lt;&lt;%s&gt;&gt;\" % self.text,\n                \"Delimiter counts: &lt;&lt;%s&gt;&gt;\"\n                % \", \".join(\n                    [\n                        \"%s: %d\" % (k, self.delim_counts[k])\n                        for k in self.delim_counts.keys()\n                    ]\n                ),\n            ]\n        )\n\n    def count_delim(self, delim):\n        # If the delimiter is a space, consider multiple spaces to be equivalent\n        # to a single delimiter, split on the space(s), and consider the delimiter\n        # count to be one fewer than the items returned.\n        if delim == \" \":\n            self.delim_counts[delim] = max(0, len(re.split(r\" +\", self.text)) - 1)\n        else:\n            self.delim_counts[delim] = self.text.count(delim)\n\n    def delim_count(self, delim):\n        return self.delim_counts[delim]\n\n    def _well_quoted(self, element, qchar):\n        # A well-quoted element has either no quotes, a quote on each end and none\n        # in the middle, or quotes on both ends and every internal quote is either\n        # doubled or escaped.\n        # Returns a tuple of three booleans; the first indicates whether the element is\n        # well-quoted, the second indicates whether the quote character is used\n        # at all, and the third indicates whether the escape character is used.\n        if qchar not in element:\n            return (True, False, False)\n        if len(element) == 0:\n            return (True, False, False)\n        if element[0] == qchar and element[-1] == qchar and qchar not in element[1:-1]:\n            return (True, True, False)\n        # The element has quotes; if it doesn't have one on each end, it is not well-quoted.\n        if not (element[0] == qchar and element[-1] == qchar):\n            return (False, True, False)\n        e = element[1:-1]\n        # If there are no quotes left after removing doubled quotes, this is well-quoted.\n        if qchar not in e.replace(qchar + qchar, \"\"):\n            return (True, True, False)\n        # if there are no quotes left after removing escaped quotes, this is well-quoted.\n        if qchar not in e.replace(self.escchar + qchar, \"\"):\n            return (True, True, True)\n        return (False, True, False)\n\n    def record_format_error(self, pos_no, errmsg):\n        self.item_errors.append(\"%s in position %d.\" % (errmsg, pos_no))\n\n    def items(self, delim, qchar):\n        # Parses the line into a list of items, breaking it at delimiters that are not\n        # within quoted stretches.  (This is a almost CSV parser, for valid delim and qchar,\n        # except that it does not eliminate quote characters or reduce escaped quotes.)\n        self.item_errors = []\n        if qchar is None:\n            if delim is None:\n                return self.text\n            else:\n                if delim == \" \":\n                    return re.split(r\" +\", self.text)\n                else:\n                    return self.text.split(delim)\n        elements = []  # The list of items on the line that will be returned.\n        eat_multiple_delims = delim == \" \"\n        # States of the FSM:\n        # _IN_QUOTED: An opening quote has been seen, but no closing quote encountered.\n        #  Actions / transition:\n        #   quote: save char in escape buffer / _ESCAPED\n        #   esc_char : save char in escape buffer / _ESCAPED\n        #   delimiter: save char in element buffer / _IN_QUOTED\n        #   other: save char in element buffer / _IN_QUOTED\n        # _ESCAPED: An escape character has been seen while _IN_QUOTED (and is in the escape buffer).\n        #  Actions / transitions\n        #   quote: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        #   delimiter: save escape buffer in element buffer, empty escape buffer,\n        #    save element buffer, empty element buffer / _BETWEEN\n        #   other: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        # _QUOTE_IN_QUOTED: A quote has been seen while _IN_QUOTED (and is in the escape buffer).\n        #  Actions / transitions\n        #   quote: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        #   delimiter: save escape buffer in element buffer, empty escape buffer,\n        #    save element buffer, empty element buffer / _DELIMITED\n        #   other: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        #     (An 'other' character in this position represents a bad format:\n        #     a quote not followed by another quote or a delimiter.)\n        # _IN_UNQUOTED: A non-delimiter, non-quote has been seen.\n        #  Actions / transitions\n        #   quote: save char in element buffer / _IN_UNQUOTED\n        #    (This represents a bad format.)\n        #   delimiter: save element buffer, empty element buffer / _DELIMITED\n        #   other: save char in element buffer / _IN_UNQUOTED\n        # _BETWEEN: Not in an element, and a delimiter not seen.  This is the starting state,\n        #   and the state following a closing quote but before a delimiter is seen.\n        #  Actions / transition:\n        #   quote: save char in element buffer / _IN_QUOTED\n        #   delimiter: save element buffer, empty element buffer / _DELIMITED\n        #    (The element buffer should be empty, representing a null data item.)\n        #   other: save char in element buffer / _IN_UNQUOTED\n        # _DELIMITED: A delimiter has been seen while not in a quoted item.\n        #  Actions / transition:\n        #   quote: save char in element buffer / _IN_QUOTED\n        #   delimiter: if eat_multiple: no action / _DELIMITED\n        #     if not eat_multiple: save element buffer, empty element buffer / _DELIMITED\n        #   other: save char in element buffer / _IN_UNQUOTED\n        # At end of line: save escape buffer in element buffer, save element buffer.  For a well-formed\n        # line, these should be empty, but they may not be.\n        #\n        # Define the state constants, which will also be used as indexes into an execution vector.\n        (\n            _IN_QUOTED,\n            _ESCAPED,\n            _QUOTE_IN_QUOTED,\n            _IN_UNQUOTED,\n            _BETWEEN,\n            _DELIMITED,\n        ) = range(6)\n        #\n        # Because of Python 2.7's scoping rules:\n        # * The escape buffer and current element are defined as mutable objects that will have their\n        #  first elements modified, rather than as string variables.  (Python 2.x does not allow\n        #  modification of a variable in an enclosing scope that is not the global scope, but\n        #  mutable objects like lists can be altered.  Another approach would be to implement this\n        #  as a class and use instance variables.)\n        # * The action functions return the next state rather than assigning it directly to the 'state' variable.\n        esc_buf = [\"\"]\n        current_element = [\"\"]\n\n        def in_quoted():\n            if c == self.escchar:\n                esc_buf[0] = c\n                return _ESCAPED\n            elif c == qchar:\n                esc_buf[0] = c\n                return _QUOTE_IN_QUOTED\n            else:\n                current_element[0] += c\n                return _IN_QUOTED\n\n        def escaped():\n            if c == delim:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _BETWEEN\n            else:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                current_element[0] += c\n                return _IN_QUOTED\n\n        def quote_in_quoted():\n            if c == qchar:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                current_element[0] += c\n                return _IN_QUOTED\n            elif c == delim:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                current_element[0] += c\n                self.record_format_error(\n                    i + 1, \"Unexpected character following a closing quote\"\n                )\n                return _IN_QUOTED\n\n        def in_unquoted():\n            if c == delim:\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += c\n                return _IN_UNQUOTED\n\n        def between():\n            if c == qchar:\n                current_element[0] += c\n                return _IN_QUOTED\n            elif c == delim:\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += c\n                return _IN_UNQUOTED\n\n        def delimited():\n            if c == qchar:\n                current_element[0] += c\n                return _IN_QUOTED\n            elif c == delim:\n                if not eat_multiple_delims:\n                    elements.append(current_element[0])\n                    current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += c\n                return _IN_UNQUOTED\n\n        # Functions in the execution vector must be ordered identically to the\n        # indexes represented by the state constants.\n        exec_vector = [\n            in_quoted,\n            escaped,\n            quote_in_quoted,\n            in_unquoted,\n            between,\n            delimited,\n        ]\n        # Set the starting state.\n        state = _BETWEEN\n        # Process the line of text.\n        for i, c in enumerate(self.text):\n            state = exec_vector[state]()\n        # Process the end-of-line condition.\n        if len(esc_buf[0]) &gt; 0:\n            current_element[0] += esc_buf[0]\n        if len(current_element[0]) &gt; 0:\n            elements.append(current_element[0])\n        return elements\n\n    def well_quoted_line(self, delim, qchar):\n        # Returns a tuple of boolean, int, and boolean, indicating: 1) whether the line is\n        # well-quoted, 2) the number of elements for which the quote character is used,\n        # and 3) whether the escape character is used.\n        wq = [self._well_quoted(el, qchar) for el in self.items(delim, qchar)]\n        return (\n            all([b[0] for b in wq]),\n            sum([b[1] for b in wq]),\n            any([b[2] for b in wq]),\n        )\n\ndef diagnose_delim(linestream, possible_delimiters=None, possible_quotechars=None):\n    # Returns a tuple consisting of the delimiter, quote character, and escape\n    # character for quote characters within elements of a line.  All may be None.\n    # If the escape character is not None, it will be u\"\\\".\n    # Arguments:\n    # * linestream: An iterable file-like object with a 'next()' method that returns lines of text\n    #  as bytes or unicode.\n    # * possible_delimiters: A list of single characters that might be used to separate items on\n    #  a line.  If not specified, the default consists of tab, comma, semicolon, and vertical rule.\n    #  If a space character is included, multiple space characters will be treated as a single\n    #  delimiter--so it's best if there are no missing values on space-delimited lines, though\n    #  that is not necessarily a fatal flaw unless there is a very high fraction of missing values.\n    # * possible_quotechars: A list of single characters that might be used to quote items on\n    #  a line.  If not specified, the default consists of single and double quotes.\n    if not possible_delimiters:\n        possible_delimiters = [\"\\t\", \",\", \";\", \"|\"]\n    if not possible_quotechars:\n        possible_quotechars = ['\"', \"'\"]\n    lines = []\n    for i in range(100):\n        try:\n            ln = linestream.next()\n        except StopIteration:\n            break\n        except:\n            raise\n        while len(ln) &gt; 0 and ln[-1] in (\"\\n\", \"\\r\"):\n            ln = ln[:-1]\n        if len(ln) &gt; 0:\n            lines.append(CsvLine(ln))\n    if len(lines) == 0:\n        raise CsvDiagError(\"CSV diagnosis error: no lines read\")\n    for ln in lines:\n        for d in possible_delimiters:\n            ln.count_delim(d)\n    # For each delimiter, find the minimum number of delimiters found on any line, and the number of lines\n    # with that minimum number\n    delim_stats = {}\n    for d in possible_delimiters:\n        dcounts = [ln.delim_count(d) for ln in lines]\n        min_count = min(dcounts)\n        delim_stats[d] = (min_count, dcounts.count(min_count))\n    # Remove delimiters that were never found.\n    for k in delim_stats.keys():\n        if delim_stats[k][0] == 0:\n            del delim_stats[k]\n\n    def all_well_quoted(delim, qchar):\n        # Returns a tuple of boolean, int, and boolean indicating: 1) whether the line is\n        # well-quoted, 2) the total number of lines and elements for which the quote character\n        # is used, and 3) the escape character used.\n        wq = [l.well_quoted_line(delim, qchar) for l in lines]\n        return (\n            all([b[0] for b in wq]),\n            sum([b[1] for b in wq]),\n            CsvLine.escchar if any([b[2] for b in wq]) else None,\n        )\n\n    def eval_quotes(delim):\n        # Returns a tuple of the form to be returned by 'diagnose_delim()'.\n        ok_quotes = {}\n        for q in possible_quotechars:\n            allwq = all_well_quoted(delim, q)\n            if allwq[0]:\n                ok_quotes[q] = (allwq[1], allwq[2])\n        if len(ok_quotes) == 0:\n            return (delim, None, None)  # No quotes, no escapechar\n        else:\n            max_use = max([v[0] for v in ok_quotes.values()])\n            if max_use == 0:\n                return (delim, None, None)\n            # If multiple quote characters have the same usage, return (arbitrarily) the first one.\n            for q in ok_quotes.keys():\n                if ok_quotes[q][0] == max_use:\n                    return (delim, q, ok_quotes[q][1])\n\n    if len(delim_stats) == 0:\n        # None of the delimiters were found.  Some other delimiter may apply,\n        # or the input may contain a single value on each line.\n        # Identify possible quote characters.\n        return eval_quotes(None)\n    else:\n        if len(delim_stats) &gt; 1:\n            # If one of them is a space, prefer the non-space\n            if \" \" in delim_stats.keys():\n                del delim_stats[\" \"]\n        if len(delim_stats) == 1:\n            return eval_quotes(delim_stats.keys()[0])\n        # Assign weights to the delimiters.  The weight is the square of the minimum number of delimiters\n        # on a line times the number of lines with that delimiter.\n        delim_wts = {}\n        for d in delim_stats.keys():\n            delim_wts[d] = delim_stats[d][0] ** 2 * delim_stats[d][1]\n        # Evaluate quote usage for each delimiter, from most heavily weighted to least.\n        # Return the first good pair where the quote character is used.\n        delim_order = sorted(delim_wts, key=delim_wts.get, reverse=True)\n        for d in delim_order:\n            quote_check = eval_quotes(d)\n            if quote_check[0] and quote_check[1]:\n                return quote_check\n        # There are no delimiters for which quotes are OK.\n        return (delim_order[0], None, None)\n    # Should never get here\n    raise CsvDiagError(\"CSV diagnosis error: an untested set of conditions are present\")"
  },
  {
    "objectID": "docs/development/code/python.html#import-csv-to-a-database",
    "href": "docs/development/code/python.html#import-csv-to-a-database",
    "title": "Python",
    "section": "Import CSV to a Database",
    "text": "Import CSV to a Database\nclass CsvFile(object):\n    \"\"\"CsvFile class automatically opens a file and creates a CSV reader, reads the first row containing column headers, and stores those headers so that they can be used to construct the INSERT statement.\"\"\"\n    def __init__(self, filename):\n        self.fn = filename\n        self.f = None\n        self.open()\n        self.rdr = csv.reader(self.f)\n        self.headers = next(self.rdr)\n    def open(self):\n        if self.f is None:\n            mode = \"rb\" if sys.version_info &lt; (3,) else \"r\"\n            self.f = open(self.fn, mode)\n    def reader(self):\n        return self.rdr\n    def close(self):\n        self.rdr = None\n        self.f.close()\n        self.f = None\n\n\nclass Database(object):\n    \"\"\"The Database class and subclasses provide a database connection for each type of DBMS, and a method to construct an INSERT statement for a given CsvFile object, using that DBMS's parameter substitution string.  The conn_info argument is a dictionary containing the host name, user name, and password.\"\"\"\n    def __init__(self, conn_info):\n        self.paramstr = '%s'\n        self.conn = None\n    def insert_sql(self, tablename, csvfile):\n        return \"insert into %s (%s) values (%s);\" % (\n                tablename,\n                \",\".join(csvfile.headers),\n                \",\".join([self.paramstr] * len(csvfile.headers))\n                )\n\n\nclass PgDb(Database):\n    def __init__(self, conn_info):\n        self.db_type = \"p\"\n        import psycopg2\n        self.paramstr = \"%s\"\n        connstr = \"host=%(server)s dbname=%(db)s user=%(user)s password=%(pw)s\" % conn_info\n        self.conn = psycopg2.connect(connstr)\n\n    def postgres_copy(csvfile, db):\n        \"\"\"Postgres COPY command. Fastest implementation\"\"\"\n        curs = db.conn.cursor()\n        rf = open(csvfile.fn, \"rt\")\n        # Read and discard headers\n        hdrs = rf.readline()\n        copy_cmd = \"copy copy_test from stdin with (format csv)\"\n        curs.copy_expert(copy_cmd, rf)\n\n    def simple_copy(csvfile, db):\n        \"\"\"Row-by-row reading and writing\"\"\"\n        ins_sql = db.insert_sql(\"copy_test\", csvfile)\n        curs = db.conn.cursor()\n        rdr = csvfile.reader()\n        for line in rdr:\n            curs.execute(ins_sql, clean_line(line))\n        db.conn.commit()\n\n    def buffer1_copy(csvfile, db, buflines):\n        \"\"\"Buffered reading and writing\"\"\"\n        ins_sql = db.insert_sql(\"copy_test\", csvfile)\n        curs = db.conn.cursor()\n        rdr = csvfile.reader()\n        eof = False\n        while True:\n            b = []\n            for j in range(buflines):\n                try:\n                    line = next(rdr)\n                except StopIteration:\n                    eof = True\n                else:\n                    b.append(clean_line(line))\n            if len(b) &gt; 0:\n                curs.executemany(ins_sql, b)\n            if eof:\n                break\n        db.conn.commit()"
  },
  {
    "objectID": "docs/development/code/regex.html",
    "href": "docs/development/code/regex.html",
    "title": "Regex",
    "section": "",
    "text": "\\[([^\\]]+)\\]\\(([^\\)]+)\\)"
  },
  {
    "objectID": "docs/development/code/regex.html#markdown-link-pattern",
    "href": "docs/development/code/regex.html#markdown-link-pattern",
    "title": "Regex",
    "section": "",
    "text": "\\[([^\\]]+)\\]\\(([^\\)]+)\\)"
  },
  {
    "objectID": "docs/development/code/sql.html",
    "href": "docs/development/code/sql.html",
    "title": "SQL",
    "section": "",
    "text": "The code snippets on this page are primarily geared towards the PostgreSQL dialect."
  },
  {
    "objectID": "docs/development/code/sql.html#pg_dump",
    "href": "docs/development/code/sql.html#pg_dump",
    "title": "SQL",
    "section": "pg_dump",
    "text": "pg_dump\nReference\n\nExport table to sql\npg_dump -h [host] -d [database] -U [user] -s [schema only] -W [force password] &gt; &lt;file&gt;.sql"
  },
  {
    "objectID": "docs/development/code/sql.html#primary-key-relationships",
    "href": "docs/development/code/sql.html#primary-key-relationships",
    "title": "SQL",
    "section": "Primary Key Relationships",
    "text": "Primary Key Relationships\nColumns\n\ntable_schema: PK schema name\ntable_name: PK table name\nconstraint_name: PK constraint name\nposition: index of column in table (1, 2, …). 2 or higher means key is composite (contains more than one column)\nkey_column: PK column name\n\nRows\n\nOne row represents one primary key column\nScope of rows: columns of all PK constraints in a database\nOrdered by table schema, table name, column position\n\nselect * from (\n -- Main query. Returns all tables\n select kcu.table_schema,\n     kcu.table_name,\n     tco.constraint_name,\n     kcu.ordinal_position as position,\n     kcu.column_name as key_column\n from information_schema.table_constraints tco\n join information_schema.key_column_usage kcu \n   on kcu.constraint_name = tco.constraint_name\n   and kcu.constraint_schema = tco.constraint_schema\n   and kcu.constraint_name = tco.constraint_name\n where tco.constraint_type = 'PRIMARY KEY'\n order by kcu.table_schema,\n    kcu.table_name,\n    position\n) main\nwhere table_name = 'd_location'"
  },
  {
    "objectID": "docs/development/code/sql.html#foreign-key-relationships",
    "href": "docs/development/code/sql.html#foreign-key-relationships",
    "title": "SQL",
    "section": "Foreign Key Relationships",
    "text": "Foreign Key Relationships\nColumns\n\nforeign_table: foreign table schema and name\nrel: relationship symbol implicating direction\nprimary_table: primary (rerefenced) table schema and name\nfk_columns: list of FK colum names, separated with “,”\nconstraint_name: foreign key constraint name\n\nRows\n\nOne row represents one foreign key.\nIf foreign key consists of multiple columns (composite key) it is still represented as one row.\nScope of rows: all foregin keys in a database.\nOrdered by foreign table schema name and table name."
  },
  {
    "objectID": "docs/development/code/sql.html#ordering-tables-by-fk",
    "href": "docs/development/code/sql.html#ordering-tables-by-fk",
    "title": "SQL",
    "section": "Ordering Tables by FK",
    "text": "Ordering Tables by FK\ndrop table if exists dependencies cascade;\ncreate temporary table dependencies as\nselect \n        tc.table_name as child,\n        tu.table_name as parent\nfrom \n        information_schema.table_constraints as tc\n        inner join information_schema.constraint_table_usage as tu\n             on tu.constraint_name = tc.constraint_name\nwhere \n        tc.constraint_type = 'FOREIGN KEY'\n        and tc.table_name &lt;&gt; tu.table_name;\n\nwith recursive dep_depth as (\nselect\n    dep.child, dep.parent, 1 as lvl\nfrom\n    dependencies as dep\nunion all\nselect\n    dep.child, dep.parent, dd.lvl + 1 as lvl\nfrom\n    dep_depth as dd\n    inner join dependencies as dep on dep.parent = dd.child\n)\nselect\n    table_name, table_order\nfrom (\n    select\n        dd.parent as table_name, max(lvl) as table_order\n    from\n        dep_depth as dd\n    group by\n        table_name\n    union\n    select\n        dd.child as table_name, max(lvl) + 1 as level\n    from\n        dep_depth as dd\n        left join dependencies as dp on dp.parent = dd.child\n    where\n        dp.parent is null\n    group by dd.child\n) as all_levels\norder by table_order;\nselect * from (\n -- Main query. Returns FK relationships for all tables\n select \n   kcu.table_schema as table_schema,\n   kcu.table_name as foreign_table,\n     '&gt;-' as relationship,\n     rel_tco.table_name as primary_table,\n     string_agg(kcu.column_name, ', ') as fk_columns,\n     kcu.constraint_name\n from information_schema.table_constraints tco\n join information_schema.key_column_usage kcu\n     on tco.constraint_schema = kcu.constraint_schema\n     and tco.constraint_name = kcu.constraint_name\n join information_schema.referential_constraints rco\n     on tco.constraint_schema = rco.constraint_schema\n     and tco.constraint_name = rco.constraint_name\n join information_schema.table_constraints rel_tco\n     on rco.unique_constraint_schema = rel_tco.constraint_schema\n     and rco.unique_constraint_name = rel_tco.constraint_name\n where tco.constraint_type = 'FOREIGN KEY'\n group by kcu.table_schema,\n    kcu.table_name,\n    rel_tco.table_name,\n    rel_tco.table_schema,\n    kcu.constraint_name\n order by kcu.table_schema,\n    kcu.table_name\n) main\nwhere primary_table = 'd_location'"
  },
  {
    "objectID": "docs/development/code/sql.html#common-functions",
    "href": "docs/development/code/sql.html#common-functions",
    "title": "SQL",
    "section": "Common Functions",
    "text": "Common Functions\n\nLENGTH(string): Returns the length of the provided string\nPOSITION(string IN substring): Returns the position of the substring within the specified string.\nCAST(expression AS datatype): Converts an expression into the specified data type.\n`NOW: Returns the current date, including time.\nCEIL(input_val): Returns the smallest integer greater than the provided number.\nFLOOR(input_val): Returns the largest integer less than the provided number.\nROUND(input_val, [round_to]): Rounds a number to a specified number of decimal places.\nTRUNC(input_value, num_decimals): Truncates a number to a number of decimals.\nREPLACE(whole_string, string_to_replace, replacement_string): Replaces one string inside the whole string with another string.\nSUBSTRING(string, [start_pos], [length]): Returns part of a value, based on a position and length."
  },
  {
    "objectID": "docs/development/code/sql.html#add-role",
    "href": "docs/development/code/sql.html#add-role",
    "title": "SQL",
    "section": "Add Role",
    "text": "Add Role\n1create user &lt;username&gt; with password 'password';\ngrant connect on database &lt;db_name&gt; to &lt;username&gt;;\ngrant usage on schema &lt;schema_name&gt; to &lt;username&gt;;\ngrant select on all tables in schema &lt;schema_name&gt; to &lt;username&gt;;\nalter default privileges in schema &lt;schema_name&gt; grant select on tables to &lt;username&gt;;\n\n2grant create on database &lt;db_name&gt; to &lt;username&gt;;\n\n3grant insert on database &lt;db_name&gt; to &lt;username&gt;;\n\n4grant update on database &lt;db_name&gt; to &lt;username&gt;;\n\n5grant update on database &lt;db_name&gt; to &lt;username&gt;;\n\n1\n\nCreate a read-only user.\n\n2\n\nAllow user to create database objects.\n\n3\n\nAllow user to insert rows to any schema and table.\n\n4\n\nAllow user to update rows in any schema and table.\n\n5\n\nAllow user to delete rows in any schema and table."
  },
  {
    "objectID": "docs/development/code/sql.html#window-functions",
    "href": "docs/development/code/sql.html#window-functions",
    "title": "SQL",
    "section": "Window Functions",
    "text": "Window Functions\nfunction_name ( arguments ) OVER (\n [query_partition_clause]\n [ORDER BY order_by_clause\n [windowing_clause] ] \n)"
  },
  {
    "objectID": "docs/development/code/sql.html#finding-temporary-objects",
    "href": "docs/development/code/sql.html#finding-temporary-objects",
    "title": "SQL",
    "section": "Finding Temporary Objects",
    "text": "Finding Temporary Objects\nSELECT\n n.nspname as SchemaName,\n c.relname as RelationName,\n CASE c.relkind\n  WHEN 'r' THEN 'table'\n  WHEN 'v' THEN 'view'\n  WHEN 'i' THEN 'index'\n  WHEN 'S' THEN 'sequence'\n  WHEN 's' THEN 'special'\n  END as RelationType,\n pg_catalog.pg_get_userbyid(c.relowner) as RelationOwner,             \n pg_size_pretty(pg_relation_size(n.nspname ||'.'|| c.relname)) as RelationSize\nFROM pg_catalog.pg_class c\nLEFT JOIN pg_catalog.pg_namespace AS n ON n.oid = c.relnamespace\n WHERE  c.relkind IN ('r','s') \n AND  (n.nspname !~ '^pg_toast' and nspname like 'pg_temp%')\nORDER BY pg_relation_size(n.nspname ||'.'|| c.relname) DESC;"
  },
  {
    "objectID": "docs/development/code/sql.html#list-tables-in-database",
    "href": "docs/development/code/sql.html#list-tables-in-database",
    "title": "SQL",
    "section": "List tables in database",
    "text": "List tables in database\nSELECT table_schema, table_name \nFROM information_schema.tables \nORDER BY table_schema,table_name;"
  },
  {
    "objectID": "docs/development/code/sql.html#list-columns-in-table",
    "href": "docs/development/code/sql.html#list-columns-in-table",
    "title": "SQL",
    "section": "List columns in table",
    "text": "List columns in table\nSELECT column_name\nFROM   information_schema.columns\nWHERE  table_schema = 'schema'\nAND    table_name = 'table';"
  },
  {
    "objectID": "docs/development/code/sql.html#create-a-read-only-user",
    "href": "docs/development/code/sql.html#create-a-read-only-user",
    "title": "SQL",
    "section": "Create a read-only user",
    "text": "Create a read-only user\ngrant connect on database db_name to user;\ngrant usage on schema schema_name to user;\ngrant select on all tables in schema schema_name to user;\nalter default privileges in schema schema_name grant select on tables to user;"
  },
  {
    "objectID": "docs/development/code/sql.html#create-db",
    "href": "docs/development/code/sql.html#create-db",
    "title": "SQL",
    "section": "Create DB",
    "text": "Create DB\ncreate database &lt;new_db_name&gt; owner &lt;user_or_group&gt; template &lt;name_of_db_to_use_as_template&gt;;\n-- show search_path;\nset search_path to &lt;default_schema&gt;,public;\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\n-- Database Creation\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncreate database &lt;new_db_name&gt; owner &lt;user_or_group&gt; template &lt;name_of_db_to_use_as_template&gt;;\n-- show search_path;\nset search_path to idb, public;\n\ngrant connect, temporary on database &lt;new_db_name&gt; to public;\ngrant all on database &lt;new_db_name&gt; to &lt;user&gt;;\ngrant all on database &lt;new_db_name&gt; to &lt;group&gt;;\n\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\ncreate schema staging;\n\n-- Add a unique constraint to e_analyte.full_name and e_analyte.cas_rn so that\n--  no full name or cas_rn can be used for more than one analyte\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nalter table e_analyte\n  add constraint uc_fullname unique(full_name),\n  add constraint uc_casrn unique(cas_rn);"
  },
  {
    "objectID": "docs/development/code/sql.html#dblink",
    "href": "docs/development/code/sql.html#dblink",
    "title": "SQL",
    "section": "DBLink",
    "text": "DBLink\nselect\n  a.*, b.*\nfrom\n  table1 as a\n  left join (\n    select * from dblink(\n      'dbname=&lt;database&gt;',\n      'select col1, col2, col3 from &lt;table&gt;'\n    ) as d (\n      col1 text, col2 text, col3 text\n    )\n  ) as b\n  on a.col1 = b.col2"
  },
  {
    "objectID": "docs/development/code/sql.html#partitioning",
    "href": "docs/development/code/sql.html#partitioning",
    "title": "SQL",
    "section": "Partitioning",
    "text": "Partitioning\nselect * from (\n    select *, row_number() over(\n        partition by\n            col1, col2, col3\n        order by col1 desc\n    ) rowid\n    from sometable\n) someid\nwhere rowid &gt; 1;"
  },
  {
    "objectID": "docs/development/code/sql.html#table-relationships",
    "href": "docs/development/code/sql.html#table-relationships",
    "title": "SQL",
    "section": "Table Relationships",
    "text": "Table Relationships\nhttps://dataedo.com/kb/query/postgresql/list-tables-with-most-relationships\nselect * from\n(select relations.table_name as table_name, -- schema name and table name\n       count(relations.table_name) as relationships, -- number of table relationships\n       count(relations.referenced_tables) as foreign_keys, -- number of foreign keys in a table\n       count(relations.referencing_tables) as references, -- number of foreign keys that are refering to this table\n       count(distinct related_table) as related_tables, -- number of related tables\n       count(distinct relations.referenced_tables) as referenced_tables, -- number of different tables referenced with FKs (multiple FKs can refer to one table, so number of FKs might be different than number of referenced tables)\n       count(distinct relations.referencing_tables) as referencing_tables -- number of different tables that are refering to this table (similar to referenced_tables)\nfrom(\n     select pk_tco.table_schema || '.' || pk_tco.table_name as table_name,\n            fk_tco.table_schema || '.' || fk_tco.table_name as related_table,\n            fk_tco.table_name as referencing_tables,\n            null::varchar(100) as referenced_tables\n     from information_schema.referential_constraints rco\n     join information_schema.table_constraints fk_tco\n          on rco.constraint_name = fk_tco.constraint_name\n          and rco.constraint_schema = fk_tco.table_schema\n     join information_schema.table_constraints pk_tco\n          on rco.unique_constraint_name = pk_tco.constraint_name\n          and rco.unique_constraint_schema = pk_tco.table_schema\n    union all\n    select fk_tco.table_schema || '.' || fk_tco.table_name as table_name,\n           pk_tco.table_schema || '.' || pk_tco.table_name as related_table,\n           null as referencing_tables,\n           pk_tco.table_name as referenced_tables\n    from information_schema.referential_constraints rco\n    join information_schema.table_constraints fk_tco \n         on rco.constraint_name = fk_tco.constraint_name\n         and rco.constraint_schema = fk_tco.table_schema\n    join information_schema.table_constraints pk_tco\n         on rco.unique_constraint_name = pk_tco.constraint_name\n         and rco.unique_constraint_schema = pk_tco.table_schema\n) relations\ngroup by table_name\norder by relationships asc) results\n\nwhere substring(table_name, 5, 2) = 'd_'; -- substring(string, start_position, length)"
  },
  {
    "objectID": "docs/development/code/sql.html#current-database",
    "href": "docs/development/code/sql.html#current-database",
    "title": "SQL",
    "section": "Current Database",
    "text": "Current Database\nselect * from pg_catalog.current_database()"
  },
  {
    "objectID": "docs/development/code/sql.html#current-userrole",
    "href": "docs/development/code/sql.html#current-userrole",
    "title": "SQL",
    "section": "Current user/role",
    "text": "Current user/role\nselect * from current_role\nselect * from current_user"
  },
  {
    "objectID": "docs/development/code/sql.html#process-id",
    "href": "docs/development/code/sql.html#process-id",
    "title": "SQL",
    "section": "Process ID",
    "text": "Process ID\nselect * from pg_catalog.pg_backend_pid()"
  },
  {
    "objectID": "docs/development/code/sql.html#list-functionsdefsargs",
    "href": "docs/development/code/sql.html#list-functionsdefsargs",
    "title": "SQL",
    "section": "List functions/defs/args",
    "text": "List functions/defs/args\nselect \n    pg_get_userbyid(p.proowner) as owner,\n    n.nspname as function_schema,\n    p.proname as function_name,\n    l.lanname as function_language,\n    case when l.lanname = 'internal' then p.prosrc\n        else pg_get_functiondef(p.oid)\n        end as definition,\n    pg_get_function_arguments(p.oid) as function_arguments,\n    t.typname as return_type\nfrom pg_proc p\n    left join pg_namespace n on p.pronamespace = n.oid\n    left join pg_language l on p.prolang = l.oid\n    left join pg_type t on t.oid = p.prorettype \nwhere n.nspname not in ('pg_catalog', 'information_schema')\nand n.nspname = 'idb'\norder by function_schema, function_name;"
  },
  {
    "objectID": "docs/development/code/sql.html#whos-logged-in",
    "href": "docs/development/code/sql.html#whos-logged-in",
    "title": "SQL",
    "section": "Whos logged in",
    "text": "Whos logged in\nselect * from pg_stat_activity\nwhere usename != '' and usename != 'postgres'\norder by usename, pid"
  },
  {
    "objectID": "docs/development/code/sql.html#aggregate-functions",
    "href": "docs/development/code/sql.html#aggregate-functions",
    "title": "SQL",
    "section": "Aggregate Functions",
    "text": "Aggregate Functions\nhttps://www.postgresql.org/docs/9.6/catalog-pg-aggregate.html\n-- pg_proc contains data for aggregate functions as well as plain functions\nselect * from pg_proc\n-- pg_aggregate is an extension of pg_proc.\nselect * from pg_aggregate"
  },
  {
    "objectID": "docs/development/code/sql.html#list-users",
    "href": "docs/development/code/sql.html#list-users",
    "title": "SQL",
    "section": "List users",
    "text": "List users\nSELECT rolname FROM pg_roles;"
  },
  {
    "objectID": "docs/development/code/sql.html#update-from",
    "href": "docs/development/code/sql.html#update-from",
    "title": "SQL",
    "section": "Update From",
    "text": "Update From\nUPDATE tablename\nSET columnname = someothervalue\nFROM ...\nWHERE ..."
  },
  {
    "objectID": "docs/development/code/sql.html#materialized-view",
    "href": "docs/development/code/sql.html#materialized-view",
    "title": "SQL",
    "section": "Materialized View",
    "text": "Materialized View\nReference\nCREATE MATERIALIZED VIEW view_name\nAS\nquery\nWITH [NO] DATA;\nWhen you refresh data for a materialized view, PostgreSQL locks the entire table therefore you cannot query data against it. To avoid this, you can use the CONCURRENTLY option.\nWith CONCURRENTLY option, PostgreSQL creates a temporary updated version of the materialized view, compares two versions, and performs INSERT and UPDATE only the differences.\nREFRESH MATERIALIZED VIEW CONCURRENTLY view_name;"
  },
  {
    "objectID": "docs/development/code/sql.html#constants",
    "href": "docs/development/code/sql.html#constants",
    "title": "SQL",
    "section": "Constants",
    "text": "Constants\nWITH myconstants (analyte_search) as (\n   values ('%Hexachlorocyclopentadiene%')\n)\n\nSELECT *\nFROM e_analyte, myconstants\nWHERE analyte ilike analyte_search\n   OR full_name ilike analyte_search\n   OR aliases ilike analyte_search;"
  },
  {
    "objectID": "docs/development/code/sql.html#sequential-keys",
    "href": "docs/development/code/sql.html#sequential-keys",
    "title": "SQL",
    "section": "Sequential Keys",
    "text": "Sequential Keys\nseq_key bigint NOT NULL DEFAULT nextval('seq_key'::regclass)\n\nALTER SEQUENCE seq_key RESTART WITH 3;"
  },
  {
    "objectID": "docs/development/code/sql.html#cross-database-search",
    "href": "docs/development/code/sql.html#cross-database-search",
    "title": "SQL",
    "section": "Cross-Database Search",
    "text": "Cross-Database Search\nThese could be refined further by creating a function.\n\nAnalytes\nwith\nconst (param) as (\n    values ('%solid%')\n),\ndbrows as (\n   select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte\n  union\n    select * from dblink(\n        'dbname=chemcrit',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=ahtna',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=arkema_ph',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=bae_north',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=bayer_ldw',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=bcsa',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=c840_livent',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=cabotroad',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=centralia',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=centredale',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=eos',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=evraz_inwater',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=frenchtown_mill',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_columbus',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n    'dbname=gemt_meridian',\n    'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_springfield',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=reddog',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=shoreham',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=solvay',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=three_m_mb',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'     \n    ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n)\nselect \n    analyte, full_name, chem_class, cas_rn, aliases, \n    count(*) as num_instances, string_agg(db, '; ') as db\nfrom dbrows, const\nwhere analyte ilike param\n   or full_name ilike param\n   or aliases ilike param\ngroup by analyte, full_name, chem_class, cas_rn, aliases\norder by chem_class, analyte;\n\n\nAnalytical Methods\nwith\nconst (param) as (\n    values ('%SW8260%')\n),\ndbrows as (\n   select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod\n  union\n    select * from dblink(\n        'dbname=ahtna',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=arkema_ph',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=bae_north',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=bayer_ldw',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=bcsa',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=c840_livent',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=cabotroad',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=centralia',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=centredale',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=eos',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=evraz_inwater',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=frenchtown_mill',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_columbus',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_meridian',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=gemt_springfield',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=reddog',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=shoreham',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=solvay',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n  union\n    select * from dblink(\n        'dbname=three_m_mb',\n        'select method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method, current_database() as db from e_analmethod'\n    ) as d (method_code text, description text, lab_prep_method text, lab_extraction_method text, lab_anal_method text, lab_leach_method text, db text)\n)\nselect \n    method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method,\n    count(*) as num_instances, string_agg(db, '; ') as db\nfrom dbrows, const\nwhere method_code ilike param\n   or description ilike param\n   or lab_prep_method ilike param\n   or lab_extraction_method ilike param\n   or lab_anal_method ilike param\n   or lab_leach_method ilike param\ngroup by method_code, description, lab_prep_method, lab_extraction_method, lab_anal_method, lab_leach_method\norder by method_code;"
  },
  {
    "objectID": "docs/development/code/sql.html#wipe-schema-tables",
    "href": "docs/development/code/sql.html#wipe-schema-tables",
    "title": "SQL",
    "section": "Wipe Schema Tables",
    "text": "Wipe Schema Tables\n-- DROP FUNCTION IF EXISTS idb.wipe_staging();\nCREATE OR REPLACE FUNCTION idb.wipe_staging()\nRETURNS TABLE(staging_schema text, deleted_tables integer) \nLANGUAGE 'plpgsql'\nCOST 100\nVOLATILE PARALLEL UNSAFE\nROWS 1000\nAS $BODY$\n#variable_conflict use_column\nDECLARE\n  staging_schema TEXT;\n  table_name TEXT;\n  deleted_tables INTEGER := 0;\nBEGIN\nstaging_schema = (select 'stg_' || user)::text;\nFOR table_name IN (\n    SELECT table_name \n    FROM information_schema.tables\n    WHERE table_schema = staging_schema\n)\nLOOP\nEXECUTE format('DROP TABLE %I.%I CASCADE', staging_schema, table_name );\ndeleted_tables := deleted_tables + 1;\nEND LOOP;\nRETURN query select staging_schema, deleted_tables;\nEND;\n$BODY$;\nALTER FUNCTION idb.wipe_staging()\n    OWNER TO envdb_dm;"
  },
  {
    "objectID": "docs/development/code/sql.html#create-a-read-only-user-1",
    "href": "docs/development/code/sql.html#create-a-read-only-user-1",
    "title": "SQL",
    "section": "Create a read-only user",
    "text": "Create a read-only user\ngrant connect on database db_name to user;\ngrant usage on schema schema_name to user;\ngrant select on all tables in schema schema_name to user;\nalter default privileges in schema schema_name grant select on tables to user;"
  },
  {
    "objectID": "docs/development/code/sql.html#sandbox",
    "href": "docs/development/code/sql.html#sandbox",
    "title": "SQL",
    "section": "Sandbox",
    "text": "Sandbox\n\nBasic Table Joins\n\nSELECT * \nFROM regions \nINNER JOIN countries ON regions.region_id=countries.region_id \nINNER JOIN locations ON countries.country_id=locations.country_id;\n\n\n\nAggregating Data\n\nSELECT regions.region_name, count(distinct countries.country_id) as number_of_countries\nFROM regions \nINNER JOIN countries ON regions.region_id=countries.region_id \nINNER JOIN locations ON countries.country_id=locations.country_id\nGROUP BY regions.region_name;"
  },
  {
    "objectID": "docs/development/server/docker.html",
    "href": "docs/development/server/docker.html",
    "title": "Docker",
    "section": "",
    "text": "Check out additional Docker examples here.\n\n\nSteps are pulled from a Digital Ocean tutorial.\n\nUpdate list of packages and install prerequisites.\nsudo apt update && \\\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\nAdd the GPG key for the official Docker repository.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nAdd the Docker repository to APT sources and update package list from the new repo.\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"\nMake sure install is pulling from Docker repo instead of default Ubuntu.\napt-cache policy docker-ce\nInstall Docker.\nsudo apt install docker-ce\nCheck that Docker is running.\nsudo systemctl status docker\nExecute Docker commands without sudo.\nsudo usermod -aG docker ${USER} && \\\nsu - ${USER}\nVerify user is now aded to the docker group.\ngroups"
  },
  {
    "objectID": "docs/development/server/docker.html#docker",
    "href": "docs/development/server/docker.html#docker",
    "title": "Docker",
    "section": "",
    "text": "Check out additional Docker examples here.\n\n\nSteps are pulled from a Digital Ocean tutorial.\n\nUpdate list of packages and install prerequisites.\nsudo apt update && \\\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\nAdd the GPG key for the official Docker repository.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nAdd the Docker repository to APT sources and update package list from the new repo.\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"\nMake sure install is pulling from Docker repo instead of default Ubuntu.\napt-cache policy docker-ce\nInstall Docker.\nsudo apt install docker-ce\nCheck that Docker is running.\nsudo systemctl status docker\nExecute Docker commands without sudo.\nsudo usermod -aG docker ${USER} && \\\nsu - ${USER}\nVerify user is now aded to the docker group.\ngroups"
  },
  {
    "objectID": "docs/development/server/jupyter-server.html",
    "href": "docs/development/server/jupyter-server.html",
    "title": "Jupyter Server",
    "section": "",
    "text": "Create jupyter user\nsudo adduser jupyter && \\\nsudo usermod -a -G staff jupyter\nsudo su jupyter && \\\nInstall Jupyter Lab\nsource /home/jupyter/.venv/bin/activate && \\\npython -m pip install jupyterlab && \\\njupyter-lab --generate-config\nConfigure Jupyter\nc.NotebookApp.ip = \"*\"\nc.NotebookApp.notebook_dir = \"/home/jupyter/notebooks/\"\nc.NotebookApp.open_browser = False\nc.NotebookApp.password = \"\"  # hashed password\nc.NotebookApp.port = 9999\nConfigure Apache:\n&lt;VirtualHost *:80&gt;\n    ServerName &lt;DNS ENTRY&gt;\n    ServerSignature Off\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n\n    ProxyPreserveHost On\n    ProxyPass \"/\" \"http://localhost:9999/\"\n    ProxyPassReverse \"/\" \"http://localhost:9999/\"\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =&lt;DNS ENTRY&gt;\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n&lt;/VirtualHost&gt;\n\n&lt;IfModule mod_ssl.c&gt;\n    &lt;VirtualHost *:443&gt;\n        ServerName &lt;DNS ENTRY&gt;\n        ServerSignature Off\n\n        ErrorLog /var/log/apache2/redirect.error.log\n        LogLevel warn\n\n        ProxyPreserveHost On\n        ProxyPass \"/\" \"http://localhost:9999/\"\n        ProxyPassReverse \"/\" \"http://localhost:9999/\"\n\n    &lt;Location \"/api/kernels/\"&gt;\n        ProxyPass        ws://localhost:9999/api/kernels/\n            ProxyPassReverse ws://localhost:9999/api/kernels/\n    &lt;/Location&gt;\n\n        SSLCertificateFile /etc/letsencrypt/live/&lt;DNS ENTRY&gt;/fullchain.pem\n        SSLCertificateKeyFile /etc/letsencrypt/&lt;DNS ENTRY&gt;/privkey.pem\n        Include /etc/letsencrypt/options-ssl-apache.conf\n    &lt;/VirtualHost&gt;\n&lt;/IfModule&gt;\nEnable Apache modules\nsudo a2enmod proxy\nsudo a2enmod proxy_http\nsudo a2enmod proxy_wstunnel\nGenerate SSL certs\nsudo certbot --apache certonly\nEnable the site\nsudo a2ensite jupyter.conf && \\\nsudo systemctl reload apache2 && \\\nsudo systemctl status apache2\nCreate the Jupyter service: /lib/systemd/system/jupyter.service\n# service name:     jupyter.service\n# path:             /lib/systemd/system/jupyter.service\n\n[Unit]\nDescription=Jupyter Notebook Server\n\n[Service]\nType=simple\nPIDFile=/run/jupyter.pid\nExecStart=/bin/bash -c \"/home/jupyter/.venv/bin/jupyter lab --no-browser\"\nUser=jupyter\nGroup=staff\nWorkingDirectory=/home/jupyter/notebooks\nRestart=always\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target\nEnable the service\nsudo systemctl daemon-reload && \\\nsudo systemctl start jupyter.service && \\\nsudo service jupyter status\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/development/server/rstudio-server.html",
    "href": "docs/development/server/rstudio-server.html",
    "title": "Rstudio Server",
    "section": "",
    "text": "Install:\nwget https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2022.07.2-576-amd64.deb && \\\nsudo gdebi rstudio-server-2022.07.2-576-amd64.deb && \\\nrm rstudio-server-2022.07.2-576-amd64.deb && \\\nsudo adduser rstudio\nApache config\n&lt;VirtualHost *:80&gt;\n    ServerName &lt;DNS ENTRY&gt;\n    ServerSignature Off\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n\n    ProxyPreserveHost On\n    ProxyPass \"/\" \"http://localhost:8787/\"\n    ProxyPassReverse \"/\" \"http://localhost:8787/\"\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =&lt;DNS ENTRY&gt;\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n&lt;/VirtualHost&gt;\n\n&lt;IfModule mod_ssl.c&gt;\n    &lt;VirtualHost *:443&gt;\n        ServerName &lt;DNS ENTRY&gt;\n        ServerSignature Off\n\n        ErrorLog /var/log/apache2/redirect.error.log\n        LogLevel warn\n\n        ProxyPreserveHost On\n        ProxyPass \"/\" \"http://localhost:8787/\"\n        ProxyPassReverse \"/\" \"http://localhost:8787/\"\n\n        SSLCertificateFile /etc/letsencrypt/live/&lt;DNS ENTRY&gt;/fullchain.pem\n        SSLCertificateKeyFile /etc/letsencrypt/live/&lt;DNS ENTRY&gt;/privkey.pem\n        Include /etc/letsencrypt/options-ssl-apache.conf\n    &lt;/VirtualHost&gt;\n&lt;/IfModule&gt;\nGenerate SSL certs\nsudo certbot --apache certonly\nEnable the site\nsudo a2ensite rstudio.conf && \\\nsudo systemctl reload apache2 && \\\nsudo systemctl status apache2\n\n\n\n\n Back to top"
  },
  {
    "objectID": "docs/resources/bookmarks.html",
    "href": "docs/resources/bookmarks.html",
    "title": "Bookmarks",
    "section": "",
    "text": "devdocs.io - Searchable documentations\nss64 - CLI reference guide\nReadTheDocs - Create, host, and browse documentation\nexecsql - Run SQL with metacommands\nBootstrap - Web framework\npgAdmin - PostgreSQL sandbox\nRegex - Regular expressions\nGoogle Colab - Collaborative Python notebooks\ngeojson.io - Create, view, and share maps\nrepl.it - Collaborative in-browser IDE. 50+ languages\nIntegromat - Online scenario automation\nPostgreSQL cheatsheet - PostgreSQL cheatsheet\npython-utils - Playground for Python utilities\nSpektran - Collection of useful color tools\nObservable - JavaScript notebooks\nHTML Dog - HTML tutorials\nCrontab - Crontab scheduler\nMedia Library\nUTF-8 Character Debug - UTF-8 character debugging chart\nLaTeX Basics\nQuarto - Scientific and technical publishing system built on Pandoc"
  },
  {
    "objectID": "docs/resources/bookmarks.html#developer",
    "href": "docs/resources/bookmarks.html#developer",
    "title": "Bookmarks",
    "section": "",
    "text": "devdocs.io - Searchable documentations\nss64 - CLI reference guide\nReadTheDocs - Create, host, and browse documentation\nexecsql - Run SQL with metacommands\nBootstrap - Web framework\npgAdmin - PostgreSQL sandbox\nRegex - Regular expressions\nGoogle Colab - Collaborative Python notebooks\ngeojson.io - Create, view, and share maps\nrepl.it - Collaborative in-browser IDE. 50+ languages\nIntegromat - Online scenario automation\nPostgreSQL cheatsheet - PostgreSQL cheatsheet\npython-utils - Playground for Python utilities\nSpektran - Collection of useful color tools\nObservable - JavaScript notebooks\nHTML Dog - HTML tutorials\nCrontab - Crontab scheduler\nMedia Library\nUTF-8 Character Debug - UTF-8 character debugging chart\nLaTeX Basics\nQuarto - Scientific and technical publishing system built on Pandoc"
  },
  {
    "objectID": "docs/resources/bookmarks.html#gis",
    "href": "docs/resources/bookmarks.html#gis",
    "title": "Bookmarks",
    "section": "GIS",
    "text": "GIS\n\nepsg.io - Spatial reference systems\nArcGIS - ArcGIS Python"
  },
  {
    "objectID": "docs/resources/bookmarks.html#music",
    "href": "docs/resources/bookmarks.html#music",
    "title": "Bookmarks",
    "section": "Music",
    "text": "Music\n\nSongsterr - Guitar tabs\nUltimate Guitar - Guitar tabs\nFlagrantior - Music theory\nmusictheory.net - Music theory\nTeoria - Music theory\nJustinGuitar - Guitar lessons\nHowToPlayPiano - Piano lessons\nSoundation - Broswer based music maker"
  },
  {
    "objectID": "docs/resources/bookmarks.html#e-books",
    "href": "docs/resources/bookmarks.html#e-books",
    "title": "Bookmarks",
    "section": "e-Books",
    "text": "e-Books\n\nProject Gutenberg\nLibriVox\nStandard eBooks"
  },
  {
    "objectID": "docs/resources/bookmarks.html#miscellaneous",
    "href": "docs/resources/bookmarks.html#miscellaneous",
    "title": "Bookmarks",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nFree Learning - List of free educational resources"
  },
  {
    "objectID": "docs/resources/index.html",
    "href": "docs/resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nTitle\n\n\nDescription\n\n\nReading Time\n\n\n\n\n\n\n\n\n\nBookmarks\n\n\nBookmarks to miscellaneous sites.\n\n\n1 min\n\n\n\n\n\n\n\nData Management\n\n\nData management guidelines and resources\n\n\n23 min\n\n\n\n\n\n\n\nRecipes\n\n\nRecipies\n\n\n5 min\n\n\n\n\n\n\n\nWorkouts\n\n\nWorkouts and routines\n\n\n1 min\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "docs/resources/workouts.html",
    "href": "docs/resources/workouts.html",
    "title": "Workouts",
    "section": "",
    "text": "Code\nimport pandas as pd\n\nworkouts = \"../../static/resources/workouts.xlsx\"\nw1 = pd.read_excel(workouts, sheet_name=0)\nw2 = pd.read_excel(workouts, sheet_name=1)\nw3 = pd.read_excel(workouts, sheet_name=2)\nw4 = pd.read_excel(workouts, sheet_name=3)\nschedule = pd.read_excel(workouts, sheet_name=4)\n\n\nThe following regimen outlines a 4 week lift cycle. The routine is a slightly modified version of this. Non-lift days should be supplemented with active recovery workouts.\n\n\n\n\nCode\nschedule.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nWeek\n\n\n\nSunday\n\n\n\nMonday\n\n\n\nTuesday\n\n\n\nWednesday\n\n\n\nThursday\n\n\n\nFriday\n\n\n\nSaturday\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nBack & Biceps\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nChest & Triceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n2\n\n\n\nCardio\n\n\n\nShoulders & Traps, Run\n\n\n\nChest & Triceps\n\n\n\nBack & Biceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n3\n\n\n\nCardio\n\n\n\nBack & Biceps, Run\n\n\n\nShoulders & Traps\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n4\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw1.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nDeadlift\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nPull-Ups\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nLat Pull Downs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nRows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFace Pulls\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBarbell Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\n60 seconds\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n15\n\n\n\n\n\n\n\nSit Ups\n\n\n\n3\n\n\n\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw2.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBench Press\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nIncline Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFlys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDips\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nTricep Pushdowns\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nAb Roller\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nSitting Twists\n\n\n\n3\n\n\n\n30\n\n\n\n\n\n\n\nWipers\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw3.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBack Squat\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nFront Squat\n\n\n\n4\n\n\n\n6\n\n\n\n\n\n\n\nLunges\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nKettelbell Deadlift\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nLeg Kickbacks\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nGlute Bridges\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw4.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nMilitary Press\n\n\n\n4\n\n\n\n12, 8, 5, 3\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nRear Delt Flys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nBarbell Shrugs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nOne Arm Dumbbell Snatch\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nKettlebell Swing\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nFarmers Carry\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nScissor Kicks\n\n\n\n3\n\n\n\n30 seconds\n\n\n\n\n\n\n\nL-sit\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nToe Taps\n\n\n\n3\n\n\n\n20"
  },
  {
    "objectID": "docs/resources/workouts.html#routines",
    "href": "docs/resources/workouts.html#routines",
    "title": "Workouts",
    "section": "",
    "text": "Code\nimport pandas as pd\n\nworkouts = \"../../static/resources/workouts.xlsx\"\nw1 = pd.read_excel(workouts, sheet_name=0)\nw2 = pd.read_excel(workouts, sheet_name=1)\nw3 = pd.read_excel(workouts, sheet_name=2)\nw4 = pd.read_excel(workouts, sheet_name=3)\nschedule = pd.read_excel(workouts, sheet_name=4)\n\n\nThe following regimen outlines a 4 week lift cycle. The routine is a slightly modified version of this. Non-lift days should be supplemented with active recovery workouts.\n\n\n\n\nCode\nschedule.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nWeek\n\n\n\nSunday\n\n\n\nMonday\n\n\n\nTuesday\n\n\n\nWednesday\n\n\n\nThursday\n\n\n\nFriday\n\n\n\nSaturday\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nBack & Biceps\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nChest & Triceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n2\n\n\n\nCardio\n\n\n\nShoulders & Traps, Run\n\n\n\nChest & Triceps\n\n\n\nBack & Biceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n3\n\n\n\nCardio\n\n\n\nBack & Biceps, Run\n\n\n\nShoulders & Traps\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n4\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw1.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nDeadlift\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nPull-Ups\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nLat Pull Downs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nRows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFace Pulls\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBarbell Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\n60 seconds\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n15\n\n\n\n\n\n\n\nSit Ups\n\n\n\n3\n\n\n\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw2.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBench Press\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nIncline Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFlys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDips\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nTricep Pushdowns\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nAb Roller\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nSitting Twists\n\n\n\n3\n\n\n\n30\n\n\n\n\n\n\n\nWipers\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw3.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBack Squat\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nFront Squat\n\n\n\n4\n\n\n\n6\n\n\n\n\n\n\n\nLunges\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nKettelbell Deadlift\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nLeg Kickbacks\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nGlute Bridges\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw4.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nMilitary Press\n\n\n\n4\n\n\n\n12, 8, 5, 3\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nRear Delt Flys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nBarbell Shrugs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nOne Arm Dumbbell Snatch\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nKettlebell Swing\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nFarmers Carry\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nScissor Kicks\n\n\n\n3\n\n\n\n30 seconds\n\n\n\n\n\n\n\nL-sit\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nToe Taps\n\n\n\n3\n\n\n\n20"
  },
  {
    "objectID": "docs/resources/workouts.html#introductory-routines",
    "href": "docs/resources/workouts.html#introductory-routines",
    "title": "Workouts",
    "section": "Introductory Routines",
    "text": "Introductory Routines\n\n\nCode\nw5 = pd.read_excel(workouts, sheet_name=5)\nw6 = pd.read_excel(workouts, sheet_name=6)\nw7 = pd.read_excel(workouts, sheet_name=7)\n\n\n\nPush\n\n\nCode\nw5.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nDumbbell Bench Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbell Incline Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Flys\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nTricep Push Down\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\nFailure\n\n\n\nView\n\n\n\n\n\n\n\nSit-ups\n\n\n\n3\n\n\n\n12\n\n\n\nView\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\nPull\n\n\nCode\nw6.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nCable Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFarmer Carry\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\nLegs\n\n\nCode\nw6.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nCable Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFarmer Carry\n\n\n\n3\n\n\n\n10\n\n\n\nView"
  }
]