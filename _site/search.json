[
  {
    "objectID": "content/development/code/notebooks/web-scraping.html",
    "href": "content/development/code/notebooks/web-scraping.html",
    "title": "Web Scraping",
    "section": "",
    "text": "# Standard libraries\nimport os\nimport sys\nimport requests\nimport datetime\nimport urllib.request\n\n# Third party packages\nfrom bs4 import BeautifulSoup\n\n\n# WAVEWATCH III web server base URL\nbase_url = \"https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/\"\n\n\n# Remove logs and data files at termination of this script.\ncleanup = True\n\n\n# Send an HTTP request\ndef page_request(url):\n    page = requests.get(url)\n    if not page.ok:\n        print(\"Error reaching URL: {}\".format(url))\n        print(\"Page returned status code &lt;{}&gt;\".format(page.status_code))\n    else:\n        return page\n\n\ndef do_cleanup():\n    if os.path.exists(logfile):\n        os.remove(logfile)\n\n    import shutil\n\n    if os.path.exists(\"./data/\"):\n        shutil.rmtree(\"./data/\")\n\n\n# Convert the requested page content to a Beautiful Soup object\nsoup = BeautifulSoup(page_request(base_url).content, \"html.parser\")\n\n\n# Search for all the anchor tags with an HREF. Each tag represents a data subdirectory. Discard the first and last anchor tag [0 = parent dir, -1 = reference].\nlinks = soup.find_all(\"a\", href=True)[1:-1]\ngrib_dirs = dict(map(lambda x: (x[\"href\"], base_url + x[\"href\"] + \"/gribs\"), links))\n\n# Only want the latest years data (for now)\ngrib_dirs = dict(map(lambda x: x, list(grib_dirs.items())[-12:]))\n\n\ngribs = {}\nfor grib_dir in grib_dirs:\n    grib_tags = BeautifulSoup(\n        page_request(grib_dirs[grib_dir]).content, \"html.parser\"\n    ).find_all(\"a\", href=True)[1:]\n    grib_links = dict(\n        map(lambda x: (x[\"href\"], grib_dirs[grib_dir] + \"/\" + x[\"href\"]), grib_tags)\n    )\n    gribs.update({grib_dir: grib_links})\n\n\nlogfile = \"logfile_{}.log\".format(datetime.datetime.now().strftime(\"%Y%m%d\"))\nwith open(logfile, \"w\") as log:\n    log.write(\"Date;Last_Updated;Content_Size_MB;URL;Data_Dir;Filename\\n\")\n\n\n# Loop through data directories separated by month\nfor month in gribs:\n    # Loop through each file in the data directory\n    i = 0\n    for grib in gribs[month]:\n        # Only interested in two gribs for now\n        if not (\"glo_30m\" in grib or \"ecg_10m\" in grib):\n            continue\n        print(\"Retrieving file: {}\".format(gribs[month][grib]))\n        # Path to save grib file\n        data_dir = os.path.join(os.getcwd(), \"data\", month)\n        # Make directory to save file\n        if not os.path.exists(data_dir):\n            os.makedirs(data_dir)\n        # Get header info from URL endpoint\n        meta = urllib.request.urlopen(gribs[month][grib]).info()\n        # Write some information to a log file\n        with open(logfile, \"a\") as log:\n            log.write(\n                \"{};{};{};{};{};{}\\n\".format(\n                    meta[\"Date\"],\n                    meta[\"Last-Modified\"],\n                    int(meta[\"Content-Length\"]) / 1000000,\n                    gribs[month][grib],\n                    data_dir,\n                    grib,\n                )\n            )\n        # Send a request to the url and save the response file\n        urllib.request.urlretrieve(gribs[month][grib], os.path.join(data_dir, grib))\n\n        # Only download a couple files for development purposes. Remove in production\n        if i &gt;= 3:\n            break\n        else:\n            i += 1\n    break  # Remove if in production\n\nif cleanup:\n    do_cleanup()\n\nRetrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.dp.200901.grb2\nRetrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.hs.200901.grb2\nRetrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.tp.200901.grb2\nRetrieving file: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2/200901/gribs/multi_reanal.ecg_10m.wind.200901.grb2"
  },
  {
    "objectID": "content/development/code/notebooks/sleep-data.html",
    "href": "content/development/code/notebooks/sleep-data.html",
    "title": "Sleep Data",
    "section": "",
    "text": "Sleep Cycle\nimport os\nimport re\nimport pandas as pd\nimport psycopg2\nfrom sqlalchemy import create_engine\nsleepdata = \"./data/sleepdata.csv\""
  },
  {
    "objectID": "content/development/code/notebooks/sleep-data.html#database-credentials",
    "href": "content/development/code/notebooks/sleep-data.html#database-credentials",
    "title": "Sleep Data",
    "section": "Database credentials",
    "text": "Database credentials\n\nwith open(\"../../postgres.txt\", \"r\") as f:\n    user, pwd = [s.strip() for s in f.readlines()]"
  },
  {
    "objectID": "content/development/code/notebooks/sleep-data.html#create-dataframe",
    "href": "content/development/code/notebooks/sleep-data.html#create-dataframe",
    "title": "Sleep Data",
    "section": "Create DataFrame",
    "text": "Create DataFrame\n\ndf = pd.read_csv(sleepdata, delimiter=\";\")\ndf.dtypes\n\nStart                           object\nEnd                             object\nSleep Quality                   object\nRegularity                      object\nMood                           float64\nHeart rate (bpm)                 int64\nSteps                            int64\nAlarm mode                      object\nAir Pressure (Pa)              float64\nCity                            object\nMovements per hour             float64\nTime in bed (seconds)          float64\nTime asleep (seconds)          float64\nTime before sleep (seconds)    float64\nWindow start                    object\nWindow stop                     object\nDid snore                         bool\nSnore time                     float64\nWeather temperature (°F)       float64\nWeather type                    object\nNotes                           object\ndtype: object"
  },
  {
    "objectID": "content/development/code/notebooks/sleep-data.html#create-db-safe-column-names",
    "href": "content/development/code/notebooks/sleep-data.html#create-db-safe-column-names",
    "title": "Sleep Data",
    "section": "Create DB Safe Column Names",
    "text": "Create DB Safe Column Names\n\nnonword_pattern = re.compile(r\"[^\\w]\")\nspacing_pattern = re.compile(r\"[_]{2,}\")\nending_pattern = re.compile(r\"_$\")\ndf.columns = [\n    re.sub(\n        ending_pattern,\n        \"\",\n        re.sub(spacing_pattern, \"_\", re.sub(nonword_pattern, \"_\", col)),\n    ).lower()\n    for col in df.columns\n]\ndf.columns\n\nIndex(['start', 'end', 'sleep_quality', 'regularity', 'mood', 'heart_rate_bpm',\n       'steps', 'alarm_mode', 'air_pressure_pa', 'city', 'movements_per_hour',\n       'time_in_bed_seconds', 'time_asleep_seconds',\n       'time_before_sleep_seconds', 'window_start', 'window_stop', 'did_snore',\n       'snore_time', 'weather_temperature_f', 'weather_type', 'notes'],\n      dtype='object')\n\n\n\ndb_host = \"localhost\"\ndb_table = \"sleepdata\"\ndb_name = \"personal\"\n\nengine = create_engine(f\"postgresql://{user}:{pwd}@{db_host}/{db_name}\")\n\ndf.to_sql(f\"{db_table}\", engine, if_exists=\"replace\", index=False)\n\n509"
  },
  {
    "objectID": "content/development/code/notebooks/pyspark-practice.html",
    "href": "content/development/code/notebooks/pyspark-practice.html",
    "title": "PySpark Practice",
    "section": "",
    "text": "This is intended to run on Google Colab\nInstall everything necessary to make spark work.\n\n!apt-get install openjdk-11-jdk-headless -qq &gt; /dev/null\n!wget -q https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n!tar xf spark-3.0.3-bin-hadoop2.7.tgz\n!pip install -q findspark\n\nSet the paths to the installs\n\nimport os\n\nos.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\nos.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\"\n\nFind the spark installation\n\nimport findspark\n\nfindspark.init()\n\nStart doing fancy pyspark stuff\n\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql.types import (\n    StructType,\n    StructField,\n    StringType,\n    DoubleType,\n    IntegerType,\n    ArrayType,\n)\nfrom pyspark.sql import DataFrame, SparkSession\n\nimport json\n\nRequest Orders.json from google drive via command line.\n\n!wget -q --no-check-certificate 'https://drive.google.com/uc?export=download&id=1I6VuRILNtyhnWMUml61Dv58YOP2dqlvx' -O 'Orders.json'\n\nOr, do it the Python way.\n\n# from google.colab import drive\n# drive.mount('/content/drive')\n# INPUT_FILE = '/content/drive/MyDrive/Colab Notebooks/Starbucks/1_basic_exercise/resources/Order.json'\n\nSet input/output variables\n\nINPUT_FILE = \"/content/Orders.json\"  # TODO: Change this based on actual location for your environment setup\nOUTPUT_CSV_FILE = \"./output/files/output.csv\"\nOUTPUT_DELTA_PATH = \"./output/delta/\"\n\nJSON data summary:\n\nEach list element is an event\nEvents contain a message about an order\nOrders contain a list of items that contain attributes about the item\nItems can also contain a lists of items (childItems, discounts)\n\nCreate spark session\n\nspark = (\n    SparkSession.builder.appName(\"programming\")\n    .master(\"local\")\n    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.7.0\")\n    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n    .config(\n        \"spark.sql.catalog.spark_catalog\",\n        \"org.apache.spark.sql.delta.catalog.DeltaCatalog\",\n    )\n    .config(\"spark.ui.port\", \"4050\")\n    .getOrCreate()\n)\n\n\nfrom traitlets.traitlets import default\n\n\ndef read_json(file_path: str, schema: StructType) -&gt; DataFrame:\n    \"\"\"\n    The goal of this method is to parse the input json data using the schema from another method.\n\n    We are only interested in data starting at orderPaid attribute.\n\n    :param file_path: Order.json will be provided\n    :param schema: schema that needs to be passed to this method\n    :return: Dataframe containing records from Order.json\n    \"\"\"\n    # Only interested in data starting at orderPaid\n    with open(file_path) as f:\n        js = json.load(f)[0][\"data\"][\"message\"][\"orderPaid\"]\n\n    # Create Dataframe\n    #   - Use spark JSON method for reading object\n    #   - Use parallelize on array object, which contains json structured data\n    #   - Use custom schema so data type/structure is defined instead of inferred\n    df = spark.read.json(\n        spark.sparkContext.parallelize([js]), schema=schema[\"order_paid_type\"]\n    )\n    return df\n\nThe schema outlined below represents a “one-to-many” relationship and is defined in a bottom-up fashion.\n\ndef get_struct_type() -&gt; StructType:\n    \"\"\"\n    Build a schema based on the the file Order.json\n\n    :return: Structype of equivalent JSON schema\n    \"\"\"\n    discount_type = StructType(\n        [\n            StructField(\"amount\", IntegerType(), True),\n            StructField(\"description\", StringType(), True),\n        ]\n    )\n\n    child_item_type = StructType(\n        [\n            StructField(\"lineItemNumber\", StringType(), True),\n            StructField(\"itemLabel\", StringType(), True),\n            StructField(\"quantity\", DoubleType(), True),\n            StructField(\"price\", IntegerType(), True),\n            StructField(\n                \"discounts\", ArrayType(discount_type), True\n            ),  # Changed \"TODO --&gt; ArrayType(discount_type). Will inherit discout_type attributes.\n        ]\n    )\n\n    item_type = StructType(\n        [\n            StructField(\"lineItemNumber\", StringType(), True),\n            StructField(\"itemLabel\", StringType(), True),\n            StructField(\"quantity\", DoubleType(), True),\n            StructField(\"price\", IntegerType(), True),\n            StructField(\n                \"discounts\", ArrayType(discount_type), True\n            ),  # Changed \"TODO\" --&gt; ArrayType(discount_type). Will inherit discount_type attributes.\n            StructField(\n                \"childItems\", ArrayType(child_item_type), True\n            ),  # Changed \"TODO\" --&gt; ArrayType(child_item_type). Will inherit chile_item_type attributes.\n        ]\n    )\n\n    order_paid_type = StructType(\n        [\n            StructField(\"orderToken\", StringType(), True),\n            StructField(\"preparation\", StringType(), True),\n            StructField(\n                \"items\", ArrayType(item_type), True\n            ),  # Changed \"TODO\" --&gt; ArrayType(item_type). Will inherit item_type attributes.\n        ]\n    )\n\n    message_type = StructType(\n        [StructField(\"orderPaid\", order_paid_type, True)]\n    )  # Changed \"TODO\" --&gt; order_paid_type. Will inherit order_paid_type attributes.\n\n    data_type = StructType(\n        [StructField(\"message\", message_type, True)]\n    )  # Changed \"TODO\" --&gt; message_type. Will inherit message_type attributes.\n\n    body_type = StructType(\n        [\n            StructField(\"id\", StringType(), True),\n            StructField(\"subject\", StringType(), True),\n            StructField(\n                \"data\", data_type, True\n            ),  # Changed \"TODO\" --&gt; data_type. Will inherit data_type attributes.\n            StructField(\"eventTime\", StringType(), True),\n        ]\n    )\n    return {\n        \"body_type\": body_type,\n        \"data_type\": data_type,\n        \"message_type\": message_type,\n        \"order_paid_type\": order_paid_type,\n        \"item_type\": item_type,\n        \"child_item_type\": child_item_type,\n        \"discount_type\": discount_type,\n    }\n\n\ndef get_rows_from_array(df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Input data frame contains columns of type array. Identify those columns and convert them to rows.\n\n    :param df: Contains column with data type of type array.\n    :return: The dataframe should not contain any columns of type array\n    \"\"\"\n    # explode will create a new row for each element in an array\n    from pyspark.sql.functions import explode\n\n    # Iterate over field names\n    for i, f in enumerate(df.schema.fields):\n        # Check datatype of field\n        if isinstance(f.dataType, ArrayType):\n            arrayCol = f.name\n\n    # Overwrite dataframe object\n    # Create a new row for every element in \"arrayCol\".\n    # Each new row will contain the same values from\n    #   columns that are not \"arrayCol\".\n    # Use \"withColumn()\" to transform dataframe.\n    #   First argument - What column will be transformed (will overwrite because already exists)\n    #   Second argument - Expression to create/modify values for the column\n    df = df.withColumn(arrayCol, explode(arrayCol))\n    return df\n\n\ndef get_unwrapped_nested_structure(df: DataFrame) -&gt; DataFrame:\n    \"\"\"\n    Convert columns that contain multiple attributes to columns of their own\n\n    :param df: Contains columns that have multiple attributes\n    :return: Dataframe should not contain any nested structures\n    \"\"\"\n\n    def parse_struct(df):\n        \"\"\"Create an array of columns to be selected.\n        If column type = StructType, select all attributes\n        in that StructType as additional columns.\n        Returns list of columns.\"\"\"\n        cols = []\n        for i, d in enumerate(df.schema.fields):\n            if isinstance(d.dataType, StructType):\n                cols.append(f\"{d.name}.*\")\n            else:\n                cols.append(d.name)\n        return cols\n\n    df = df.select(parse_struct(df))\n\n    # Check for columns of type Array.\n    # If type Array, transform elements to rows\n    arrayCols = [c.name for c in df.schema.fields if isinstance(c.dataType, ArrayType)]\n    if len(arrayCols) &gt; 0:\n        for col in arrayCols:\n            df = get_rows_from_array(df)\n\n    # Could have multiple instances of key names\n    # Will have to add columns manually\n    # If unique names, could probably reuse \"parse_struct()\"\n    df = df.withColumn(\"discountAmount\", df.discounts.amount).withColumn(\n        \"discountDescription\", df.discounts.description\n    )\n    df = df.drop(\"discounts\")\n\n    df = (\n        df.withColumn(\"childItemLineNumber\", df.childItems.lineItemNumber)\n        .withColumn(\"childItemLabel\", df.childItems.itemLabel)\n        .withColumn(\"childItemQuantity\", df.childItems.quantity)\n        .withColumn(\"childItemPrice\", df.childItems.price)\n        .withColumn(\"childItemDiscounts\", df.childItems.discounts)\n    )\n    df = df.drop(\"childItems\")\n\n    df = get_rows_from_array(df)\n    df = df.withColumn(\n        \"childItemDiscountAmount\", df.childItemDiscounts.amount\n    ).withColumn(\"childItemDiscountDescription\", df.childItemDiscounts.description)\n    df = df.drop(\"childItemDiscounts\")\n\n    return df\n\n\ndef write_df_as_csv(df: DataFrame) -&gt; None:\n    \"\"\"\n    Write the data frame to a local destination of your choice with headers\n\n    :param df: Contains flattened order data\n    \"\"\"\n    df.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(\n        OUTPUT_CSV_FILE\n    )\n    return None\n\n\ndef create_delta_table(spark: SparkSession) -&gt; None:\n    spark.sql(\"CREATE DATABASE IF NOT EXISTS EXERCISE\")\n\n    spark.sql(\n        \"\"\"\n    CREATE TABLE IF NOT EXISTS EXERCISE.ORDERS(\n        OrderToken String,\n        Preparation  String,\n        ItemLineNumber String,\n        ItemLabel String,\n        ItemQuantity Double,\n        ItemPrice Integer,\n        ItemDiscountAmount Integer,\n        ItemDiscountDescription String,\n        ChildItemLineNumber String, \n        ChildItemLabel String,\n        ChildItemQuantity Double,\n        ChildItemPrice Integer,\n        ChildItemDiscountAmount Integer,\n        ChildItemDiscountDescription String\n    ) USING DELTA\n    LOCATION \"{0}\"\n    \"\"\".format(\n            OUTPUT_DELTA_PATH\n        )\n    )\n\n    return None\n\nHaven’t used Delta before. Reference material: https://docs.microsoft.com/en-us/azure/databricks/delta/quick-start\n\ndef write_df_as_delta(df: DataFrame) -&gt; None:\n    \"\"\"\n    Write the dataframe output to the table created, overwrite mode can be used\n\n    :param df: flattened data\n    :return: Data from the orders table\n    \"\"\"\n    # Rename columns to match delta table schema\n    df = (\n        df.withColumnRenamed(\"orderToken\", \"OrderToken\")\n        .withColumnRenamed(\"preparation\", \"Rreparation\")\n        .withColumnRenamed(\"lineItemNumber\", \"LineItemNumber\")\n        .withColumnRenamed(\"itemLabel\", \"ItemLabel\")\n        .withColumnRenamed(\"quantity\", \"Quantity\")\n        .withColumnRenamed(\"price\", \"Price\")\n        .withColumnRenamed(\"discountAmount\", \"DiscountAmount\")\n        .withColumnRenamed(\"discountDescription\", \"DiscountDescription\")\n        .withColumnRenamed(\"childItemLineNumber\", \"ChildItemLineNumber\")\n        .withColumnRenamed(\"childItemLabel\", \"ChildItemLabel\")\n        .withColumnRenamed(\"childItemQuantity\", \"ChildItemQuantity\")\n        .withColumnRenamed(\"childItemPrice\", \"ChildItemPrice\")\n        .withColumnRenamed(\"childItemDiscountAmount\", \"ChildItemDiscountAmount\")\n        .withColumnRenamed(\n            \"childItemDiscountDescription\", \"ChildItemDiscountDescription\"\n        )\n    )\n\n    df.write.insertInto(\"EXERCISE.ORDERS\", overwrite=True)\n\n    return None\n\n\ndef read_data_delta(spark: SparkSession) -&gt; DataFrame:\n    \"\"\"\n    Read data from the table created\n\n    :param spark:\n    :return:\n    \"\"\"\n    return spark.sql(\"select * from exercise.orders;\")\n\n\nif __name__ == \"__main__\":\n    input_schema = get_struct_type()\n\n    input_df = read_json(INPUT_FILE, input_schema)\n\n    arrays_to_rows_df = get_rows_from_array(input_df)\n\n    unwrap_struct_df = get_unwrapped_nested_structure(arrays_to_rows_df)\n\n    write_df_as_csv(unwrap_struct_df)\n\n    create_delta_table(spark)\n    write_df_as_delta(unwrap_struct_df)\n\n    result_df = read_data_delta(spark)\n    result_df.show(truncate=False)\n\n+-----------------------+------------------+--------------+---------+------------+---------+------------------+-----------------------+-------------------+--------------+-----------------+--------------+-----------------------+----------------------------+\n|OrderToken             |Preparation       |ItemLineNumber|ItemLabel|ItemQuantity|ItemPrice|ItemDiscountAmount|ItemDiscountDescription|ChildItemLineNumber|ChildItemLabel|ChildItemQuantity|ChildItemPrice|ChildItemDiscountAmount|ChildItemDiscountDescription|\n+-----------------------+------------------+--------------+---------+------------+---------+------------------+-----------------------+-------------------+--------------+-----------------+--------------+-----------------------+----------------------------+\n|97331549875122744335422|Magic happens here|1             |COFFEE   |1.0         |345      |495               |Item 1, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 1, Discount 1    |\n|97331549875122744335422|Magic happens here|2             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|3             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|4             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|5             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|6             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|7             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|8             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|9             |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n|97331549875122744335422|Magic happens here|10            |COFFEE   |2.0         |945      |295               |Item 2, Discount 1     |1                  |CREAM         |1.0              |0             |495                    |Child Item 2, Discount 1    |\n+-----------------------+------------------+--------------+---------+------------+---------+------------------+-----------------------+-------------------+--------------+-----------------+--------------+-----------------------+----------------------------+"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html",
    "href": "content/development/code/notebooks/ml-notes.html",
    "title": "ML Notes",
    "section": "",
    "text": "Use of algorithms and statistical models to perform tasks without explicit instructions, instead using paterns and inference.\nExamples:\nimport numpy as np\nimport matplotlib.pyplot as plt\nMatplotlib:"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#practical-example---plotting",
    "href": "content/development/code/notebooks/ml-notes.html#practical-example---plotting",
    "title": "ML Notes",
    "section": "Practical Example - Plotting",
    "text": "Practical Example - Plotting\n\n# Effect of time spent walking (hours) on the distance travelled (miles)\ntime_spent_walking = [1, 2, 3, 4, 5]  # (independent variable)\ndistance = [2, 4, 6, 8, 10]  # (dependent variable)\n\nplt.plot(time_spent_walking, distance)\nplt.xlabel(\"Time Spent Walking (hours)\")\nplt.ylabel(\"Distance (miles)\")\nplt.title(\"Effect of time spent walking on distance travelled\")\nplt.show()\n\n\n\n\n\n# Effect of car age (years) on price ($)\ncar_age = [1, 2, 5, 10, 30]\nprice = [30000, 25000, 18000, 10000, 4000]\n\nplt.plot(car_age, price)\nplt.xlabel(\"Car Age (years)\")\nplt.ylabel(\"Price ($)\")\nplt.title(\"Effect of car age on price\")\nplt.show()\n\n\n\n\n\n# Effect of amount of time spent studying (hours) on test score results (%)\nstudy_time = [1, 5, 10, 20, 50, 100]\ntest_score_results = [40, 60, 70, 75, 88, 93]\n\nplt.plot(study_time, test_score_results)\nplt.xlabel(\"Study Time (hours)\")\nplt.ylabel(\"Test Score (%)\")\nplt.title(\"Effect of study time on test scores\")\nplt.show()"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#linear-regression-y-mx-b",
    "href": "content/development/code/notebooks/ml-notes.html#linear-regression-y-mx-b",
    "title": "ML Notes",
    "section": "Linear Regression: y = mx + b",
    "text": "Linear Regression: y = mx + b\n\nx: x axis\ny: y axis\nm: gradient of line\nb: value of y when x = 0\n\n\nExample 1\n\nx = [1, 2, 3, 4, 5]\ny = [2, 4, 6, 8, 10]\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n# y = 2x + 2\nfor i in x:\n    y = 2 * i\n    print(y)\n\n2\n4\n6\n8\n10\n\n\n\n\nExample 2\n\nx = [1, 2, 3, 4, 5]\ny = [6, 9, 12, 15, 18]\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n# y = mx + b\nfor i in x:\n    y = (3 * i) + 3\n    print(y)\n\n6\n9\n12\n15\n18\n\n\n\n\nExample 3\n\nx = [0, 1, 2, 3, 4]\ny = [6, 9, 12, 15, 18]\n\nplt.plot(x, y)\nplt.show()\n\n\n\n\n\n# y = mx + b\n# y = 3x + 6\n\nfor i in x:\n    y = (3 * i) + 6\n    print(i, y)\n\n0 6\n1 9\n2 12\n3 15\n4 18"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#practical-examples---linear-regression-y-mx-b",
    "href": "content/development/code/notebooks/ml-notes.html#practical-examples---linear-regression-y-mx-b",
    "title": "ML Notes",
    "section": "Practical Examples - Linear Regression (y = mx + b)",
    "text": "Practical Examples - Linear Regression (y = mx + b)\n\nQuestion 1\n\n# Effect of amount of water provided (L) per day on size of trees (m)\nwater = [0, 1, 2, 3, 4, 5, 6, 7, 8]\ntree_size = [4, 5, 6, 7, 8, 9, 10, 11, 12]\n\nplt.plot(water, tree_size)\nplt.show()\n\n\n\n\n\n\nSolution\n\n# y = mx + b\nfor x in water:\n    y = (1 * x) + 4\n    print(x, y)\n    # y = x + 4\n\n0 4\n1 5\n2 6\n3 7\n4 8\n5 9\n6 10\n7 11\n8 12\n\n\n\n\nQuestion 2\n\n# Effect of wingspan (cm) on flying speed (km/h)\nwingspan = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\nflying_speed = [0, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\nplt.plot(wingspan, flying_speed)\nplt.show()\n\n\n\n\n\n\nSolution\n\n# y = mx + b\nfor x in wingspan:\n    y = 0.5 * x\n    print(x, y)\n    # y = 0.5x + 0\n\n0 0.0\n10 5.0\n20 10.0\n30 15.0\n40 20.0\n50 25.0\n60 30.0\n70 35.0\n80 40.0\n90 45.0\n100 50.0\n\n\n\n\nQuestion 3\n\n# Effect of number of gifts given to employees each year, on staff statisfaction levels (100%)\nnum_of_gifts = [0, 1, 2, 3, 4, 5]\nsatisfaction = [50, 55, 60, 65, 70, 75]\nplt.plot(num_of_gifts, satisfaction)\nplt.show()\n\n\n\n\n\n\nSolution\n\n# y = mx + b\nfor x in num_of_gifts:\n    y = (5 * x) + 50\n    print(x, y)\n    # y = 5x + 50\n\n0 50\n1 55\n2 60\n3 65\n4 70\n5 75"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#line-of-best-fit",
    "href": "content/development/code/notebooks/ml-notes.html#line-of-best-fit",
    "title": "ML Notes",
    "section": "Line of Best Fit",
    "text": "Line of Best Fit\nCost: distance of each point from best fit line\nLoss: Sum of all distances between best fit line and data points (sum of costs)\n\n\n\nimage.png"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#mean-squared-error-mse",
    "href": "content/development/code/notebooks/ml-notes.html#mean-squared-error-mse",
    "title": "ML Notes",
    "section": "Mean Squared Error (MSE)",
    "text": "Mean Squared Error (MSE)\n\n\n\nimage.png\n\n\nMean Squared Error = sum (Y @ prediction - y at datapoint)^2 / number of datapoints\n\nExample\n\n# Data\nx_data = [1, 5, 8, 10]\ny_data = [4, 8, 9, 7]\n\n# Data plot\nplt.plot(x_data, y_data)\nplt.show()\n\n\n\n\nBest fit line (y = mx + b)\n\n# Calculate m, b\nm, b = np.polyfit(x_data, y_data, 1)\n\n# Best fit line equation:\nfor x in x_data:\n    y = (m * x) + b\n    print(x, y)\n\n1 5.04347826086957\n5 6.608695652173917\n8 7.7826086956521765\n10 8.56521739130435\n\n\nCost & Loss - Mean Squared Error\n\n\n\nimage.png\n\n\n\n\n\nvariable\n\n\n\n\n\n\n\n\nx\n1\n5\n8\n10\n\n\ny\n4\n8\n9\n7\n\n\ny[hat]\n5\n6\n7\n8\n\n\ncost\n1\n4\n4\n1\n\n\n\n\n# cost = (y[hat] - y) ^ 2 / 4\n\nmse = 10 / 4\nmse\n\n2.5"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#logistic-regression",
    "href": "content/development/code/notebooks/ml-notes.html#logistic-regression",
    "title": "ML Notes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\n\n\n\nimage.png"
  },
  {
    "objectID": "content/development/code/notebooks/ml-notes.html#overfitting",
    "href": "content/development/code/notebooks/ml-notes.html#overfitting",
    "title": "ML Notes",
    "section": "Overfitting",
    "text": "Overfitting\nBest fit too accuratly defines traning data and minimizes the loss… but thats only training data and new data will keep the model humble.\n\n\n\nimage.png\n\n\n\n\n\nimage.png\n\n\n\nRandom Forests\n\nRandom forests takes an average or majority decision from numerous decision trees and creates an output from this"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html",
    "href": "content/development/code/notebooks/grib.html",
    "title": "GRIB",
    "section": "",
    "text": "#! /usr/bin/env python3\n#\n#\n#   Purpose:\n#       Test manipulation of .grb2 files from NOAA WAVEWATCH III hindcast repo\n#\n#   Notes:\n#       1. pygrib is not compatible with Windows.\n#       2. GRIB = General Regularly distributed Information in Binary form\n#           GRIB is a binary format of gridded data.\n#       3. WAVEWATCH III: https://polar.ncep.noaa.gov/waves/hindcasts/nopp-phase2.php\n#       3. pygrib docs: https://jswhit.github.io/pygrib/api.html#example-usage\n#\n#   Author(s):\n#       Caleb Grant\n#\n#   Revision History\n#   Date        Reason\n#   ----------------------------\n#   2021-09-23  Created. CG.\n# ===========================================\n# Standard libraries\nimport os\nimport datetime\n\n# Third party packages\nimport pygrib\nimport numpy as np\n\n# Make output of plotting commands display inline with notebook\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\n\nModuleNotFoundError: No module named 'pygrib'"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#object-methods",
    "href": "content/development/code/notebooks/grib.html#object-methods",
    "title": "GRIB",
    "section": "Object methods",
    "text": "Object methods\n\nprint(dir(dp_grb_obj))\n\n['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__lt__', '__ne__', '__new__', '__next__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '_advance', 'close', 'closed', 'has_multi_field_msgs', 'message', 'messagenumber', 'messages', 'name', 'read', 'readline', 'rewind', 'seek', 'select', 'tell']"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#grib-object-keys",
    "href": "content/development/code/notebooks/grib.html#grib-object-keys",
    "title": "GRIB",
    "section": "GRIB Object keys",
    "text": "GRIB Object keys\n\nprint(sorted(dp_grb_obj[1].keys()))\n\n['GRIBEditionNumber', 'NV', 'Ni', 'Nj', 'PLPresent', 'PVPresent', 'alternativeRowScanning', 'analDate', 'angleDivisor', 'angleMultiplier', 'angleSubdivisions', 'average', 'backgroundProcess', 'basicAngleOfTheInitialProductionDomain', 'binaryScaleFactor', 'bitMapIndicator', 'bitmapPresent', 'bitsPerValue', 'bottomLevel', 'centre', 'centreDescription', 'cfName', 'cfNameECMF', 'cfVarName', 'cfVarNameECMF', 'changeDecimalPrecision', 'codedValues', 'dataDate', 'dataRepresentationTemplateNumber', 'dataTime', 'day', 'decimalPrecision', 'decimalScaleFactor', 'deleteCalendarId', 'deleteLocalDefinition', 'deletePV', 'discipline', 'distinctLatitudes', 'distinctLongitudes', 'editionNumber', 'endStep', 'forecastTime', 'g2grid', 'genVertHeightCoords', 'generatingProcessIdentifier', 'getNumberOfValues', 'globalDomain', 'grib2LocalSectionPresent', 'grib2divider', 'gridDefinitionDescription', 'gridDefinitionTemplateNumber', 'gridDescriptionSectionPresent', 'gridType', 'hour', 'hoursAfterDataCutoff', 'iDirectionIncrement', 'iDirectionIncrementGiven', 'iDirectionIncrementInDegrees', 'iScansNegatively', 'iScansPositively', 'identifier', 'ieeeFloats', 'ifsParam', 'ijDirectionIncrementGiven', 'indicatorOfUnitOfTimeRange', 'interpretationOfNumberOfPoints', 'isConstant', 'isHindcast', 'is_aerosol', 'is_aerosol_optical', 'is_chemical', 'is_chemical_distfn', 'is_efas', 'is_uerra', 'jDirectionIncrement', 'jDirectionIncrementGiven', 'jDirectionIncrementInDegrees', 'jPointsAreConsecutive', 'jScansPositively', 'julianDay', 'kurtosis', 'latLonValues', 'latitudeOfFirstGridPoint', 'latitudeOfFirstGridPointInDegrees', 'latitudeOfLastGridPoint', 'latitudeOfLastGridPointInDegrees', 'latitudes', 'lengthOfHeaders', 'level', 'localTablesVersion', 'longitudeOfFirstGridPoint', 'longitudeOfFirstGridPointInDegrees', 'longitudeOfLastGridPoint', 'longitudeOfLastGridPointInDegrees', 'longitudes', 'mAngleMultiplier', 'mBasicAngle', 'masterDir', 'maximum', 'md5Headers', 'md5Section1', 'md5Section3', 'md5Section4', 'md5Section5', 'md5Section6', 'md5Section7', 'minimum', 'minute', 'minutesAfterDataCutoff', 'missingValue', 'modelName', 'month', 'name', 'nameECMF', 'nameOfFirstFixedSurface', 'nameOfSecondFixedSurface', 'neitherPresent', 'numberOfDataPoints', 'numberOfMissing', 'numberOfOctectsForNumberOfPoints', 'numberOfSection', 'numberOfSection', 'numberOfSection', 'numberOfSection', 'numberOfSection', 'numberOfSection', 'numberOfValues', 'offsetValuesBy', 'optimizeScaleFactor', 'packingType', 'paramId', 'paramIdECMF', 'parameterCategory', 'parameterName', 'parameterNumber', 'parameterUnits', 'pressureUnits', 'productDefinitionTemplateNumber', 'productType', 'productionStatusOfProcessedData', 'radius', 'referenceValue', 'referenceValueError', 'resolutionAndComponentFlags', 'resolutionAndComponentFlags1', 'resolutionAndComponentFlags2', 'resolutionAndComponentFlags6', 'resolutionAndComponentFlags7', 'resolutionAndComponentFlags8', 'scaleFactorOfEarthMajorAxis', 'scaleFactorOfEarthMinorAxis', 'scaleFactorOfFirstFixedSurface', 'scaleFactorOfRadiusOfSphericalEarth', 'scaleFactorOfSecondFixedSurface', 'scaleValuesBy', 'scaledValueOfEarthMajorAxis', 'scaledValueOfEarthMinorAxis', 'scaledValueOfFirstFixedSurface', 'scaledValueOfRadiusOfSphericalEarth', 'scaledValueOfSecondFixedSurface', 'scanningMode', 'scanningMode5', 'scanningMode6', 'scanningMode7', 'scanningMode8', 'second', 'section0Length', 'section1Length', 'section3Length', 'section4Length', 'section5Length', 'section6Length', 'section7Length', 'section8Length', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'sectionNumber', 'selectStepTemplateInstant', 'selectStepTemplateInterval', 'setBitsPerValue', 'setCalendarId', 'shapeOfTheEarth', 'shortName', 'shortNameECMF', 'significanceOfReferenceTime', 'skewness', 'sourceOfGridDefinition', 'standardDeviation', 'startStep', 'stepRange', 'stepType', 'stepTypeInternal', 'stepUnits', 'subCentre', 'subdivisionsOfBasicAngle', 'tablesVersion', 'tablesVersionLatest', 'targetCompressionRatio', 'tempPressureUnits', 'topLevel', 'totalLength', 'typeOfCompressionUsed', 'typeOfFirstFixedSurface', 'typeOfGeneratingProcess', 'typeOfLevel', 'typeOfOriginalFieldValues', 'typeOfProcessedData', 'typeOfSecondFixedSurface', 'units', 'unitsECMF', 'unitsOfFirstFixedSurface', 'unitsOfSecondFixedSurface', 'uvRelativeToGrid', 'validDate', 'validityDate', 'validityTime', 'values', 'year']"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#test-value-arrays",
    "href": "content/development/code/notebooks/grib.html#test-value-arrays",
    "title": "GRIB",
    "section": "Test value arrays",
    "text": "Test value arrays\n\nvalues = np.array([grb.values for grb in hs_grb_obj])\nprint(values.shape, values.min(), values.max())\n\n(249, 331, 301) 0.0 9999.0\n\n\n\nprint(dir(values))\n\n['T', '__abs__', '__add__', '__and__', '__array__', '__array_finalize__', '__array_function__', '__array_interface__', '__array_prepare__', '__array_priority__', '__array_struct__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__complex__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__divmod__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__xor__', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n\n\n\n# sets, rows per set, columns per row\nprint(values.shape)\n\n# would equate to\nsets = len(values)\nrows = len(values[0])\ncols = len(values[0][0])\nprint(sets, rows, cols)\n\n(249, 331, 301)\n249 331 301"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#testing-latlongs",
    "href": "content/development/code/notebooks/grib.html#testing-latlongs",
    "title": "GRIB",
    "section": "Testing lat/longs",
    "text": "Testing lat/longs\n\nfor grb in dp_grb_obj[1:6]:\n    lats, lons = grb.latlons()\n    print(\"min/max lat ang lon: \", lats.min(), lats.max(), lons.min(), lons.max())\n\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\nmin/max lat ang lon:  -0.00010999999961602835 55.0 260.0 310.0000000000057\n\n\n\nwind_grb_file = \"multi_reanal.ecg_10m.wind.200901.grb2\"\nwind_grb_obj = pygrib.open(os.path.join(data_dir, wind_grb_file))\n\n# Reqind the iterator\nwind_grb_obj.rewind()\n\n# values = np.array([grb.values for grb in wind_grb_obj])\n# np.savetxt(\"test_wind.csv\", values, delimiter=\",\")\n\ndata_dict = {}\n# Each iteration captures point in time across entire grid\nfor grb in wind_grb_obj:\n    data_dict.update({grb.validDate: {grb.parameterName: np.array(grb.values)}})\n\n\nprint(data_dict[datetime.datetime(2009, 1, 1, 0, 0)])\n\n{'v-component of wind': array([[9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       ...,\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.]])}\n\n\n\nhs_grb_obj.rewind()\n\nprint(hs_grb_obj[1].parameterName)\n\ndate_selected = datetime.datetime(2009, 1, 1, 0, 0)\n\nvalues = []\n\nfor grb in hs_grb_obj:\n    if (\n        grb.validDate == date_selected\n        and grb.parameterName == \"Significant height of combined wind waves and swell\"\n    ):\n        values.append(grb.values)\nvalues = np.array(values)\nprint(values.shape, values.min(), values.max())\n\nlats, longs = grb.latlons()\nprint(lats.min(), lats.max(), longs.min(), longs.max())\n\nSignificant height of combined wind waves and swell\n(1, 331, 301) 0.0 9999.0\n-0.00010999999961602835 55.0 260.0 310.0000000000057\n\n\n\n\"\"\"\nfig = plt.figure(figsize=(16,35))\n# https://matplotlib.org/basemap/users/ortho.html\nm = Basemap(projection='lcc', lon_0=-80, lat_0=30, resolution='l', width=5.e6, height=5.e6)\nx, y = m(longs, lats)\n\n# for vals in range(1, 2):\n#     print(values[vals])\n\nax = plt.plot(values[0])\nm.drawcoastlines()\ncs = m.contourf(x, y, values[0], np.linspace(230, 300, 41), cmap=plt.cm.jet, extend='both')\nt = plt.title('test')\n\"\"\"\n\n\"\\nfig = plt.figure(figsize=(16,35))\\n# https://matplotlib.org/basemap/users/ortho.html\\nm = Basemap(projection='lcc', lon_0=-80, lat_0=30, resolution='l', width=5.e6, height=5.e6)\\nx, y = m(longs, lats)\\n\\n# for vals in range(1, 2):\\n#     print(values[vals])\\n\\nax = plt.plot(values[0])\\nm.drawcoastlines()\\ncs = m.contourf(x, y, values[0], np.linspace(230, 300, 41), cmap=plt.cm.jet, extend='both')\\nt = plt.title('test')\\n\"\n\n\n\nhs_grb_obj.rewind()\n\nfor grb in hs_grb_obj[1:2]:\n    print(grb.validDate, grb.parameterName, grb.latlons())\n\n2009-01-01 00:00:00 Significant height of combined wind waves and swell (array([[ 5.5000000e+01,  5.5000000e+01,  5.5000000e+01, ...,\n         5.5000000e+01,  5.5000000e+01,  5.5000000e+01],\n       [ 5.4833333e+01,  5.4833333e+01,  5.4833333e+01, ...,\n         5.4833333e+01,  5.4833333e+01,  5.4833333e+01],\n       [ 5.4666666e+01,  5.4666666e+01,  5.4666666e+01, ...,\n         5.4666666e+01,  5.4666666e+01,  5.4666666e+01],\n       ...,\n       [ 3.3322400e-01,  3.3322400e-01,  3.3322400e-01, ...,\n         3.3322400e-01,  3.3322400e-01,  3.3322400e-01],\n       [ 1.6655700e-01,  1.6655700e-01,  1.6655700e-01, ...,\n         1.6655700e-01,  1.6655700e-01,  1.6655700e-01],\n       [-1.1000000e-04, -1.1000000e-04, -1.1000000e-04, ...,\n        -1.1000000e-04, -1.1000000e-04, -1.1000000e-04]]), array([[260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       ...,\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ],\n       [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n        309.83333333, 310.        ]]))\n\n\n\nprint(\n    (datetime.datetime(2009, 1, 1, 0, 0) + datetime.timedelta(hours=744)).strftime(\n        \"%Y-%m-%d\"\n    )\n)\n\n2009-02-01\n\n\n\nBetter iteration method:\n\n# hs_grb_obj.rewind()\nhs_grb_obj.seek(0)\nprint(hs_grb_obj.tell())\nprint(hs_grb_obj.read(1))\n\n0\n[1:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 0 hrs:from 200901010000]\n\n\n\nhs_grb_obj.seek(0)\nprint(hs_grb_obj.tell())\n\nfor grb in hs_grb_obj:\n    grb\n\nprint(hs_grb_obj.tell())\n\n0\n249\n\n\n\nhs_grb_obj.seek(0)\ngrb = hs_grb_obj.read(1)[0]\nprint(\"1\")\nprint(grb.values)\n\n\nprint(\"2\")\ngrb = hs_grb_obj.select(name=\"Significant height of combined wind waves and swell\")[0]\nprint(grb.values)\n\n1\n[[-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n ...\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]]\n2\n[[-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n ...\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]]\n\n\n\nhs_grb_obj.select(name=\"Significant height of combined wind waves and swell\")[1:6]\n\n[2:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 3 hrs:from 200901010000,\n 3:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 6 hrs:from 200901010000,\n 4:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 9 hrs:from 200901010000,\n 5:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 12 hrs:from 200901010000,\n 6:Significant height of combined wind waves and swell:m (instant):regular_ll:surface:level 1:fcst time 15 hrs:from 200901010000]\n\n\n\nFind the first grib message with a matching name:\n\ngrb = hs_grb_obj.select(name=\"Significant height of combined wind waves and swell\")[0]\n\nExtract the data values using the values key (grb.keys() will return a list of the available keys):\n\n# The data is returned as a numpy array, or if missing values or a bitmap\n# are present, a numpy masked array.  Reduced lat/lon or gaussian grid\n# data is automatically expanded to a regular grid. Details of the internal\n# representation of the grib data (such as the scanning mode) are handled\n# automatically.\n\ngrb.values\n\nmasked_array(\n  data=[[--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        ...,\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --],\n        [--, --, --, ..., --, --, --]],\n  mask=[[ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        ...,\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True],\n        [ True,  True,  True, ...,  True,  True,  True]],\n  fill_value=9999.0)\n\n\n\ngrb.values.shape, grb.values.min(), grb.values.max()\n\n((331, 301), 0.0, 6.5600000000000005)\n\n\nGet the latitudes and longitudes of the grid:\n\nlats, lons = grb.latlons()\nlats.shape, lats.min(), lats.max(), lons.shape, lons.min(), lons.max()\n\n((331, 301),\n -0.00010999999961602835,\n 55.0,\n (331, 301),\n 260.0,\n 310.0000000000057)\n\n\nExtract data and get lat/lon values for a subset over North America:\n\ndata, lats, lons = grb.data(lat1=20, lat2=70, lon1=220, lon2=320)\n\ndata.shape, lats.min(), lats.max(), lons.min(), lons.max()\n\n((210, 301), 20.166597000000415, 55.0, 260.0, 310.0000000000057)\n\n\nClose the grib file\n\nhs_grb_obj.close()\n\n\n\n\nTesting docstrings\n\ngrbs = wind_grb_obj\n\ngrbs.seek(0)\n\nu_grb = grbs.select(name=\"U component of wind\")\nv_grb = grbs.select(name=\"V component of wind\")\n\nprint(len(u_grb), len(v_grb))\n\n249 249\n\n\n\nprint(u_grb[0].values.shape, v_grb[0].values.shape)\n\n(331, 301) (331, 301)\n\n\n\ngrbs = wind_grb_obj\ngrbs.seek(0)\ngrb = grbs.select(name=\"U component of wind\")[0]\n\n\ngrb.has_key(\"gaulats\")\n\nFalse\n\n\n\n\npygrib.gribmessage object\n\nprint(\"grb.messagenumber: \", grb.messagenumber)\nprint(\"grb.fcstimeunits: \", grb.fcstimeunits)\nprint(\"grb.analDate: \", grb.analDate)\nprint(\"grb.validDate: \", grb.validDate)\n\ngrb.messagenumber:  1\ngrb.fcstimeunits:  hrs\ngrb.analDate:  2009-01-01 00:00:00\ngrb.validDate:  2009-01-01 00:00:00\n\n\n\n# If the default values of lat1,lat2,lon1,lon2 are None, which means the entire grid is returned\ngrb.data(lat1=None, lat2=None, lon1=None, lon2=None)\n\n(masked_array(\n   data=[[--, --, --, ..., --, --, --],\n         [--, --, --, ..., --, --, --],\n         [--, --, --, ..., --, --, --],\n         ...,\n         [--, --, --, ..., --, --, --],\n         [--, --, --, ..., --, --, --],\n         [--, --, --, ..., --, --, --]],\n   mask=[[ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ...,  True,  True,  True],\n         ...,\n         [ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ...,  True,  True,  True],\n         [ True,  True,  True, ...,  True,  True,  True]],\n   fill_value=9999.0),\n array([[ 5.5000000e+01,  5.5000000e+01,  5.5000000e+01, ...,\n          5.5000000e+01,  5.5000000e+01,  5.5000000e+01],\n        [ 5.4833333e+01,  5.4833333e+01,  5.4833333e+01, ...,\n          5.4833333e+01,  5.4833333e+01,  5.4833333e+01],\n        [ 5.4666666e+01,  5.4666666e+01,  5.4666666e+01, ...,\n          5.4666666e+01,  5.4666666e+01,  5.4666666e+01],\n        ...,\n        [ 3.3322400e-01,  3.3322400e-01,  3.3322400e-01, ...,\n          3.3322400e-01,  3.3322400e-01,  3.3322400e-01],\n        [ 1.6655700e-01,  1.6655700e-01,  1.6655700e-01, ...,\n          1.6655700e-01,  1.6655700e-01,  1.6655700e-01],\n        [-1.1000000e-04, -1.1000000e-04, -1.1000000e-04, ...,\n         -1.1000000e-04, -1.1000000e-04, -1.1000000e-04]]),\n array([[260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        ...,\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ],\n        [260.        , 260.16666667, 260.33333333, ..., 309.66666667,\n         309.83333333, 310.        ]]))\n\n\n\ngrbs = wind_grb_obj\n\ngrbs.seek(0)\n\nu_grb = grbs.select(name=\"U component of wind\")\nv_grb = grbs.select(name=\"V component of wind\")\n\ngrbs = u_grb\n\n\nfor grb in grbs:\n    print(\"grb.messagenumber: \", grb.messagenumber)\n    print(\"grb.fcstimeunits: \", grb.fcstimeunits)\n    print(\"grb.validDate: \", grb.validDate)\n    print(\"type(grb.values): \", type(grb.values))\n    print(grb.values)\n    break\n\ngrb.messagenumber:  1\ngrb.fcstimeunits:  hrs\ngrb.validDate:  2009-01-01 00:00:00\ntype(grb.values):  &lt;class 'numpy.ma.core.MaskedArray'&gt;\n[[-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n ...\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]\n [-- -- -- ... -- -- --]]"
  },
  {
    "objectID": "content/development/code/notebooks/grib.html#masked-arrays",
    "href": "content/development/code/notebooks/grib.html#masked-arrays",
    "title": "GRIB",
    "section": "Masked arrays",
    "text": "Masked arrays\n\n# Masked arrays are arrays that may have missing or invalid entries\n# https://numpy.org/doc/stable/reference/maskedarray.generic.html\n\n# When an element of the mask is False, the corresponding element of the associated array is valid and is said to be unmasked.\n# When an element of the mask is True, the corresponding element of the associated array is said to be masked (invalid).\n# The package ensures that masked entries are not used in computations.\n\nimport numpy.ma as ma\n\n\nMasked array example\n\nx = np.array([1, 2, 3, -1, 5])\nprint(x.mean())\n\n# Mark the fourth element as invalid\nmx = ma.masked_array(x, mask=[0, 0, 0, 1, 0])\nprint(mx.mean())\n\n2.0\n2.75\n\n\n\n\nUsage\n\ngrbs = wind_grb_obj\n\ngrbs.seek(0)\n\nu_grb = grbs.select(name=\"U component of wind\")\nv_grb = grbs.select(name=\"V component of wind\")\n\ngrb = u_grb[0]\n\n\n# Test if input is instance of masked array\nma.isMaskedArray(grb.values)\n\nTrue\n\n\n\n# Check if all elements evaluate to True\n# If false, invalid values exist\nma.all(grb.values)\n\nFalse\n\n\n\nprint(ma.count(grb.values))  # non-maked elements\nprint(ma.count_masked(grb.values))  # masked elements\n\n30096\n69535\n\n\n\nma.getdata(grb.values)\n\narray([[9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       ...,\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.]])\n\n\n\nma.ravel(grb.values)  # Returns a 1D version of self, as a view.\n\nmasked_array(data=[--, --, --, ..., --, --, --],\n             mask=[ True,  True,  True, ...,  True,  True,  True],\n       fill_value=9999.0)\n\n\n\nma.ravel(grb.values).fill_value\n\n9999.0\n\n\n\nma.filled(grb.values)\n\narray([[9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       ...,\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.],\n       [9999., 9999., 9999., ..., 9999., 9999., 9999.]])\n\n\n\nma.MaskedArray.torecords(grb.values)\n\narray([[(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       ...,\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)],\n       [(9999.,  True), (9999.,  True), (9999.,  True), ...,\n        (9999.,  True), (9999.,  True), (9999.,  True)]],\n      dtype=[('_data', '&lt;f8'), ('_mask', '?')])\n\n\n\n\nwind_grb_obj.name  # name of object\nwind_grb_obj.messages  # count of messages\nwind_grb_obj.messagenumber  # Current message index\n\n0"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "This site is designed to be my personal curated toolkit. It is where I store useful information, resources, tools, and code snippets I use on a regular basis."
  },
  {
    "objectID": "content/resources/recipes.html",
    "href": "content/resources/recipes.html",
    "title": "Recipes",
    "section": "",
    "text": "Typical schedule for baking sourdough bread. Timing could change based on room temperature.\n\n\n\n\n\n\n\n\nTime\nStep\nNotes\n\n\n\n\n8:00am\nFeed starter\nDiscard all but at least 2 tbsp\n\n\n12:00pm\nAutolyse\n\n\n\n1:00pm\nAdd starter & salt\n\n\n\n1:30pm\nStretch & fold\nEvery 30 min for 2-2.5 hours\n\n\n3:30pm\nBulk rise\n1-3 hours, should rise 30-50%\n\n\n8:30pm\nPre shape & bench rest\n\n\n\n9:00pm\nFinal shaping & proof\nPlace in proofing baskets, cover, place in fridge overnight\n\n\n9:00am\nBake\n500 F in dutch oven, 20 min lid on, lower to 450, 15-30 min lid off"
  },
  {
    "objectID": "content/resources/recipes.html#sourdough-bread",
    "href": "content/resources/recipes.html#sourdough-bread",
    "title": "Recipes",
    "section": "",
    "text": "Typical schedule for baking sourdough bread. Timing could change based on room temperature.\n\n\n\n\n\n\n\n\nTime\nStep\nNotes\n\n\n\n\n8:00am\nFeed starter\nDiscard all but at least 2 tbsp\n\n\n12:00pm\nAutolyse\n\n\n\n1:00pm\nAdd starter & salt\n\n\n\n1:30pm\nStretch & fold\nEvery 30 min for 2-2.5 hours\n\n\n3:30pm\nBulk rise\n1-3 hours, should rise 30-50%\n\n\n8:30pm\nPre shape & bench rest\n\n\n\n9:00pm\nFinal shaping & proof\nPlace in proofing baskets, cover, place in fridge overnight\n\n\n9:00am\nBake\n500 F in dutch oven, 20 min lid on, lower to 450, 15-30 min lid off"
  },
  {
    "objectID": "content/resources/recipes.html#greek-yogurt-protein-smoothie",
    "href": "content/resources/recipes.html#greek-yogurt-protein-smoothie",
    "title": "Recipes",
    "section": "Greek Yogurt Protein Smoothie",
    "text": "Greek Yogurt Protein Smoothie\n\n1-2 scoops protein powder\n1/2 cup greek yogurt\n1 banana\n1/3 cup berries\n1/2 tbsp flaxseed\n1/3 tbsp chia seed\n1 tbsp honey\nice & water for desired consistency"
  },
  {
    "objectID": "content/resources/recipes.html#overnight-protein-oats",
    "href": "content/resources/recipes.html#overnight-protein-oats",
    "title": "Recipes",
    "section": "Overnight Protein Oats",
    "text": "Overnight Protein Oats\n\n1-2 scoops protein powder\n90 grams oats\n90 grams greek yogurt\n140 grams milk\n5 grams chia and/or flaxseed\n15 grams honey\n\nStir or shake to combine. Refrigerate overnight."
  },
  {
    "objectID": "content/resources/recipes.html#buffalo-cauliflower",
    "href": "content/resources/recipes.html#buffalo-cauliflower",
    "title": "Recipes",
    "section": "Buffalo Cauliflower",
    "text": "Buffalo Cauliflower\n\nPreheat oven to 450 F\nBatter:\n\n3/4 cup flour\n1 tsp paprika\n2 tsp garlic powder\n1 tsp salt\n1/2 tsp black pepper\n3/4 cup milk\n\nSauce:\n\n1/4 cup buffalo sauce\n1 tbsp Honey\n1/2 tsp black pepper\n\nSplit cauliflower into florets and mix in batter\nBake for 20 minutes, flip after 10.\nBrush sauce onto cauliflower, bake 10 minutes.\nFlip cauliflower and brush again with sauce, bake 10 minutes."
  },
  {
    "objectID": "content/resources/recipes.html#quinoa-cucumber-salad",
    "href": "content/resources/recipes.html#quinoa-cucumber-salad",
    "title": "Recipes",
    "section": "Quinoa & Cucumber Salad",
    "text": "Quinoa & Cucumber Salad\n\nSalad:\n\n2 cups cucumber, spiralized or julienned\n2 cups chopped tomatoes\n2 large avocados, diced\n1 red onion, sliced\n2 cups cooked quinoa\n1 handful chopped parsley (or cilantro)\n\nDressing - blend the following:\n\n1 ripe avocado\n1/4 cup white wine vinegar\nJuice of one lime\nSalt and fresh cracked pepper, to taste\n3/4 cup olive oil"
  },
  {
    "objectID": "content/resources/recipes.html#cougar-gold-mac-cheese",
    "href": "content/resources/recipes.html#cougar-gold-mac-cheese",
    "title": "Recipes",
    "section": "Cougar Gold Mac & Cheese",
    "text": "Cougar Gold Mac & Cheese\n\n6 cups water\n2 1/2 cups pasta\n2 tbsp butter\n2 tbsp all-purpose flour\n1 1/3 cups milk\n1 1/3 cups heavy cream\n1 1/2 cups Cougar Gold, grated\n1 1/2 cups Crimson Fire, grated\n1/2 cup ricotta\n1/2 tbsp chicken bouillon\nground black pepper, paprika to taste\n1/2 cup fresh breadcrumbs (optional)\n\nPreheat oven to 350° F.\nBring the water, salt and pasta to a rolling boil in a medium saucepan. Cook just until tender. Drain pasta, put into prepared baking dish and set aside. Meanwhile prepare the sauce. In a saucepan over medium-low heat, melt 2 tablespoons of butter. While whisking, gradually add the flour. Whisk for about 2 minutes or until golden and bubbling. Very slowly add the milk, whisking constantly to avoid developing lumps. Simmer for 15 minutes until thickened (alfredo sauce consistency), stirring often to prevent mixture from burning. Remove from heat and stir in Cougar cheese, ricotta, chicken bouillon, paprika and black pepper to taste. Pour sauce onto cooked pasta (do not stir). Optionally top with breadcrumbs tossed in melted butter. Bake for 30 minutes until browned and bubbling."
  },
  {
    "objectID": "content/resources/recipes.html#lemon-dill-sauce",
    "href": "content/resources/recipes.html#lemon-dill-sauce",
    "title": "Recipes",
    "section": "Lemon Dill Sauce",
    "text": "Lemon Dill Sauce\n\n1/2 lemon\n1 tsp dill\n4 tbsp olive oil\n1 tsp honey\n1-2 tsp dijon mustard\n1 clove garlic finely minced\npaprika & ground black pepper to taste"
  },
  {
    "objectID": "content/resources/recipes.html#salad-dressing",
    "href": "content/resources/recipes.html#salad-dressing",
    "title": "Recipes",
    "section": "Salad Dressing",
    "text": "Salad Dressing\n\n1/2 cup extra virgin olive oil\n1/3 cup apple cider vinegar\n1/4 cup honey, use maple syrup for vegan\n2 teaspoons balsamic vinegar\n1 teaspoon dijon mustard\n2 cloves garlic finely minced\npinch of sea salt"
  },
  {
    "objectID": "content/resources/recipes.html#rice-pilaf",
    "href": "content/resources/recipes.html#rice-pilaf",
    "title": "Recipes",
    "section": "Rice Pilaf",
    "text": "Rice Pilaf\n\n1 cup rice\n1/8 cup spaghetti noodle broken into 1/4 inch pieces\n1.5 tbsp butter\n1 tsp chicken bouillon\n2 cups water\n\nHeat the butter in a sauce pan then add rice and noodle. Add boillon to the water and mix to combine then add to sauce pan. Cook on high for 1 minute then cover and simmer for 40 minutes."
  },
  {
    "objectID": "content/resources/recipes.html#egg-protein-bars",
    "href": "content/resources/recipes.html#egg-protein-bars",
    "title": "Recipes",
    "section": "Egg Protein Bars",
    "text": "Egg Protein Bars\n\n1.5 lbs sausage\n15 eggs\n10 button mushrooms\n20 cherry tomatoes\n5 ounce container baby spinach\n2 cups cheddar cheese\nSalt and pepper\n\n\nPreheat your oven to 350 degrees.\nHeat a large sautee pan on high heat and brown the sausage. Add the mushrooms and sautee until browned, about 3 minutes. Add the spinach and cook for an additional minute, until slightly browned, then add sliced cherry tomatoes, stir, and take off the heat.\nWhisk eggs, salt and pepper, and cheese together, then toss in your sausage and vegetables and whisk.\nLine a deep baking dish with parchment paper, then pour egg mixture on top, and bake at 350 degrees for 40 minutes, until cooked.\nRemove from oven, allow to cool for 20 minutes, then turn over, cut into 5 portions, and wrap."
  },
  {
    "objectID": "content/resources/recipes.html#buffalo-chicken-meatballs",
    "href": "content/resources/recipes.html#buffalo-chicken-meatballs",
    "title": "Recipes",
    "section": "Buffalo Chicken Meatballs",
    "text": "Buffalo Chicken Meatballs\n\n3 lbs ground turkey\n0.75 cup breadcrumbs\n1 tsp salt\n6 ounces crumbled blue cheese\n0.5 cup buffalo sauce\n1 bunch scallions\n3 eggs\n3 garlic cloves\nAmoroso hoagie roll\nShrettuce\nSliced tomato\nPickles\nRanch\n\n\nAdd ground turkey, blue cheese, salt, and breadcrumbs to a large bowl.\nAdd eggs, buffalo sauce, scallions, and garlic to a blender and blend. Add the wet ingredients to the meat and mix.\nUse an ice cream scoop to scoop out the meatballs onto a parchment lined baking sheet, and bake at 550 degrees for 12 minutes.\nDouse in buffalo sauce and toss.\nSlice a hoagie roll in half, add ranch, lettuce, tomato, and meatballs, and eat."
  },
  {
    "objectID": "content/resources/recipes.html#hummus",
    "href": "content/resources/recipes.html#hummus",
    "title": "Recipes",
    "section": "Hummus",
    "text": "Hummus\n\n3 cups chickpeas\n1-2 cloves garlic\n1/3 cup tahini\n1 lemon\ndrizzle of olive oil\nsalt\n\n\nCook the chickpeas well (even canned chickpeas can use a quick simmer!). Overcooked chichpeas = thicker hummus.\nPeel the chickpeas. Chickpea skins are edible, but if you want creamy hummus, peel the chickpeas and discard the skins.\nIn a food processor, combine chickpeas, tahini, garlic, lemon, and salt\nFor fluffy hummus, add ice cube while blending\nLet the food processor run for 4 to 5 minutes. Run the food processor for a few minutes so the hummus mixture will blend well enough to a smooth texture. Test, and if needed, add a tiny bit of hot water through the opener of the food processor as it’s running to help continue to smooth the hummus even more until it’s utterly creamy.\nAdd to a bowl for storing in refridgerator. Top with olive oil."
  },
  {
    "objectID": "content/resources/recipes.html#others",
    "href": "content/resources/recipes.html#others",
    "title": "Recipes",
    "section": "Others",
    "text": "Others\n\nSteak Diane\nCilantro Lime Shrimp with Zucchini Noodles"
  },
  {
    "objectID": "content/resources/data-management.html",
    "href": "content/resources/data-management.html",
    "title": "Data Management",
    "section": "",
    "text": "EPA Guidance on Systematic Planning Using the Data Quality Objectives Process\n\n\n\n\n\nCode\nimport pandas as pd\n\ndqo = \"../../static/resources/DQO.xlsx\"\nelements = pd.read_excel(dqo, sheet_name=0)\nfactors = pd.read_excel(dqo, sheet_name=1)\n\n\n\n\n\n\n\nCode\nelements.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nElement\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nOrganization\n\n\n\nIdentification and involvement of the project manager, sponsoring organization and responsible official, project personnel, stakeholders, scientific experts, etc. (e.g., all customers and suppliers).\n\n\n\n\n\n\n\nProject Goal\n\n\n\nDescription of the project goal, objectives, and study questions and issues.\n\n\n\n\n\n\n\nSchedule\n\n\n\nIdentification of project schedule, resources (including budget), milestones, and any applicable requirements (e.g., regulatory requirements, contractual requirements).\n\n\n\n\n\n\n\nData Needs\n\n\n\nIdentification of the type of data needed and how the data will be used to support the project’s objectives.\n\n\n\n\n\n\n\nCriteria\n\n\n\nDetermination of the quantity of data needed and specification of performance criteria for measuring quality.\n\n\n\n\n\n\n\nData Collection\n\n\n\nDescription of how and where the data will be obtained (including existing data) and identification of any constraints on data collection.\n\n\n\n\n\n\n\nQuality Assurance (QA)\n\n\n\nSpecification of needed QA and quality control (QC) activities to assess the quality performance criteria (e.g., QC samples for both field and laboratory, audits, technical assessments, performance evaluations, etc.).\n\n\n\n\n\n\n\nAnalysis\n\n\n\nDescription of how the acquired data will be analyzed (either in the field or the laboratory), evaluated (i.e., QA review/verification/validation), and assessed against its intended use and the quality performance criteria.\n\n\n\n\n\n\n\n\n\n\n\nWhen specifying the project goal (element #2 in Table 1), a key activity is to determine the key questions which the study will address once data and information are properly collected and analyzed.\n\n\n\n\n\n\nCode\nfactors.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nFactor\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nSoundness\n\n\n\nThe extent to which the scientific and technical procedures, measures, methods or models employed to generate the information are reasonable for, and consistent with, the intended application.\n\n\n\n\n\n\n\nApplicability and Utility\n\n\n\nThe extent to which the information is relevant for the Agency’s intended use.\n\n\n\n\n\n\n\nClarity and Completeness\n\n\n\nThe degree of clarity and completeness with which the data, assumptions, methods, quality assurance, sponsoring organizations and analyses employed to generate the information are documented.\n\n\n\n\n\n\n\nUncertainty and Variability\n\n\n\nThe extent to which the variability and uncertainty (quantitative and qualitative) in the information or the procedures, measures, methods or models are evaluated and characterized.\n\n\n\n\n\n\n\nEvaluation and Review\n\n\n\nThe extent of independent verification, validation, and peer review of the information or of the procedures, measures, methods or models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchieving clarity in a project’s development becomes straightforward when using systematic planning, as almost every element of the planning process contributes to understanding how the project’s assumptions, methods, and proposed analyses will be conducted.\nPlanning for analyzing the data and information before collection clearly meets the intent of the GAFs.\nClear statements on the goals of the project developed through systematic planning leads to a better understanding of purpose and credibility of the results.\nSystematic planning leads to a clear statement of information needs and how the information will be collected, and leads to transparency in data quality."
  },
  {
    "objectID": "content/resources/data-management.html#data-quality",
    "href": "content/resources/data-management.html#data-quality",
    "title": "Data Management",
    "section": "",
    "text": "EPA Guidance on Systematic Planning Using the Data Quality Objectives Process\n\n\n\n\n\nCode\nimport pandas as pd\n\ndqo = \"../../static/resources/DQO.xlsx\"\nelements = pd.read_excel(dqo, sheet_name=0)\nfactors = pd.read_excel(dqo, sheet_name=1)\n\n\n\n\n\n\n\nCode\nelements.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nElement\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nOrganization\n\n\n\nIdentification and involvement of the project manager, sponsoring organization and responsible official, project personnel, stakeholders, scientific experts, etc. (e.g., all customers and suppliers).\n\n\n\n\n\n\n\nProject Goal\n\n\n\nDescription of the project goal, objectives, and study questions and issues.\n\n\n\n\n\n\n\nSchedule\n\n\n\nIdentification of project schedule, resources (including budget), milestones, and any applicable requirements (e.g., regulatory requirements, contractual requirements).\n\n\n\n\n\n\n\nData Needs\n\n\n\nIdentification of the type of data needed and how the data will be used to support the project’s objectives.\n\n\n\n\n\n\n\nCriteria\n\n\n\nDetermination of the quantity of data needed and specification of performance criteria for measuring quality.\n\n\n\n\n\n\n\nData Collection\n\n\n\nDescription of how and where the data will be obtained (including existing data) and identification of any constraints on data collection.\n\n\n\n\n\n\n\nQuality Assurance (QA)\n\n\n\nSpecification of needed QA and quality control (QC) activities to assess the quality performance criteria (e.g., QC samples for both field and laboratory, audits, technical assessments, performance evaluations, etc.).\n\n\n\n\n\n\n\nAnalysis\n\n\n\nDescription of how the acquired data will be analyzed (either in the field or the laboratory), evaluated (i.e., QA review/verification/validation), and assessed against its intended use and the quality performance criteria.\n\n\n\n\n\n\n\n\n\n\n\nWhen specifying the project goal (element #2 in Table 1), a key activity is to determine the key questions which the study will address once data and information are properly collected and analyzed.\n\n\n\n\n\n\nCode\nfactors.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nFactor\n\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n\nSoundness\n\n\n\nThe extent to which the scientific and technical procedures, measures, methods or models employed to generate the information are reasonable for, and consistent with, the intended application.\n\n\n\n\n\n\n\nApplicability and Utility\n\n\n\nThe extent to which the information is relevant for the Agency’s intended use.\n\n\n\n\n\n\n\nClarity and Completeness\n\n\n\nThe degree of clarity and completeness with which the data, assumptions, methods, quality assurance, sponsoring organizations and analyses employed to generate the information are documented.\n\n\n\n\n\n\n\nUncertainty and Variability\n\n\n\nThe extent to which the variability and uncertainty (quantitative and qualitative) in the information or the procedures, measures, methods or models are evaluated and characterized.\n\n\n\n\n\n\n\nEvaluation and Review\n\n\n\nThe extent of independent verification, validation, and peer review of the information or of the procedures, measures, methods or models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAchieving clarity in a project’s development becomes straightforward when using systematic planning, as almost every element of the planning process contributes to understanding how the project’s assumptions, methods, and proposed analyses will be conducted.\nPlanning for analyzing the data and information before collection clearly meets the intent of the GAFs.\nClear statements on the goals of the project developed through systematic planning leads to a better understanding of purpose and credibility of the results.\nSystematic planning leads to a clear statement of information needs and how the information will be collected, and leads to transparency in data quality."
  },
  {
    "objectID": "content/resources/data-management.html#benefits-of-using-the-dqo-process",
    "href": "content/resources/data-management.html#benefits-of-using-the-dqo-process",
    "title": "Data Management",
    "section": "Benefits of Using the DQO Process",
    "text": "Benefits of Using the DQO Process\n\nThe interaction amongst a multidisplinary team results in a clear understanding of the problem and the options available. Organizations that have used the DQO Process have found the structured format facilitated good communicaitons, documentation, and data collection design, all of which facilitated rapid peer review and approval.\n\n\nThe structure of the DQO Process provides a convenient way to document activities and decisions and to communicate the data collection design to others.\nThe DQO Process is an effective planning tool that can save resources by making data collection operations more resource-effective.\nThe DQO Process enables data users and technical experts to participate collectively in planning and to specify their needs prior to data collection. The DQO Process helps to focus studies by encouraging data users to clarify vague objectives and document clearly how scientific theory motivating this project is applicable to the intended use of the data.\nThe DQO Process provides a method for defining performance requirements appropriate for the intended use of the data by considering the consequences of drawing incorrect conclusions and then placing tolerable limits on them.\nThe DQO Process encourages good documentation for a model-based approach to investigate the objectives of a project, with discussion on how the key parameters were estimated or derived, and the robustness of the model to small perturbations. Upon implementing the DQO Process, your environmental programs can be strengthened"
  },
  {
    "objectID": "content/resources/data-management.html#data-handling",
    "href": "content/resources/data-management.html#data-handling",
    "title": "Data Management",
    "section": "Data Handling",
    "text": "Data Handling\n\nexecSQL\nPostgreSQL Tutorial\nPostgreSQL Tips\nDevDocs.io\n\n\nCharacter Encoding\nWe deal with character encoding issues when importing data to databases all the time. All databases are encoded in UTF-8, and the most common non-ASCII, non-UTF-8 format is Microsoft’s custom encoding, which is CP-1252 (also known as win-1252 and a few other things). The character encoding of a file can’t necessarily be definitively determined by examination, but the Linux command-line tool file generally does a good job. If you want to change the encoding of a file rather than importing it in a known format, the Linux command-line tool iconv will do that for you. There’s a Python library on PyPI named chardet that will also diagnose file encodings.\nFor data managers, the workflow is to first guess that encoding errors on data import are due to the file being cp-1252. That covers about 90% of cases. Our import tool also automatically diagnoses instances where a file starts with a byte order mark (BOM), which covers most of the rest of the cases. For the remainders, Geany is usually the quickest way to check the file encoding.\nEverything that comes out of our databases is always in UTF-8, so I, at least, don’t ordinarily have encoding issues when importing data to R. For those who use data from other sources, it is a good idea to document a recommended workflow and set of tools."
  },
  {
    "objectID": "content/resources/data-management.html#analytical-chemistry",
    "href": "content/resources/data-management.html#analytical-chemistry",
    "title": "Data Management",
    "section": "Analytical Chemistry",
    "text": "Analytical Chemistry\n\nSummarization\nChemistry data frequently is summarized for use in analyses or for presentation using tables or maps. Summarization is ordinarily performed when there are multiple concentration values measured for a sample, or for a specific location, date, and depth. Multiple concentration values result from field or laboratory replications, from field splits created for quality control evaluations, and sometimes from sample reanalyses. Although field splits and laboratory replicates are created to support data quality assessments, all of the valid results that are produced are informative, are ordinarily stored in the project database, and are used to produce the most accurate possible estimate of the true concentration in a sample. When there are replicate results for a sample, the data will be averaged in a stepwise, or hierarchical, fashion. Because each level of the hierarchy represents a different source of variation, all the results at a single level are averaged together before results are averaged across levels. The different levels of replication, and the source of variation that each represents, are as follows:\n\nAverage across lab replicates\nAverage across multiple analyses (if they exist) for the same lab sample. This will average across multiple methods and reanalyses (different SDGs)\nAverage across multiple lab samples (if they exist) for the same sample number (split) and lab. Multiple lab samples may exist for the same sample number if the laboratory has created new laboratory sample IDs for reanalyses.\nAverage across multiple splits (if they exist) for the same lab and interpretive sample. Multiple sample numbers (splits) may exist for the same lab, or the same sample number may exist for different labs. Splits sent to different labs potentially have an additional source of variability in addition to variability between splits sent to the same lab.\nAverage across laboratories for the same main sample. This summarizes data by main sample (sample_id), which is typically the lowest level of data summarization to be used for data interpretation.\n\nBy default, data are summarized by successive averaging across these levels of replication, in the order given above. During the averaging process, data validation qualifiers and significant digits must be propagated. The rules for propagating the data validation qualifiers U (undetected), J (estimated), and R (rejected) are as follows:\n\nIf both detected and undetected data are to be averaged, then undetected data lower than the highest detected value will be taken at one-half the detection limit and averaged with the detected data, and the result will be identified as detected. Non-detects that are higher than the highest detected value will be omitted from the average.\nIf all data to be averaged are undetected, the result will be taken to be the lowest detection limit, and will be identified as undetected.\nIf J-qualified data are averaged with non-J-qualified data, the result will be J-qualified.\nIf R-qualified data are averaged with non-R-qualified data, the result will be R-qualified.\n\nSignificant digits are propagated so that the place (in the sense of one’s place, ten’s place, etc.) of the least significant digit of the average is equal to the highest place of the least significant digit of any of the values that are averaged.\nThese rules are built into custom aggregate functions in IDB that use the measurement_result data type.\nThese default data handling rules will be applied if no project-specific alternate rules are specified. The project manager, project technical staff, and data manager should evaluate, at the start of a project, whether an alternative approach is needed. Alternate data summarization rules should be summarized in the project plan or in the data management plan, if it exists. (Data managers: if not documented elsewhere, record this information in the Data Manager’s Manual.)\nNote that the handling of nondetects during hierarchical averaging and the presentation of nondetects in data summaries may be different. Regardless of whether nondetects are taken at half the detection limit or the full detection limit when averaging, the summarized result may be presented with either the full detection limit or half the detection limit. Reporting nondetects at the full detection limit should ordinarily be done when preparing data tables for reports or other deliverables. Data analyses to be conducted by Integral may be carried out using either half or full detection limits. The method of reporting nondetects should be specified when requesting data summaries."
  },
  {
    "objectID": "content/resources/data-management.html#chemistry",
    "href": "content/resources/data-management.html#chemistry",
    "title": "Data Management",
    "section": "Chemistry",
    "text": "Chemistry\n\nResources\n\nHazardous Waste Test Methods\nNational Environmental Methods Index\nSubstance Registry Service\nEIM Valid Values\nVerification and Validation\nQualifiers\nData Review\nChemical Lists\nPCBs\nWashington Water Resources Data Defs\nMeasurement Basis Conversions\nhttps://pubs.usgs.gov/wdr/WDR-WA-03-1/pdf/ADR_N.pdf\nhttp://www.eccsmobilelab.com/resources/literature/?Id=117\nConversions\n\n\n\nCode\nimport pandas as pd\n\nwb = \"../../static/resources/Chemical_Lists.xls\"\n\nchem_lists = pd.read_excel(wb, sheet_name=0)\ncwa = pd.read_excel(wb, sheet_name=1)\npfas = pd.read_excel(wb, sheet_name=2)\npesticides = pd.read_excel(wb, sheet_name=3)\npcb = pd.read_excel(wb, sheet_name=4)\npah = pd.read_excel(wb, sheet_name=5)\nfertilizers = pd.read_excel(wb, sheet_name=6)\ndioxfuran = pd.read_excel(wb, sheet_name=7)\npbde = pd.read_excel(wb, sheet_name=8)\n\n\n\n\nTEFs\n\nVan den Berg source document\nVan den Berg TEF table\n\n\n\nChemical Groups\n\nDioxin & Furans\nReference\n\n\nPCBs\nLearn about PCBs\n\nGeneral\nPCBs are a group of man-made organic chemicals consisting of carbon, hydrogen and chlorine atoms. The number of chlorine atoms and their location in a PCB molecule determine many of its physical and chemical properties. PCBs have no known taste or smell, and range in consistency from an oil to a waxy solid.\nPCBs belong to a broad family of man-made organic chemicals known as chlorinated hydrocarbons. PCBs were domestically manufactured from 1929 until manufacturing was banned in 1979. They have a range of toxicity and vary in consistency from thin, light-colored liquids to yellow or black waxy solids. Due to their non-flammability, chemical stability, high boiling point and electrical insulating properties, PCBs were used in hundreds of industrial and commercial applications including:\n\nElectrical, heat transfer and hydraulic equipment\nPlasticizers in paints, plastics and rubber products\nPigments, dyes and carbonless copy paper\nOther industrial applications\n\nCommercial Uses for PCBs\nAlthough no longer commercially produced in the United States, PCBs may be present in products and materials produced before the 1979 PCB ban. Products that may contain PCBs include:\n\nTransformers and capacitors\nElectrical equipment including voltage regulators, switches, re-closers, bushings, and electromagnets\nOil used in motors and hydraulic systems\nOld electrical devices or appliances containing PCB capacitors\nFluorescent light ballasts\nCable insulation\nThermal insulation material including fiberglass, felt, foam, and cork\nAdhesives and tapes\nOil-based paint\nCaulking\nPlastics\nCarbonless copy paper\nFloor finish\n\nThe PCBs used in these products were chemical mixtures made up of a variety of individual chlorinated biphenyl components known as congeners. Most commercial PCB mixtures are known in the United States by their industrial trade names, the most common being Arochlor.\nRelease and Exposure of PCBs\nToday, PCBs can still be released into the environment from:\n\nPoorly maintained hazardous waste sites that contain PCBs\nIllegal or improper dumping of PCB wastes\nLeaks or releases from electrical transformers containing PCBs\nDisposal of PCB-containing consumer products into municipal or other landfills not designed to handle hazardous waste\nBurning some wastes in municipal and industrial incinerators\n\nPCBs do not readily break down once in the environment. They can remain for long periods cycling between air, water and soil. PCBs can be carried long distances and have been found in snow and sea water in areas far from where they were released into the environment. As a consequence, they are found all over the world. In general, the lighter the form of PCB, the further it can be transported from the source of contamination.\nPCBs can accumulate in the leaves and above-ground parts of plants and food crops. They are also taken up into the bodies of small organisms and fish. As a result, people who ingest fish may be exposed to PCBs that have bioaccumulated in the fish they are ingesting.\nThe National Center for Health Statistics, a division of the Centers for Disease Control and Prevention, conducts the National Health and Nutrition Examination Surveys (NHANES). NHANES is a series of U.S. national surveys on the health and nutrition status of the noninstitutionalized civilian population, which includes data collection on selected chemicals. Interviews and physical examinations are conducted with approximately 10,000 people in each two-year survey cycle. PCBs are one of the chemicals where data are available from the NHANES surveys.\n\n\nPCB Congeners\nA PCB congener is any single, unique well-defined chemical compound in the PCB category. The name of a congener specifies the total number of chlorine substituents, and the position of each chlorine. For example: 4,4’-Dichlorobiphenyl is a congener comprising the biphenyl structure with two chlorine substituents - one on each of the #4 carbons of the two rings. In 1980, a numbering system was developed which assigned a sequential number to each of the 209 PCB congeners.\n\n\nPCB Homologs\nHomologs are subcategories of PCB congeners that have equal numbers of chlorine substituents. For example, the tetrachlorobiphenyls are all PCB congeners with exactly 4 chlorine substituents that can be in any arrangement.\n\n\nPCB Aroclor\nAroclor is a PCB mixture produced from approximately 1930 to 1979. It is one of the most commonly known trade names for PCB mixtures. There are many types of Aroclors and each has a distinguishing suffix number that indicates the degree of chlorination. The numbering standard for the different Aroclors is as follows:\n\nThe first two digits usually refer to the number of carbon atoms in the phenyl rings (for PCBs this is 12)\nThe second two numbers indicate the percentage of chlorine by mass in the mixture. For example, the name Aroclor 1254 means that the mixture contains approximately 54% chlorine by weight.\n\n\n\n\n\nQualifiers\n\nLabs may apply whatever flags they want to a result. Some data qualifiers are defined by EPA’s Functional Guidelines documents, which describe how data validation is to be conducted, and the use and interpretation of U, J, and R qualifiers is pretty universal (but older standards for Puget Sound data used E instead of J). Because the U, J, and R qualifiers are pretty universal and have implications for data usability, they are the only ones that are represented as Boolean fields in the meas_value column. All lab flags are put into the lab_flags column, and there is no lookup table for them, and there is no defined use for them. Similarly, the validator qualifiers (U, J, R, and possibly others) are put in the validator_flags column. If any of those three common qualifiers is in the validator_flags column, then the corresponding flags in meas_value should be set. The lab_conc_qual column is something of a relic, left over from the days when data were commonly provided in EPA’s Contract Laboratory Program (CLP) data format, which had a corresponding column. The lab_conc_qual column was meant to either contain “U” or be null. We don’t ordinarily use that column any more. Of the qualifiers you listed above, other than U, J, and R, N is commonly used to flag a tentatively identified compound, which means that the analyte code itself is uncertain. The d_labresult.tic column is meant to hold that information. The tic column is not part of the measurement_result data type because it is not used in any way during data aggregation (averaging or summing). I see that Jerry added other flags and qualifiers to the e_concqual table, but needn’t—really shouldn’t—be there. The e_concqual dictionary should have only “U” defined. It may seem odd to define a lookup table for only one value when a check constraint on the concentration qualifier columns could be used instead, but it’s easier to check relational constraints than to check check constraints programmatically.\n\n\n\nDuplicates\n“Duplicate” is a somewhat ambiguous term, but in practice it most commonly refers to field duplicates, which we ordinarily refer to as splits to avoid that ambiguity. Some QC data, particularly spikes, are frequently duplicated, so when we have lab QC data we may have values for spikes and spike duplicates. When we receive lab results in one big flat table that includes both analytical results for natural samples and results of lab QC samples, the word or code “DUP” in a column header or table cell could mean a couple of different things. Without seeing the original data source, I’m not sure where the “DUP” code in the “labqc_samp” column of your “d_labsample” table came from. I’m going to assume that it refers to a field duplicate, and not a spike duplicate.\nIdeally, samples are submitted to the laboratory “blind” so that the laboratory does not know which field samples are duplicates of one another. This is to prevent them from seeing that there’s a lot of variation between some pair of duplicates and deciding to re-run one or both of them. If the lab is producing highly variable data, we don’t want them to be able to hide it. Unfortunately, many field sampling programs use a suffix of “-D” or “-DUP” or something like that on the sample ID, so the lab knows which samples are field duplicates. If they know, they may pass that information back in their EDD.\nAlthough field duplicates are used as a QC check on laboratory performance, they are not lab QC samples themselves. They are just normal field samples (which have been split), and don’t need to have a laboratory QC sample ID assigned to them. Thus, field duplicates should not be listed in the “d_labqcsamp” table, so that table looks fine as it appears below. The same is true for the “d_labresult” table.\nThere are a couple of things to be changed about the “d_labsample” table as shown below:\n\nThe values in the “labqc_samp” column should be identifiers that appear in the “d_labqcsamp” table, not codes. The codes for the lab QC type should be in the d_labqcsamp.qc_type column, and neither “Natural” nor “DUP” should be used there.\nThe “d_labsample” table should have values in the “study_id” and “sample_no” columns, or a value in the “labqc_samp” column, but not both. There are other invalid combinations of columns also. The “d_labsample” table may have any one of the following tables as a parent: d_sampsplit, d_fldqcsplit, d_labqcsamp, d_bioaccum_samp, d_samptreatsplit, or d_bioasrepsamp. The “ck_one_sample” check constraint on the table enforces this rule. Check constraints like this are not run by the upsert scripts, so a set of staged data may pass all the checks performed by the upsert script and yet the INSERT into d_labsample will fail.\n\n\n\nMeasurement Basis\nOrgMassSpecR\n\nData for soil and sediment are almost always reported on a dry-weight basis. If there’s anything to indicate a different basis, that deserves a closer look. Almost the only legitimate reason for a different basis for soil or sediment samples is when a leaching procedure has been applied (e.g., the Toxicity Characteristic Leaching Procedure, or TCLP); in those cases the data may be reported as the concentration in the leachate, so the basis may be “Wet” or “Whole” or “Unfiltered” – anything indicating an unfractionated liquid sample.\nData for tissues should ordinarily be reported in wet weight. Organisms’ homeostasis means that they maintain a nearly constant moisture content in their tissues, whereas the same is not true of materials like soil or sediment. If tissue data are reported in dry weight, check it carefully: labs can be sloppy about that.\nWater data are where things can be complex, because often water samples are filtered or centrifuged to remove particulates, which results in the water samples have a ‘dissolved’ basis. If the particulates are analyzed, and the results are then expressed in terms of the volume of the original sample, then the data will have a ‘particulate’ basis. Unfiltered, or whole, water, should have a basis of ‘Unfilt’, ‘Whole’, or sometimes ‘Wet’. Either of the first two of these are preferred, “Wet” is better used as a counterpart to “Dry” for soil, sediment, or tissue samples.\nThere are variations in the way things have been done in different databases. You may find that the measurement basis code for whole water samples differs from one to another, as in the third bullet above.\nIDB v.8 now has the “fraction” code, which is intended to be used to distinguish dissolved, particulate, and whole fractions of a water sample. In IDB v.8, the measurement basis for water samples will almost always be ‘Whole’. Sometimes sediment or soil samples are fractionated too, e.g., by sieving, and the fraction code should be used in those cases too, so the measurement basis will always be ‘dry’ in those cases.\nThere is an implicit association between measurement bases and units. For example, if the measurement basis is “Dry”, the units should not be “mg/L” because “…/L” implies a liquid, not a solid.\nThe measurement basis refers to the form of the sample material, which is represented in the denominator of concentration units. So codes of “Sediment” “Arsenic”, “mg/kg”, “Dry” should be read as “mg of arsenic per kg of dry sediment.”\n\n\nWet Weight\n\nWet weight (or as-is) basis means no calculation has been made to compensate for the moisture content of a sample. Wet weight refers to the weight of animal tissue or other substance including its contained water. (See also “Dry weight”)\n\n\n\nDry Weight\n\nDry weight basis means the lab has measured moisture content of a sample and calculated concentrations based on the percent solids present. Dry weight refers to the weight of animal tissue after it has been dried in an oven at 65°C until a constant weight is achieved. Dry weight represents total organic and inorganic matter in the tissue. (See also “Wet weight”).\n\n\n\nLipid\n\nLipid is any one of a family of compounds that are insoluble in water and that make up one of the principal components of living cells. Lipids include fats, oils, waxes, and steroids. Many environmental contaminants such as organochlorine pesticides are lipophilic.\n\n\n\n\n\nConversions\n\nWet to Dry\n\\[DryWt = \\frac{WetWt}{Percent Solids} * 100\\]\n\n\nDry to Wet\n\\[WetWt = DryWt * \\frac{PercentSolids}{100}\\]\n\n\nOrganic Carbon Normalization\n\\[OCnorm = \\frac{DryWt}{\\frac{PercentTOC}{100}}\\]\nDryWt & WetWt = concentration\nPercentSolids = percentage (no decimal)\n\n\nResource\n\n\n\nCalcs\n\n\n\n\n\n\nAnalytical Blanks\n\n\nTrip Blank\nThe trip blank is designed to identify levels of contamination from the exposure of the reagent or sorbent bed to the same atmospheres exposed to the analyte reagent or sorbent bed. The trip blank is prepared in the laboratory with the other reagents or adsorbents prior to shipping to the field. However, the trip blank is never exposed to the field atmospheres. It is simply sent along with the field samples to and from the site. The trip blank identified areas of exposure such as shipping temperatures and pressures, laboratory preparation of field samples and laboratory preparation of field samples for analysis.\n\nField Blank\nThe field blank is similar to the trip blank in that it is also prepared during the preparation of the field reagents or adsorbents. However, the field blank is exposed to the same atmospheres in the field as the field samples. This means that the field blank is opened during the charging of impingers or sorbents in the sample train. The field blank is also exposed during the exchanging of cartridges in SW-846, Method 0030 or when field reagents are being exchanged during a test run. In summary, field blanks consist of additional sample collection media (e.g., sorbent tubes, reagents, filters) which are transported to the monitoring site, exposed briefly at the site when the samples are exposed (but no stack gas is actually pulled through these blanks), and transported back to the laboratory for analysis, similar to a field sample. At least one field blank should be collected and analyzed for each test series.\n\n\nLaboratory Blank\nThe laboratory blank is a sample of the reagents or sorbents used during the sample train reagent preparation or recovery. The laboratory blank is a sample of the extraction solvent, the rinses used during sample recovery, or a sample from the batch of sorbent used to preparing sampling cartridges. Laboratory blanks include both method blanks and instrument blanks. Method blanks are carried through all steps of the measurement process (from extraction through analysis). A method blank is typically analyzed with each sample batch. Instrument blanks are used to demonstrate that an instrument system is free of contamination. Instrument blanks are typically analyzed prior to sample analysis and following the analysis of highly contaminated samples.\n\n\nReagent Blank\nThe reagent blank is a sample of the solvents used during recovery of the sample train after the test is completed. You recall, reagent blanks for both multi-metal and chromium +6 require that the reagent blank be the same volume as the renses used to recover the samples, from probe to impinger. This is because the blank value is substracted from the sample to obtain a final concentration.\n\n\nDiagram\n\n\n\n\nDetection Limits\nPresentation\n\nWhat affects detection limits?\n\nSample size\nConcentration of other constituents\nSample clean-up\nMethodology\nLab Performance\n\nExperience\nExtraction technique\nInstrument type and maintenance\n\n\ndetection_limit - the lowest possible value an instrument/method can sense a compound is present (think of it like a whisper - you can barely hear it, but know its there). This is better known as the “method detection limit”\nquantification_limit - the limit in which an instrument/method can actually start to quantify the amount of something which is present. If the result is between the detection_limit and the quantification_limit, the result is estimated, because the instrument/method cant confidently identify the amount of something until it reaches the quantification_limit.\nreporting_limit - usually project or dataset specific. this limit is used for data analysis/statistics. the reporting_limit is equal to either the detection_limit or quantification_limit. This is better known as the “reporting detection limit”.\n\n\nMethod Detection Limit (MDL)\n\nStatistically determined\nThe minimum concentration that can be measured with 99% confidence that the concentration is greater than zero\nConcentrations near MDL are estimates\nLaboratory, instrument, matrix, method, and analyte specific\nConcentrations at MDL expected to be a false positive 1% of the time, but false negatives 50% of the time\n\n\n\nMethod Reporting Limit (MRL)\n\nMay also be referred to as QL (quantitation limit), sample quantitation limit, or just RL (reporting limit)​\nDetermined by the lowest point of the calibration​\nNot as specific as MDL, labs can adust​\nConcentrations at MRL can be reliably quantified​\nMRL &gt; MDL​\nAlso laboratory, instrument, matrix, method, & analyte specific\n\n\n\nMDL & MRL Relationship\n\n\n\nOther Detection Limits\n\nPQL\n\nConsidered to be lowest concentration that can be reliably quantified by a method\nLimit of Detection (LOD); Lowest concentration that can be detected with a 1% false negative rate.\n\nGenerally 2x to 3X MDL\n\nLimit of Quantitation (LOQ); similar to MRL\n\n\n\nPCDD/F & PCB specific\n2.5 times signal to noise\n\nEQL: Estimated Quantitation Limit\nEDL: Estimated Detection Limit\nSDL: Sample Detection Limit\nEMPC\n\nEstimated Maximum Possible Concentration (EMPC)\nPeak present but not all of the identification criteria is met\nAlways greater than MDL, may be greater than MRL\nGenerally treated a non-detect in TEQ calculations\nEMPCs can present data management difficulties and need to be reviewed in QC checks"
  },
  {
    "objectID": "content/development/tools/miscellaneous.html",
    "href": "content/development/tools/miscellaneous.html",
    "title": "Miscellaneous",
    "section": "",
    "text": "ffmpeg -i file.mov -s 600x400 -pix_fmt rgb24 -r 20 -f gif - | gifsicle --optimize=3 --delay=3 &gt; file.gif"
  },
  {
    "objectID": "content/development/tools/miscellaneous.html#convert-.mov-to-.gif",
    "href": "content/development/tools/miscellaneous.html#convert-.mov-to-.gif",
    "title": "Miscellaneous",
    "section": "",
    "text": "ffmpeg -i file.mov -s 600x400 -pix_fmt rgb24 -r 20 -f gif - | gifsicle --optimize=3 --delay=3 &gt; file.gif"
  },
  {
    "objectID": "content/development/tools/miscellaneous.html#convert-.svg-to-.ico",
    "href": "content/development/tools/miscellaneous.html#convert-.svg-to-.ico",
    "title": "Miscellaneous",
    "section": "Convert .SVG to .ICO",
    "text": "Convert .SVG to .ICO\nUses the imagemagick utility.\nconvert -background none logo.svg -define icon:auto-size favicon.ico"
  },
  {
    "objectID": "content/development/tools/git.html",
    "href": "content/development/tools/git.html",
    "title": "Git",
    "section": "",
    "text": "https://product.hubspot.com/blog/git-and-github-tutorial-for-beginners\nhttps://docs.gitlab.com/ee/gitlab-basics/start-using-git.html"
  },
  {
    "objectID": "content/development/tools/git.html#initialize",
    "href": "content/development/tools/git.html#initialize",
    "title": "Git",
    "section": "Initialize",
    "text": "Initialize\n\nLaunch Git Bash\nNavigate to project directory\ninitialize git repository in the folder root: git init\ncreate new file in directory: touch filename.extension\nlist files in root: ls\ncheck which files git recognizes: git status"
  },
  {
    "objectID": "content/development/tools/git.html#staging",
    "href": "content/development/tools/git.html#staging",
    "title": "Git",
    "section": "Staging",
    "text": "Staging\nA commit is a record of what files you have changed since the last time you made a commit. Essentially, you make changes to your repo (for example, adding a file or modifying one) and then tell git to put those files into a commit. Commits make up the essence of your project and allow you to go back to the state of a project at any point.\nSo, how do you tell git which files to put into a commit? This is where the staging environment or index come in. When you make changes to your repo, git notices that a file has changed but won’t do anything with it (like adding it in a commit).\nTo add a file to a commit, you first need to add it to the staging environment. To do this, you can use the git add &lt;filename&gt; command.\nOnce you’ve used the git add command to add all the files you want to the staging environment, you can then tell git to package them into a commit using the git commit command. Note: The staging environment, also called ‘staging’, is the new preferred term for this, but you can also see it referred to as the ‘index’.\n\nAdd files to the staging environment: git add filename.extension\nCheck staging environment for new files: git status"
  },
  {
    "objectID": "content/development/tools/git.html#commit-locally",
    "href": "content/development/tools/git.html#commit-locally",
    "title": "Git",
    "section": "Commit Locally",
    "text": "Commit Locally\ngit commit -m \"Your message about the commit\""
  },
  {
    "objectID": "content/development/tools/git.html#branches",
    "href": "content/development/tools/git.html#branches",
    "title": "Git",
    "section": "Branches",
    "text": "Branches\nSay you want to make a new feature but are worried about making changes to the main project while developing the feature. This is where git branches come in.\nBranches allow you to move back and forth between ‘states’ of a project. For instance, if you want to add a new page to your website you can create a new branch just for that page without affecting the main part of the project. Once you’re done with the page, you can merge your changes from your branch into the master branch. When you create a new branch, Git keeps track of which commit your branch ‘branched’ off of, so it knows the history behind all the files.\n\ngit checkout -b &lt;my branch name&gt;\nShow list of branches: git branch"
  },
  {
    "objectID": "content/development/tools/git.html#commit-to-github",
    "href": "content/development/tools/git.html#commit-to-github",
    "title": "Git",
    "section": "Commit to Github",
    "text": "Commit to Github\n\nCreate new repo on GitHub\ngit remote add origin &lt;url produced on github for new repo&gt;\ngit push -u origin [master/main]"
  },
  {
    "objectID": "content/development/tools/git.html#push-a-branch-to-github",
    "href": "content/development/tools/git.html#push-a-branch-to-github",
    "title": "Git",
    "section": "Push a Branch to Github",
    "text": "Push a Branch to Github\ngit push origin &lt;my-new-branch&gt;\nYou might be wondering what that “origin” word means in the command above. What happens is that when you clone a remote repository to your local machine, git creates an alias for you. In nearly all cases this alias is called “origin.” It’s essentially shorthand for the remote repository’s URL. So, to push your changes to the remote repository, you could’ve used either the command: git push git@github.com:git/git.git yourbranchname or git push origin yourbranchname"
  },
  {
    "objectID": "content/development/tools/git.html#pull-request",
    "href": "content/development/tools/git.html#pull-request",
    "title": "Git",
    "section": "Pull Request",
    "text": "Pull Request\nA pull request (or PR) is a way to alert a repo’s owners that you want to make some changes to their code. It allows them to review the code and make sure it looks good before putting your changes on the master branch."
  },
  {
    "objectID": "content/development/tools/git.html#get-changes-on-github",
    "href": "content/development/tools/git.html#get-changes-on-github",
    "title": "Git",
    "section": "Get Changes on Github",
    "text": "Get Changes on Github\ngit pull origin master\ncheck all new commits: git log"
  },
  {
    "objectID": "content/development/tools/git.html#view-differences",
    "href": "content/development/tools/git.html#view-differences",
    "title": "Git",
    "section": "View Differences",
    "text": "View Differences\n\nrun: git diff"
  },
  {
    "objectID": "content/development/tools/git.html#remove-a-branch",
    "href": "content/development/tools/git.html#remove-a-branch",
    "title": "Git",
    "section": "Remove a Branch",
    "text": "Remove a Branch\n\nLocally\ngit branch -d &lt;branch_name&gt;\n\n\nRemote\ngit push &lt;remote_name&gt; --delete &lt;branch_name&gt;"
  },
  {
    "objectID": "content/development/tools/git.html#remove-tracked-filedirectory",
    "href": "content/development/tools/git.html#remove-tracked-filedirectory",
    "title": "Git",
    "section": "Remove tracked file/directory",
    "text": "Remove tracked file/directory\n\nFile\ngit rm --cached &lt;file&gt;\n\n\nDirectory\ngit rm --cahced -r dir/"
  },
  {
    "objectID": "content/development/tools/git.html#pre-commit",
    "href": "content/development/tools/git.html#pre-commit",
    "title": "Git",
    "section": "pre-commit",
    "text": "pre-commit\nPlease make sure to install our pre-commit hooks into your Git workflow. Pre-commit will help keep our code clean and make sure we are following best practices.\n\nInstall pre-commit hooks: python -m pre_commit install --install-hooks\nRun hooks on the entire codebase: python -m pre_commit run --all-files\n\nHooks will run on the current commit snapshot when executing a git commit. Pre-commit hooks allow us to check for potential issues and make sure we are applying standards to our code before pushing to GitHub.\nSee an example .pre-commit-config.yml"
  },
  {
    "objectID": "content/development/tools/git.html#merge",
    "href": "content/development/tools/git.html#merge",
    "title": "Git",
    "section": "Merge",
    "text": "Merge\nThe steps below can be used to merge two branches on your local machine. The braches used in this example are:\n\nmain: The authoritative or “production” code lives in this branch.\ndev: This branch is split from the main branch and a new feature or update is coded with the intent to merge changes back into the main branch.\n\n\nPull main and dev branches so local repo is up to date with the remote.\n\ngit checkout main\ngit pull origin main\ngit checkout dev\ngit pull origin dev\n\nCheckout the main branch so we can merge the dev branch into main\n\ngit checkout main\ngit merge dev\n\n\nCheck the branch status: git status\n\nEvaluate the two files with a conflict (ie. .gitignore and requirements.txt) and reconcile issues, then git add when ready.\nCommit the changes: git commit -m \"merge @tnelson-integral dev branch with main\"\nPush changes to the remote on GitHub: git push origin main\nCheck out the dev branch locally and pull the main branch changes into it so dev can be up-to-date with main\n\ngit checkout dev\ngit pull origin main\ngit push origin dev"
  },
  {
    "objectID": "content/development/server/rstudio-server.html",
    "href": "content/development/server/rstudio-server.html",
    "title": "Rstudio Server",
    "section": "",
    "text": "Install:\nwget https://download2.rstudio.org/server/jammy/amd64/rstudio-server-2022.07.2-576-amd64.deb && \\\nsudo gdebi rstudio-server-2022.07.2-576-amd64.deb && \\\nrm rstudio-server-2022.07.2-576-amd64.deb && \\\nsudo adduser rstudio\nApache config\n&lt;VirtualHost *:80&gt;\n    ServerName &lt;DNS ENTRY&gt;\n    ServerSignature Off\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n\n    ProxyPreserveHost On\n    ProxyPass \"/\" \"http://localhost:8787/\"\n    ProxyPassReverse \"/\" \"http://localhost:8787/\"\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =&lt;DNS ENTRY&gt;\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n&lt;/VirtualHost&gt;\n\n&lt;IfModule mod_ssl.c&gt;\n    &lt;VirtualHost *:443&gt;\n        ServerName &lt;DNS ENTRY&gt;\n        ServerSignature Off\n\n        ErrorLog /var/log/apache2/redirect.error.log\n        LogLevel warn\n\n        ProxyPreserveHost On\n        ProxyPass \"/\" \"http://localhost:8787/\"\n        ProxyPassReverse \"/\" \"http://localhost:8787/\"\n\n        SSLCertificateFile /etc/letsencrypt/live/&lt;DNS ENTRY&gt;/fullchain.pem\n        SSLCertificateKeyFile /etc/letsencrypt/live/&lt;DNS ENTRY&gt;/privkey.pem\n        Include /etc/letsencrypt/options-ssl-apache.conf\n    &lt;/VirtualHost&gt;\n&lt;/IfModule&gt;\nGenerate SSL certs\nsudo certbot --apache certonly\nEnable the site\nsudo a2ensite rstudio.conf && \\\nsudo systemctl reload apache2 && \\\nsudo systemctl status apache2"
  },
  {
    "objectID": "content/development/server/jupyter-server.html",
    "href": "content/development/server/jupyter-server.html",
    "title": "Jupyter Server",
    "section": "",
    "text": "Create jupyter user\nsudo adduser jupyter && \\\nsudo usermod -a -G staff jupyter\nsudo su jupyter && \\\nInstall Jupyter Lab\nsource /home/jupyter/.venv/bin/activate && \\\npython -m pip install jupyterlab && \\\njupyter-lab --generate-config\nConfigure Jupyter\nc.NotebookApp.ip = \"*\"\nc.NotebookApp.notebook_dir = \"/home/jupyter/notebooks/\"\nc.NotebookApp.open_browser = False\nc.NotebookApp.password = \"\"  # hashed password\nc.NotebookApp.port = 9999\nConfigure Apache:\n&lt;VirtualHost *:80&gt;\n    ServerName &lt;DNS ENTRY&gt;\n    ServerSignature Off\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n\n    ProxyPreserveHost On\n    ProxyPass \"/\" \"http://localhost:9999/\"\n    ProxyPassReverse \"/\" \"http://localhost:9999/\"\n\n    RewriteEngine on\n    RewriteCond %{SERVER_NAME} =&lt;DNS ENTRY&gt;\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n&lt;/VirtualHost&gt;\n\n&lt;IfModule mod_ssl.c&gt;\n    &lt;VirtualHost *:443&gt;\n        ServerName &lt;DNS ENTRY&gt;\n        ServerSignature Off\n\n        ErrorLog /var/log/apache2/redirect.error.log\n        LogLevel warn\n\n        ProxyPreserveHost On\n        ProxyPass \"/\" \"http://localhost:9999/\"\n        ProxyPassReverse \"/\" \"http://localhost:9999/\"\n\n    &lt;Location \"/api/kernels/\"&gt;\n        ProxyPass        ws://localhost:9999/api/kernels/\n            ProxyPassReverse ws://localhost:9999/api/kernels/\n    &lt;/Location&gt;\n\n        SSLCertificateFile /etc/letsencrypt/live/&lt;DNS ENTRY&gt;/fullchain.pem\n        SSLCertificateKeyFile /etc/letsencrypt/&lt;DNS ENTRY&gt;/privkey.pem\n        Include /etc/letsencrypt/options-ssl-apache.conf\n    &lt;/VirtualHost&gt;\n&lt;/IfModule&gt;\nEnable Apache modules\nsudo a2enmod proxy\nsudo a2enmod proxy_http\nsudo a2enmod proxy_wstunnel\nGenerate SSL certs\nsudo certbot --apache certonly\nEnable the site\nsudo a2ensite jupyter.conf && \\\nsudo systemctl reload apache2 && \\\nsudo systemctl status apache2\nCreate the Jupyter service: /lib/systemd/system/jupyter.service\n# service name:     jupyter.service\n# path:             /lib/systemd/system/jupyter.service\n\n[Unit]\nDescription=Jupyter Notebook Server\n\n[Service]\nType=simple\nPIDFile=/run/jupyter.pid\nExecStart=/bin/bash -c \"/home/jupyter/.venv/bin/jupyter lab --no-browser\"\nUser=jupyter\nGroup=staff\nWorkingDirectory=/home/jupyter/notebooks\nRestart=always\nRestartSec=30\n\n[Install]\nWantedBy=multi-user.target\nEnable the service\nsudo systemctl daemon-reload && \\\nsudo systemctl start jupyter.service && \\\nsudo service jupyter status"
  },
  {
    "objectID": "content/development/index.html",
    "href": "content/development/index.html",
    "title": "Best Practices",
    "section": "",
    "text": "General coding best practices are a set of guidelines and recommendations that help developers write clean, efficient, and maintainable code. While specific practices may vary depending on the programming language and the project’s requirements, here are some general coding best practices:\n\nConsistent and meaningful naming: Use descriptive names for variables, functions, classes, and other entities to make the code self-explanatory. Maintain consistency in naming conventions throughout the codebase.\nCode readability: Write code that is easy to read and understand. Use proper indentation, spacing, and formatting. Add comments to explain complex logic or important details.\nModular and reusable code: Break down the code into smaller, logical modules or functions that perform specific tasks. This improves code maintainability, readability, and allows for code reuse.\nDon’t repeat yourself (DRY): Avoid duplicating code. Instead, create reusable functions or abstractions to eliminate redundancy. This reduces the chances of introducing bugs and makes code maintenance easier.\nKeep functions and methods small: Aim for small, focused functions or methods that perform a single task. This improves code readability, testability, and makes it easier to reason about the behavior of the code.\nFollow the Single Responsibility Principle (SRP): Each module, class, or function should have a single responsibility. Separating concerns helps in maintaining and modifying code without impacting other parts of the system.\nError handling and validation: Implement proper error handling and input validation to make the code more robust. Validate user inputs, handle exceptions gracefully, and provide meaningful error messages.\nUse version control: Utilize a version control system (e.g., Git) to track changes to the codebase, collaborate with others, and easily revert or review code changes when needed.\nCode reviews: Encourage peer code reviews to catch bugs, ensure adherence to best practices, and maintain code quality. Reviews provide an opportunity to share knowledge and improve the overall quality of the codebase.\nTesting: Write automated tests to validate the behavior and correctness of the code. Unit tests, integration tests, and other testing techniques help catch bugs early, ensure code stability, and facilitate refactoring.\nPerformance optimization: Optimize code for performance when necessary. Identify bottlenecks, use appropriate data structures and algorithms, and minimize unnecessary computations or operations.\nSecurity considerations: Follow security best practices to protect against common vulnerabilities. Sanitize user inputs, use prepared statements or parameterized queries to prevent SQL injection, and encrypt sensitive data.\nDocumentation: Document the code to provide insights into its functionality, usage, and any specific requirements. Use inline comments, README files, and documentation tools to make it easier for others to understand and use the code.\nKeep up with best practices: Stay updated with the latest best practices, coding standards, and programming language conventions. Regularly learn and improve your coding skills to write better code over time.\n\nRemember that best practices may vary depending on the specific programming language, domain, and project requirements. It’s important to adapt and adjust these practices as needed for each situation."
  },
  {
    "objectID": "content/development/index.html#best-practices",
    "href": "content/development/index.html#best-practices",
    "title": "Best Practices",
    "section": "",
    "text": "General coding best practices are a set of guidelines and recommendations that help developers write clean, efficient, and maintainable code. While specific practices may vary depending on the programming language and the project’s requirements, here are some general coding best practices:\n\nConsistent and meaningful naming: Use descriptive names for variables, functions, classes, and other entities to make the code self-explanatory. Maintain consistency in naming conventions throughout the codebase.\nCode readability: Write code that is easy to read and understand. Use proper indentation, spacing, and formatting. Add comments to explain complex logic or important details.\nModular and reusable code: Break down the code into smaller, logical modules or functions that perform specific tasks. This improves code maintainability, readability, and allows for code reuse.\nDon’t repeat yourself (DRY): Avoid duplicating code. Instead, create reusable functions or abstractions to eliminate redundancy. This reduces the chances of introducing bugs and makes code maintenance easier.\nKeep functions and methods small: Aim for small, focused functions or methods that perform a single task. This improves code readability, testability, and makes it easier to reason about the behavior of the code.\nFollow the Single Responsibility Principle (SRP): Each module, class, or function should have a single responsibility. Separating concerns helps in maintaining and modifying code without impacting other parts of the system.\nError handling and validation: Implement proper error handling and input validation to make the code more robust. Validate user inputs, handle exceptions gracefully, and provide meaningful error messages.\nUse version control: Utilize a version control system (e.g., Git) to track changes to the codebase, collaborate with others, and easily revert or review code changes when needed.\nCode reviews: Encourage peer code reviews to catch bugs, ensure adherence to best practices, and maintain code quality. Reviews provide an opportunity to share knowledge and improve the overall quality of the codebase.\nTesting: Write automated tests to validate the behavior and correctness of the code. Unit tests, integration tests, and other testing techniques help catch bugs early, ensure code stability, and facilitate refactoring.\nPerformance optimization: Optimize code for performance when necessary. Identify bottlenecks, use appropriate data structures and algorithms, and minimize unnecessary computations or operations.\nSecurity considerations: Follow security best practices to protect against common vulnerabilities. Sanitize user inputs, use prepared statements or parameterized queries to prevent SQL injection, and encrypt sensitive data.\nDocumentation: Document the code to provide insights into its functionality, usage, and any specific requirements. Use inline comments, README files, and documentation tools to make it easier for others to understand and use the code.\nKeep up with best practices: Stay updated with the latest best practices, coding standards, and programming language conventions. Regularly learn and improve your coding skills to write better code over time.\n\nRemember that best practices may vary depending on the specific programming language, domain, and project requirements. It’s important to adapt and adjust these practices as needed for each situation."
  },
  {
    "objectID": "content/development/code/regex.html",
    "href": "content/development/code/regex.html",
    "title": "Regex",
    "section": "",
    "text": "Quickstart\nRegex101"
  },
  {
    "objectID": "content/development/code/regex.html#metacharacters-need-to-be-escaped",
    "href": "content/development/code/regex.html#metacharacters-need-to-be-escaped",
    "title": "Regex",
    "section": "MetaCharacters (Need to be escaped)",
    "text": "MetaCharacters (Need to be escaped)\n. ^ $ * + ? { } [ ] \\ | ( )"
  },
  {
    "objectID": "content/development/code/regex.html#characters",
    "href": "content/development/code/regex.html#characters",
    "title": "Regex",
    "section": "Characters",
    "text": "Characters\n. - Any Character Except New Line \\d - Digit (0-9) \\D - Not a Digit (0-9) \\w - Word Character (a-z, A-Z, 0-9, _) \\W - Not a Word Character \\s - Whitespace (space, tab, newline) \\S - Not Whitespace (space, tab, newline)"
  },
  {
    "objectID": "content/development/code/regex.html#character-classes",
    "href": "content/development/code/regex.html#character-classes",
    "title": "Regex",
    "section": "Character Classes",
    "text": "Character Classes\n[] - Matches Characters in brackets [^ ] - Matches Characters NOT in brackets [a-z] - Any lowercase character between a and z [A-Z] - Any UPPERCASE character between A and Z"
  },
  {
    "objectID": "content/development/code/regex.html#quantifiers",
    "href": "content/development/code/regex.html#quantifiers",
    "title": "Regex",
    "section": "Quantifiers",
    "text": "Quantifiers\n* - 0 or More + - 1 or More ? - 0 or One {3} - Exact Number {3,4} - Range of Numbers (Minimum, Maximum) {3,} - At least 3"
  },
  {
    "objectID": "content/development/code/regex.html#anchors-boundaries",
    "href": "content/development/code/regex.html#anchors-boundaries",
    "title": "Regex",
    "section": "Anchors & Boundaries",
    "text": "Anchors & Boundaries\n\\b - Word Boundary \\B - Not a Word Boundary ^ - Beginning of a String $ - End of a String"
  },
  {
    "objectID": "content/development/code/regex.html#logic",
    "href": "content/development/code/regex.html#logic",
    "title": "Regex",
    "section": "Logic",
    "text": "Logic\n| - Either Or ( ) - Group \\1 - Contents of group 1"
  },
  {
    "objectID": "content/development/code/regex.html#white-space",
    "href": "content/development/code/regex.html#white-space",
    "title": "Regex",
    "section": "White-space",
    "text": "White-space\n\\t - Tab \\r - Carriage return \\n - New line"
  },
  {
    "objectID": "content/development/code/regex.html#snippets",
    "href": "content/development/code/regex.html#snippets",
    "title": "Regex",
    "section": "Snippets",
    "text": "Snippets\n\nMarkdown link pattern\n\\[([^\\]]+)\\]\\(([^\\)]+)\\)"
  },
  {
    "objectID": "content/development/code/notebooks/index.html",
    "href": "content/development/code/notebooks/index.html",
    "title": "Notebooks",
    "section": "",
    "text": "Data Science\n\n\nIntro to data science with Python\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGRIB\n\n\nManipulating gridded datasets\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogging\n\n\nBasics of the standard library logging module\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML Notes\n\n\nMachine Learning (supervised learning) notes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nML Training\n\n\nTest a machine learning model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPySpark Practice\n\n\nIntroduction to Apache Spark | PySpark\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython Fundamentals\n\n\nConcepts and methods on the fundamentals of Python\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSleep Data\n\n\nLoad sleep data into PostgreSQL collected from the Sleep Cycle phone app\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVisualization with Seaborn\n\n\nExploring Matplotlib + Seaborn visualization capabilities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb Scraping\n\n\nDownload .grb2 files from WAVEWATCH III web server\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/development/code/html.html",
    "href": "content/development/code/html.html",
    "title": "HTML",
    "section": "",
    "text": "Below is an example of a basic document header that includes local javascript and css document resources, plus hosted Bootstrap and jQuery libraries.\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\" dir=\"ltr\"&gt;\n\n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\" /&gt;\n        &lt;title&gt;Bootstrap 5&lt;/title&gt;\n        &lt;!-- Bootstrap CSS --&gt;\n        &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\" /&gt;\n        &lt;!-- Local assets --&gt;\n        &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"assets/css/main.css\" /&gt;\n        &lt;!-- Favicon --&gt;\n        &lt;link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"\" /&gt;\n        &lt;!-- jQuery --&gt;\n        &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js\"&gt;&lt;/script&gt;\n        &lt;!-- Bootstrap JS and Popper --&gt;\n        &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n        &lt;!-- Local scripts --&gt;\n        &lt;script type=\"text/javascript\" src=\"assets/js/main.js\"&gt;&lt;/script&gt;\n    &lt;/head&gt;\n\n    &lt;body&gt;\n    &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "content/development/code/html.html#document-header",
    "href": "content/development/code/html.html#document-header",
    "title": "HTML",
    "section": "",
    "text": "Below is an example of a basic document header that includes local javascript and css document resources, plus hosted Bootstrap and jQuery libraries.\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"en\" dir=\"ltr\"&gt;\n\n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\" /&gt;\n        &lt;title&gt;Bootstrap 5&lt;/title&gt;\n        &lt;!-- Bootstrap CSS --&gt;\n        &lt;link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/css/bootstrap.min.css\" rel=\"stylesheet\" integrity=\"sha384-EVSTQN3/azprG1Anm3QDgpJLIm9Nao0Yz1ztcQTwFspd3yD65VohhpuuCOmLASjC\" crossorigin=\"anonymous\" /&gt;\n        &lt;!-- Local assets --&gt;\n        &lt;link rel=\"stylesheet\" type=\"text/css\" href=\"assets/css/main.css\" /&gt;\n        &lt;!-- Favicon --&gt;\n        &lt;link rel=\"shortcut icon\" type=\"image/x-icon\" href=\"\" /&gt;\n        &lt;!-- jQuery --&gt;\n        &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js\"&gt;&lt;/script&gt;\n        &lt;!-- Bootstrap JS and Popper --&gt;\n        &lt;script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.0.2/dist/js/bootstrap.bundle.min.js\" integrity=\"sha384-MrcW6ZMFYlzcLA8Nl+NtUVF0sA7MsXsP1UyJoMp4YLEuNSfAP+JcXn/tWtIaxVXM\" crossorigin=\"anonymous\"&gt;&lt;/script&gt;\n        &lt;!-- Local scripts --&gt;\n        &lt;script type=\"text/javascript\" src=\"assets/js/main.js\"&gt;&lt;/script&gt;\n    &lt;/head&gt;\n\n    &lt;body&gt;\n    &lt;/body&gt;\n&lt;/html&gt;"
  },
  {
    "objectID": "content/development/code/bash.html",
    "href": "content/development/code/bash.html",
    "title": "Bash",
    "section": "",
    "text": "sudo mount -t cifs //10.10.145.5/SourceDir /home/&lt;user&gt;/mnt/destination -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "content/development/code/bash.html#mount-a-share",
    "href": "content/development/code/bash.html#mount-a-share",
    "title": "Bash",
    "section": "",
    "text": "sudo mount -t cifs //10.10.145.5/SourceDir /home/&lt;user&gt;/mnt/destination -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "content/development/code/bash.html#generating-an-ssh-key",
    "href": "content/development/code/bash.html#generating-an-ssh-key",
    "title": "Bash",
    "section": "Generating an SSH Key",
    "text": "Generating an SSH Key\ncd ~/.ssh\n# Generate the Key\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\"\n# Copy th ekey\ncat gh_actions.pub &gt;&gt; ~/.ssh/authorized_keys\n# Copy the key:\ncat ~/.ssh/gh_actions"
  },
  {
    "objectID": "content/blog/posts/mounting-a-windows-share/index.html",
    "href": "content/blog/posts/mounting-a-windows-share/index.html",
    "title": "Mounting a Windows Share",
    "section": "",
    "text": "The command below can be used to create a temporary mount point although the mount will disassociate on next server reboot.\nmkdir /home/&lt;user&gt;/mnt/DataManagement && \\\nsudo mount -t cifs //10.10.145.5/DataManagement /home/&lt;user&gt;/mnt/DataManagement -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "content/blog/posts/mounting-a-windows-share/index.html#quick-reference",
    "href": "content/blog/posts/mounting-a-windows-share/index.html#quick-reference",
    "title": "Mounting a Windows Share",
    "section": "",
    "text": "The command below can be used to create a temporary mount point although the mount will disassociate on next server reboot.\nmkdir /home/&lt;user&gt;/mnt/DataManagement && \\\nsudo mount -t cifs //10.10.145.5/DataManagement /home/&lt;user&gt;/mnt/DataManagement -o username=&lt;user&gt;,password=\"\",uid=1000"
  },
  {
    "objectID": "content/blog/posts/mounting-a-windows-share/index.html#persistent-mounts",
    "href": "content/blog/posts/mounting-a-windows-share/index.html#persistent-mounts",
    "title": "Mounting a Windows Share",
    "section": "Persistent Mounts",
    "text": "Persistent Mounts\n\nCreate a mount point to a Windows share name (e.g. Q): sudo mkdir /mnt/share/Q\nAppend the following similar line to /etc/fstab– to associate the mount automatically upon machine boot:\n//192.168.50.19/Transfer /mnt/share/Q cifs credentials=/home/DatamanBU/.smbpasswd,uid=1011,gid=1005,dir_mode=0775,file_mode=0775,rw 0 0\nNotes:\n\nuid=1011: mount as user username. Find UID with id -u &lt;username&gt;\ngid=1005: mount as group datamgrs\ndir_mode=0775, file_mode=0775: readable/writeable by any users that are members of datamgrs group\nThe credentials file /home/DatamanBU/.smpbasswd must already exist (or be created) by root. It contains 2 lines:\nusername=DatamanBU\npassword=password\nMake the credentials file only readable by root: sudo chmod 700 /home/DatamanBU/.smbpasswd\n\nAssociate the mount immediately: sudo mount -a -v -t cifs"
  },
  {
    "objectID": "content/blog/posts/git-merge/index.html",
    "href": "content/blog/posts/git-merge/index.html",
    "title": "Git Merge",
    "section": "",
    "text": "The steps below can be used to merge two branches on your local machine. The braches used in this example are:\n\nmain: The authoritative or “production” code lives in this branch.\ndev: This branch is split from the main branch and a new feature or update is coded with the intent to merge changes back into the main branch.\n\n\nPull main and dev branches so local repo is up to date with the remote.\n\ngit checkout main\ngit pull origin main\ngit checkout dev\ngit pull origin dev\n\nCheckout the main branch so we can merge the dev branch into main\n\ngit checkout main\ngit merge dev\n\n\nCheck the branch status: git status\n\nEvaluate the two files with a conflict (ie. .gitignore and requirements.txt) and reconcile issues, then git add when ready.\nCommit the changes: git commit -m \"merge @tnelson-integral dev branch with main\"\nPush changes to the remote on GitHub: git push origin main\nCheck out the dev branch locally and pull the main branch changes into it so dev can be up-to-date with main\n\ngit checkout dev\ngit pull origin main\ngit push origin dev"
  },
  {
    "objectID": "content/blog/posts/accessing-quarto-variables/index.html",
    "href": "content/blog/posts/accessing-quarto-variables/index.html",
    "title": "Accessing Quarto Variables",
    "section": "",
    "text": "You can access dynamic variables within documents, which can be useful for externalizing content that varies depending on context. As an example, you can reference file metadata using the syntax {&lt; meta title &gt;}, which would include the title of this article: Accessing Quarto Variables.\nIsn’t that cool?"
  },
  {
    "objectID": "content/blog/index.html",
    "href": "content/blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\nAuthor\n\n\nReading Time\n\n\nCategories\n\n\n\n\n\n\n\n\n\nJul 5, 2023\n\n\nMounting a Windows Share\n\n\nMount a windows share drive to a Linux machine\n\n\nCaleb Grant\n\n\n2 min\n\n\nLinux\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nQuarto Tabsets\n\n\nCreating tabsets with Quarto\n\n\nCaleb Grant\n\n\n1 min\n\n\nQuarto\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nGit Merge\n\n\nMerging git branches locally\n\n\nCaleb Grant\n\n\n2 min\n\n\nGit\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nLinux Permissions\n\n\nLinux permission cheatsheet and examples\n\n\nCaleb Grant\n\n\n1 min\n\n\nLinux\n\n\n\n\n\n\n\nJun 29, 2023\n\n\nCreating a Blog Post\n\n\nThis is an example of how to create a new blog post.\n\n\nCaleb Grant\n\n\n1 min\n\n\nQuarto,Blog\n\n\n\n\n\n\n\nJun 15, 2023\n\n\nAccessing Quarto Variables\n\n\nAccess quarto variables in a document.\n\n\nCaleb Grant\n\n\n1 min\n\n\nQuarto,Variables\n\n\n\n\n\n\nNo matching items\n\n\n\n RSS"
  },
  {
    "objectID": "content/blog/posts/creating-a-post/index.html",
    "href": "content/blog/posts/creating-a-post/index.html",
    "title": "Creating a Blog Post",
    "section": "",
    "text": "To add a post, create a copy of blog/posts/_template, modify the folder name with the title of the post, then update the index.qmd with your content. New posts under blog/posts will show up in the blog listings when the site is rendered."
  },
  {
    "objectID": "content/blog/posts/linux-permissions/index.html",
    "href": "content/blog/posts/linux-permissions/index.html",
    "title": "Linux Permissions",
    "section": "",
    "text": "General: _rwxrwxrwx 1 owner group\n_ | rwx | rwx | rwx = Special | Owner | Group | All Users\n\n\n\n4 | r = Read\n2 | w = Write\n1 | x = Execute\n\n\n\n\n\n0 = ---\n1 = --x\n2 = -w-\n3 = -wx\n4 = r-\n5 = r-x\n6 = rw-\n7 = rwx"
  },
  {
    "objectID": "content/blog/posts/linux-permissions/index.html#syntax",
    "href": "content/blog/posts/linux-permissions/index.html#syntax",
    "title": "Linux Permissions",
    "section": "",
    "text": "General: _rwxrwxrwx 1 owner group\n_ | rwx | rwx | rwx = Special | Owner | Group | All Users\n\n\n\n4 | r = Read\n2 | w = Write\n1 | x = Execute\n\n\n\n\n\n0 = ---\n1 = --x\n2 = -w-\n3 = -wx\n4 = r-\n5 = r-x\n6 = rw-\n7 = rwx"
  },
  {
    "objectID": "content/blog/posts/linux-permissions/index.html#commands",
    "href": "content/blog/posts/linux-permissions/index.html#commands",
    "title": "Linux Permissions",
    "section": "Commands",
    "text": "Commands\nchgrp = Change group\nExample: sudo chgrp -R &lt;group&gt; &lt;folder&gt;\nchown = Change ownership\nExample: sudo chown -R &lt;user&gt;:&lt;group&gt; &lt;file/folder&gt;\nchmod = Change permissions\nExample: sudo chmod -R 774 &lt;file/folder&gt;\nMake new files inherit the group: sudo chmod g+s &lt;folder&gt;\n\nExample\nCreate a shared directory for a group.1. Create a shared directory for users to access: /share\n\nAssign users to a common group (staff): sudo usermod -a -G staff &lt;user&gt;\nVerify user groups: groups &lt;user&gt;\nCreate shared directory and assign permissions:\n\nsudo mkdir /share && \\\nsudo chgrp -R staff /share && \\ # assign group\nsudo chmod -R g+w /share && \\ # permissions\nsudo chmod -R +s /share # inherit permissions for newly created files/folders"
  },
  {
    "objectID": "content/blog/posts/quarto-tabsets/index.html",
    "href": "content/blog/posts/quarto-tabsets/index.html",
    "title": "Quarto Tabsets",
    "section": "",
    "text": "PythonJavaScript\n\n\nMarkup:\n```{py}\nprint(\"Hello World\")\n```\nOutput:\nprint(\"Hello World\")\n\n\nMarkup:\n```{js}\nconsole.log(\"Hello World\");\n```\nOutput:\nconsole.log(\"Hello World\");"
  },
  {
    "objectID": "content/development/code/cmd.html",
    "href": "content/development/code/cmd.html",
    "title": "CMD",
    "section": "",
    "text": "@echo off\ncls\n\n@REM Set the path to the script root (using UNC path). Note the string is not quoted.\nset SCRIPT_DIR=\\\\integral-corp.com\\data\\&lt;Project Number Range&gt;\\&lt;Project Folder&gt;\\Working_Files\\DataManagement\\IDB\\export\\script\n\n@REM Name of the script to execute. Note the string is not quoted.\nset SCRIPT_NAME=export.sql\n\n@REM Prompt the user for their Postgres password\n@REM This allows for users to run the execsql script who might not have\n@REM a custom execsql.conf file specified with their username.\nset /p \"DB_USER=Enter you PostgreSQL username: \"\n\n@REM Run the execsql script\nPowerShell -NoProfile -ExecutionPolicy Bypass -Command \"& Set-Location -Path %SCRIPT_DIR%; M:\\DataManagement\\bin\\execsql.py -u %DB_USER% %SCRIPT_NAME%\"\n\npause"
  },
  {
    "objectID": "content/development/code/cmd.html#run-a-python-script",
    "href": "content/development/code/cmd.html#run-a-python-script",
    "title": "CMD",
    "section": "",
    "text": "@echo off\ncls\n\n@REM Set the path to the script root (using UNC path). Note the string is not quoted.\nset SCRIPT_DIR=\\\\integral-corp.com\\data\\&lt;Project Number Range&gt;\\&lt;Project Folder&gt;\\Working_Files\\DataManagement\\IDB\\export\\script\n\n@REM Name of the script to execute. Note the string is not quoted.\nset SCRIPT_NAME=export.sql\n\n@REM Prompt the user for their Postgres password\n@REM This allows for users to run the execsql script who might not have\n@REM a custom execsql.conf file specified with their username.\nset /p \"DB_USER=Enter you PostgreSQL username: \"\n\n@REM Run the execsql script\nPowerShell -NoProfile -ExecutionPolicy Bypass -Command \"& Set-Location -Path %SCRIPT_DIR%; M:\\DataManagement\\bin\\execsql.py -u %DB_USER% %SCRIPT_NAME%\"\n\npause"
  },
  {
    "objectID": "content/development/code/index.html",
    "href": "content/development/code/index.html",
    "title": "Code",
    "section": "",
    "text": "Bourne Again SHell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand Prompt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperText Markup Language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollection of Jupyter Notebooks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular expressions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructured Query Language\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/development/code/index.html#code-pages",
    "href": "content/development/code/index.html#code-pages",
    "title": "Code",
    "section": "",
    "text": "Bourne Again SHell\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCommand Prompt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHyperText Markup Language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCollection of Jupyter Notebooks\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPython programming language\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegular expressions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructured Query Language\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/development/code/python.html",
    "href": "content/development/code/python.html",
    "title": "Python",
    "section": "",
    "text": "import glob\nimport os\n\nfor filename in glob.glob(\"./**/*.ext\", recursive=True):\n    new_name = \"-\".join(filename.split(\"_\"))\n    os.rename(filename, new_name)"
  },
  {
    "objectID": "content/development/code/python.html#batch-file-rename",
    "href": "content/development/code/python.html#batch-file-rename",
    "title": "Python",
    "section": "",
    "text": "import glob\nimport os\n\nfor filename in glob.glob(\"./**/*.ext\", recursive=True):\n    new_name = \"-\".join(filename.split(\"_\"))\n    os.rename(filename, new_name)"
  },
  {
    "objectID": "content/development/code/python.html#logging-basics",
    "href": "content/development/code/python.html#logging-basics",
    "title": "Python",
    "section": "Logging Basics",
    "text": "Logging Basics\nimport argparse\nimport logging\nimport os\nfrom datetime import datetime\n\n# Do not specify __name__ to use root log level\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\n    \"%(asctime)s : %(msecs)04d : %(name)s : %(levelname)s : %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlog_file = (\n    f\"{os.path.splitext(__file__)[0]}_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.log\"\n)\nstream_handler = logging.StreamHandler()\nstream_handler.setFormatter(formatter)\n\n\ndef clparser() -&gt; argparse.ArgumentParser:\n    \"\"\"Create a parser to handle input arguments and displaying a help message.\"\"\"\n    desc_msg = \"\"\"My logging program.\"\"\"\n    parser = argparse.ArgumentParser(description=desc_msg)\n    parser.add_argument(\n        \"-l\",\n        \"--logfile\",\n        action=\"store_true\",\n        help=\"Write log messages to a file.\",\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--verbose\",\n        action=\"store_true\",\n        help=\"Control the amount of information to display.\",\n    )\n    return parser\n\n\nif __name__ == \"__main__\":\n    args = clparser().parse_args()\n    if args.verbose:\n        logger.addHandler(stream_handler)\n    if args.logfile:\n        file_handler = logging.FileHandler(filename=log_file)\n        file_handler.setFormatter(formatter)\n        logger.addHandler(file_handler)\n    logger.info(\"Begin my test module\")"
  },
  {
    "objectID": "content/development/code/python.html#sending-http-requests",
    "href": "content/development/code/python.html#sending-http-requests",
    "title": "Python",
    "section": "Sending HTTP Requests",
    "text": "Sending HTTP Requests\nimport logging\n\nimport requests\n\nlogger = logging.getLogger(__name__)\n\n\ndef send_request(request_type: str, url: str, **kwargs) -&gt; requests.Response:\n    \"\"\"Send an HTTP request.\n\n    Args:\n    ----\n        request_type (str): Accepts \"GET\" or \"POST\"\n        url (str): Request URL\n\n    Returns:\n    -------\n        requests.Response: Request response.\n    \"\"\"\n    valid_methods = (\"GET\", \"POST\")\n    if request_type.upper() not in valid_methods:\n        raise ValueError(f\"Invalid request type. Supported types: {valid_methods}\")\n    try:\n        response = requests.request(request_type.upper(), url, **kwargs)\n        response.raise_for_status()  # Raises an exception if status code &gt;= 400\n        return response\n    except requests.exceptions.RequestException as err:\n        logger.error(f\"{err}. Request type: {request_type}. URL: {url}. Args: {kwargs}\")\n        raise err\n\n# Example GET request\nresponse = send_request(\"GET\", \"https://api.publicapis.org/entries\", timeout=5)\nlogger.info(response.json())\n\n# Example POST request\nresponse = send_request(\"POST\", \"https://someurl.com\", headers={}, timeout=5)\nlogger.info(response.json())"
  },
  {
    "objectID": "content/development/code/python.html#regular-expressions",
    "href": "content/development/code/python.html#regular-expressions",
    "title": "Python",
    "section": "Regular Expressions",
    "text": "Regular Expressions\nSee here for more information on regex syntax.\n\n\nCode\nimport re\n\ntext_string = \"\"\"\nHello world\n\n8001234567\n800-321-7654\n900.987.6543\n\nsome.email@email.com\nmycompany@company.net\nwierd-12-address-4@somedomain.blah\n\"\"\"\n\npattern = re.compile(r\"[0-9]{3}[.-]?[0-9]{3}[.-]?[0-9]{4}\")\nmatches = re.finditer(pattern, text_string)\n\nfor match in matches:\n    print(match)\n    print(match.span())\n    print(text_string[match.start() : match.end()])\n\n\n&lt;re.Match object; span=(14, 24), match='8001234567'&gt;\n(14, 24)\n8001234567\n&lt;re.Match object; span=(25, 37), match='800-321-7654'&gt;\n(25, 37)\n800-321-7654\n&lt;re.Match object; span=(38, 50), match='900.987.6543'&gt;\n(38, 50)\n900.987.6543"
  },
  {
    "objectID": "content/development/code/python.html#oop-basics",
    "href": "content/development/code/python.html#oop-basics",
    "title": "Python",
    "section": "OOP Basics",
    "text": "OOP Basics\n\n\nCode\nclass Employee:\n    \"\"\"Create an employee object with relevant attributes.\"\"\"\n\n    annual_raise_pct = 0.04\n    employee_no = 1\n\n    def __init__(self, first_name, last_name, position, years_employed=1):\n        \"\"\"Function called when new object is initiated.\"\"\"\n        self.first_name = first_name\n        self.last_name = last_name\n        self.position = position\n        self.years_employed = years_employed\n        self.employee_number = Employee.employee_no\n        self.email = self.first_name + \".\" + self.last_name + \"@company.com\"\n        self.salary = self.calculate_salary()\n        Employee.employee_no += 1\n\n    def starting_salary(self):\n        \"\"\"Return the starting salary for each position at company\"\"\"\n        if self.position == \"HR\":\n            return 25000\n        elif self.position == \"Management\":\n            return 50000\n        elif self.position == \"Developer\":\n            return 100000\n        elif self.position == \"CEO\":\n            return 200000\n        else:\n            return None\n\n    def calculate_salary(self):\n        salary = self.starting_salary()\n        for year in range(self.years_employed):\n            salary = int(salary + (salary * Employee.annual_raise_pct))\n        return salary\n\n    def __repr__(self):\n        \"\"\"Create representational object string\"\"\"\n        return f\"\"\"Employee(first_name = {self.first_name}, last_name = {self.last_name}, position = {self.position}, years_employed = {self.years_employed})\"\"\"\n\n\ncurrent_employees = [\n    Employee(\"Geo\", \"Coug\", \"Developer\", 6),\n    Employee(\"Jane\", \"Doe\", \"HR\", 4),\n    Employee(\"John\", \"Doe\", \"Management\", 15),\n    Employee(\"Bob\", \"Loblaw\", \"CEO\", 24),\n]\n\nfor e in current_employees:\n    print(f\"Employee Number: {e.employee_number}\")\n    print(e)\n    print(\"Email:\", e.email)\n    print(\"Salary: ${:,}\\n\".format(e.salary))\n\n\nEmployee Number: 1\nEmployee(first_name = Geo, last_name = Coug, position = Developer, years_employed = 6)\nEmail: Geo.Coug@company.com\nSalary: $126,530\n\nEmployee Number: 2\nEmployee(first_name = Jane, last_name = Doe, position = HR, years_employed = 4)\nEmail: Jane.Doe@company.com\nSalary: $29,245\n\nEmployee Number: 3\nEmployee(first_name = John, last_name = Doe, position = Management, years_employed = 15)\nEmail: John.Doe@company.com\nSalary: $90,039\n\nEmployee Number: 4\nEmployee(first_name = Bob, last_name = Loblaw, position = CEO, years_employed = 24)\nEmail: Bob.Loblaw@company.com\nSalary: $512,645"
  },
  {
    "objectID": "content/development/code/python.html#dbms-data-types",
    "href": "content/development/code/python.html#dbms-data-types",
    "title": "Python",
    "section": "DBMS Data Types",
    "text": "DBMS Data Types\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"../../../static/development/data-types.csv\")\ndf.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nData Type\n\n\n\nPostgres\n\n\n\nMariaDB\n\n\n\nSQL Server\n\n\n\nFirebird\n\n\n\nMS-Access\n\n\n\nSQLite\n\n\n\n\n\n\n\n\n\n\n\nTimestamp with time zone\n\n\n\n1184.0\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nTimestamp\n\n\n\n1184.0\n\n\n\n7\n\n\n\nNaN\n\n\n\ntype ‘datetime.datetime’\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nDatetime\n\n\n\nNaN\n\n\n\n12\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\n\n\n\n\nDate\n\n\n\n1082.0\n\n\n\n10\n\n\n\nclass ‘datetime.date’\n\n\n\ntype ‘datetime.date’\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\n\n\n\n\nTime\n\n\n\n1083.0\n\n\n\n11\n\n\n\nclass ‘datetime.time’\n\n\n\ntype ‘datetime.time’\n\n\n\nclass ‘datetime.datetime’\n\n\n\nNaN\n\n\n\n\n\n\n\nBoolean\n\n\n\n16.0\n\n\n\n16\n\n\n\nclass ‘bool’\n\n\n\nNaN\n\n\n\nclass ‘bool’\n\n\n\nNaN\n\n\n\n\n\n\n\nSmall integer\n\n\n\n21.0\n\n\n\n1\n\n\n\nclass ‘int’\n\n\n\nNaN\n\n\n\nclass ‘int’\n\n\n\nNaN\n\n\n\n\n\n\n\nInteger\n\n\n\n23.0\n\n\n\n2\n\n\n\nclass ‘int’\n\n\n\ntype ‘int’\n\n\n\nclass ‘int’\n\n\n\nNaN\n\n\n\n\n\n\n\nLong integer\n\n\n\n20.0\n\n\n\n3\n\n\n\nclass ‘int’\n\n\n\ntype ‘long’\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nSingle\n\n\n\n701.0\n\n\n\n4\n\n\n\nclass ‘float’\n\n\n\ntype ‘float’\n\n\n\nclass ‘float’\n\n\n\nNaN\n\n\n\n\n\n\n\nDouble precision\n\n\n\n701.0\n\n\n\n5\n\n\n\nclass ‘float’\n\n\n\ntype ‘float’\n\n\n\nclass ‘float’\n\n\n\nNaN\n\n\n\n\n\n\n\nDecimal\n\n\n\n1700.0\n\n\n\n0\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nNaN\n\n\n\nNaN\n\n\n\n\n\n\n\nCurrency\n\n\n\n790.0\n\n\n\nNaN\n\n\n\nNaN\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nclass ‘decimal.Decimal’\n\n\n\nNaN\n\n\n\n\n\n\n\nCharacter\n\n\n\n1042.0\n\n\n\nNaN\n\n\n\nclass ‘str’\n\n\n\ntype ‘str’\n\n\n\nclass ‘str’\n\n\n\nNaN\n\n\n\n\n\n\n\nCharacter varying\n\n\n\n1043.0\n\n\n\n15\n\n\n\nclass ‘str’\n\n\n\ntype ‘str’\n\n\n\nclass ‘str’\n\n\n\nNaN\n\n\n\n\n\n\n\nText\n\n\n\n25.0\n\n\n\nNaN\n\n\n\nclass ‘str’\n\n\n\ntype ‘str’\n\n\n\nclass ‘str’\n\n\n\nNaN\n\n\n\n\n\n\n\nBinary / BLOB\n\n\n\n17.0\n\n\n\n249,250,251,252\n\n\n\nclass ‘bytearray’\n\n\n\ntype ‘str’\n\n\n\ntype ‘bytearray’\n\n\n\nNaN"
  },
  {
    "objectID": "content/development/code/python.html#dbms-libraries",
    "href": "content/development/code/python.html#dbms-libraries",
    "title": "Python",
    "section": "DBMS Libraries",
    "text": "DBMS Libraries\n\nPostgres: psycopg2\nMariaDB: pymysql\nSQL Server: pyodbc\nFirebird: fdb\nMS-Access: pyodbc\nSQLite: sqlite3"
  },
  {
    "objectID": "content/development/code/python.html#db-table-dependencies",
    "href": "content/development/code/python.html#db-table-dependencies",
    "title": "Python",
    "section": "DB Table Dependencies",
    "text": "DB Table Dependencies\ndef dependency_order(dep_list):\n    rem_tables = list(set([t[0] for t in dep_list] + [t[1] for t in dep_list]))\n    rem_dep = copy.copy(dep_list)\n    sortkey = 1\n    ret_list = []\n    while len(rem_dep) &gt; 0:\n        tbls = [tbl for tbl in rem_tables if tbl not in [dep[0] for dep in rem_dep]]\n        ret_list.extend([(tb, sortkey) for tb in tbls])\n        rem_tables = [tbl for tbl in rem_tables if tbl not in tbls]\n        rem_dep = [dep for dep in rem_dep if dep[1] not in tbls]\n        sortkey += 1\n    if len(rem_tables) &gt; 0:\n        ret_list.extend([(tb, sortkey) for tb in rem_tables])\n    ret_list.sort(cmp=lambda x, y: cmp(x[1], y[1]))\n    return [item[0] for item in ret_list]"
  },
  {
    "objectID": "content/development/code/python.html#csv-sniffer",
    "href": "content/development/code/python.html#csv-sniffer",
    "title": "Python",
    "section": "CSV Sniffer",
    "text": "CSV Sniffer\nimport re\n\nclass CsvDiagError(Exception):\n    def __init__(self, msg):\n        self.value = msg\n\n    def __str__(self):\n        return self.value\n\nclass CsvLine:\n    escchar = \"\\\\\"\n\n    def __init__(self, line_text):\n        self.text = line_text\n        self.delim_counts = {}\n        self.item_errors = []  # A list of error messages.\n\n    def __str__(self):\n        return \"; \".join(\n            [\n                \"Text: &lt;&lt;%s&gt;&gt;\" % self.text,\n                \"Delimiter counts: &lt;&lt;%s&gt;&gt;\"\n                % \", \".join(\n                    [\n                        \"%s: %d\" % (k, self.delim_counts[k])\n                        for k in self.delim_counts.keys()\n                    ]\n                ),\n            ]\n        )\n\n    def count_delim(self, delim):\n        # If the delimiter is a space, consider multiple spaces to be equivalent\n        # to a single delimiter, split on the space(s), and consider the delimiter\n        # count to be one fewer than the items returned.\n        if delim == \" \":\n            self.delim_counts[delim] = max(0, len(re.split(r\" +\", self.text)) - 1)\n        else:\n            self.delim_counts[delim] = self.text.count(delim)\n\n    def delim_count(self, delim):\n        return self.delim_counts[delim]\n\n    def _well_quoted(self, element, qchar):\n        # A well-quoted element has either no quotes, a quote on each end and none\n        # in the middle, or quotes on both ends and every internal quote is either\n        # doubled or escaped.\n        # Returns a tuple of three booleans; the first indicates whether the element is\n        # well-quoted, the second indicates whether the quote character is used\n        # at all, and the third indicates whether the escape character is used.\n        if qchar not in element:\n            return (True, False, False)\n        if len(element) == 0:\n            return (True, False, False)\n        if element[0] == qchar and element[-1] == qchar and qchar not in element[1:-1]:\n            return (True, True, False)\n        # The element has quotes; if it doesn't have one on each end, it is not well-quoted.\n        if not (element[0] == qchar and element[-1] == qchar):\n            return (False, True, False)\n        e = element[1:-1]\n        # If there are no quotes left after removing doubled quotes, this is well-quoted.\n        if qchar not in e.replace(qchar + qchar, \"\"):\n            return (True, True, False)\n        # if there are no quotes left after removing escaped quotes, this is well-quoted.\n        if qchar not in e.replace(self.escchar + qchar, \"\"):\n            return (True, True, True)\n        return (False, True, False)\n\n    def record_format_error(self, pos_no, errmsg):\n        self.item_errors.append(\"%s in position %d.\" % (errmsg, pos_no))\n\n    def items(self, delim, qchar):\n        # Parses the line into a list of items, breaking it at delimiters that are not\n        # within quoted stretches.  (This is a almost CSV parser, for valid delim and qchar,\n        # except that it does not eliminate quote characters or reduce escaped quotes.)\n        self.item_errors = []\n        if qchar is None:\n            if delim is None:\n                return self.text\n            else:\n                if delim == \" \":\n                    return re.split(r\" +\", self.text)\n                else:\n                    return self.text.split(delim)\n        elements = []  # The list of items on the line that will be returned.\n        eat_multiple_delims = delim == \" \"\n        # States of the FSM:\n        # _IN_QUOTED: An opening quote has been seen, but no closing quote encountered.\n        #  Actions / transition:\n        #   quote: save char in escape buffer / _ESCAPED\n        #   esc_char : save char in escape buffer / _ESCAPED\n        #   delimiter: save char in element buffer / _IN_QUOTED\n        #   other: save char in element buffer / _IN_QUOTED\n        # _ESCAPED: An escape character has been seen while _IN_QUOTED (and is in the escape buffer).\n        #  Actions / transitions\n        #   quote: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        #   delimiter: save escape buffer in element buffer, empty escape buffer,\n        #    save element buffer, empty element buffer / _BETWEEN\n        #   other: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        # _QUOTE_IN_QUOTED: A quote has been seen while _IN_QUOTED (and is in the escape buffer).\n        #  Actions / transitions\n        #   quote: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        #   delimiter: save escape buffer in element buffer, empty escape buffer,\n        #    save element buffer, empty element buffer / _DELIMITED\n        #   other: save escape buffer in element buffer, empty escape buffer,\n        #    save char in element buffer / _IN_QUOTED\n        #     (An 'other' character in this position represents a bad format:\n        #     a quote not followed by another quote or a delimiter.)\n        # _IN_UNQUOTED: A non-delimiter, non-quote has been seen.\n        #  Actions / transitions\n        #   quote: save char in element buffer / _IN_UNQUOTED\n        #    (This represents a bad format.)\n        #   delimiter: save element buffer, empty element buffer / _DELIMITED\n        #   other: save char in element buffer / _IN_UNQUOTED\n        # _BETWEEN: Not in an element, and a delimiter not seen.  This is the starting state,\n        #   and the state following a closing quote but before a delimiter is seen.\n        #  Actions / transition:\n        #   quote: save char in element buffer / _IN_QUOTED\n        #   delimiter: save element buffer, empty element buffer / _DELIMITED\n        #    (The element buffer should be empty, representing a null data item.)\n        #   other: save char in element buffer / _IN_UNQUOTED\n        # _DELIMITED: A delimiter has been seen while not in a quoted item.\n        #  Actions / transition:\n        #   quote: save char in element buffer / _IN_QUOTED\n        #   delimiter: if eat_multiple: no action / _DELIMITED\n        #     if not eat_multiple: save element buffer, empty element buffer / _DELIMITED\n        #   other: save char in element buffer / _IN_UNQUOTED\n        # At end of line: save escape buffer in element buffer, save element buffer.  For a well-formed\n        # line, these should be empty, but they may not be.\n        #\n        # Define the state constants, which will also be used as indexes into an execution vector.\n        (\n            _IN_QUOTED,\n            _ESCAPED,\n            _QUOTE_IN_QUOTED,\n            _IN_UNQUOTED,\n            _BETWEEN,\n            _DELIMITED,\n        ) = range(6)\n        #\n        # Because of Python 2.7's scoping rules:\n        # * The escape buffer and current element are defined as mutable objects that will have their\n        #  first elements modified, rather than as string variables.  (Python 2.x does not allow\n        #  modification of a variable in an enclosing scope that is not the global scope, but\n        #  mutable objects like lists can be altered.  Another approach would be to implement this\n        #  as a class and use instance variables.)\n        # * The action functions return the next state rather than assigning it directly to the 'state' variable.\n        esc_buf = [\"\"]\n        current_element = [\"\"]\n\n        def in_quoted():\n            if c == self.escchar:\n                esc_buf[0] = c\n                return _ESCAPED\n            elif c == qchar:\n                esc_buf[0] = c\n                return _QUOTE_IN_QUOTED\n            else:\n                current_element[0] += c\n                return _IN_QUOTED\n\n        def escaped():\n            if c == delim:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _BETWEEN\n            else:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                current_element[0] += c\n                return _IN_QUOTED\n\n        def quote_in_quoted():\n            if c == qchar:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                current_element[0] += c\n                return _IN_QUOTED\n            elif c == delim:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += esc_buf[0]\n                esc_buf[0] = \"\"\n                current_element[0] += c\n                self.record_format_error(\n                    i + 1, \"Unexpected character following a closing quote\"\n                )\n                return _IN_QUOTED\n\n        def in_unquoted():\n            if c == delim:\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += c\n                return _IN_UNQUOTED\n\n        def between():\n            if c == qchar:\n                current_element[0] += c\n                return _IN_QUOTED\n            elif c == delim:\n                elements.append(current_element[0])\n                current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += c\n                return _IN_UNQUOTED\n\n        def delimited():\n            if c == qchar:\n                current_element[0] += c\n                return _IN_QUOTED\n            elif c == delim:\n                if not eat_multiple_delims:\n                    elements.append(current_element[0])\n                    current_element[0] = \"\"\n                return _DELIMITED\n            else:\n                current_element[0] += c\n                return _IN_UNQUOTED\n\n        # Functions in the execution vector must be ordered identically to the\n        # indexes represented by the state constants.\n        exec_vector = [\n            in_quoted,\n            escaped,\n            quote_in_quoted,\n            in_unquoted,\n            between,\n            delimited,\n        ]\n        # Set the starting state.\n        state = _BETWEEN\n        # Process the line of text.\n        for i, c in enumerate(self.text):\n            state = exec_vector[state]()\n        # Process the end-of-line condition.\n        if len(esc_buf[0]) &gt; 0:\n            current_element[0] += esc_buf[0]\n        if len(current_element[0]) &gt; 0:\n            elements.append(current_element[0])\n        return elements\n\n    def well_quoted_line(self, delim, qchar):\n        # Returns a tuple of boolean, int, and boolean, indicating: 1) whether the line is\n        # well-quoted, 2) the number of elements for which the quote character is used,\n        # and 3) whether the escape character is used.\n        wq = [self._well_quoted(el, qchar) for el in self.items(delim, qchar)]\n        return (\n            all([b[0] for b in wq]),\n            sum([b[1] for b in wq]),\n            any([b[2] for b in wq]),\n        )\n\ndef diagnose_delim(linestream, possible_delimiters=None, possible_quotechars=None):\n    # Returns a tuple consisting of the delimiter, quote character, and escape\n    # character for quote characters within elements of a line.  All may be None.\n    # If the escape character is not None, it will be u\"\\\".\n    # Arguments:\n    # * linestream: An iterable file-like object with a 'next()' method that returns lines of text\n    #  as bytes or unicode.\n    # * possible_delimiters: A list of single characters that might be used to separate items on\n    #  a line.  If not specified, the default consists of tab, comma, semicolon, and vertical rule.\n    #  If a space character is included, multiple space characters will be treated as a single\n    #  delimiter--so it's best if there are no missing values on space-delimited lines, though\n    #  that is not necessarily a fatal flaw unless there is a very high fraction of missing values.\n    # * possible_quotechars: A list of single characters that might be used to quote items on\n    #  a line.  If not specified, the default consists of single and double quotes.\n    if not possible_delimiters:\n        possible_delimiters = [\"\\t\", \",\", \";\", \"|\"]\n    if not possible_quotechars:\n        possible_quotechars = ['\"', \"'\"]\n    lines = []\n    for i in range(100):\n        try:\n            ln = linestream.next()\n        except StopIteration:\n            break\n        except:\n            raise\n        while len(ln) &gt; 0 and ln[-1] in (\"\\n\", \"\\r\"):\n            ln = ln[:-1]\n        if len(ln) &gt; 0:\n            lines.append(CsvLine(ln))\n    if len(lines) == 0:\n        raise CsvDiagError(\"CSV diagnosis error: no lines read\")\n    for ln in lines:\n        for d in possible_delimiters:\n            ln.count_delim(d)\n    # For each delimiter, find the minimum number of delimiters found on any line, and the number of lines\n    # with that minimum number\n    delim_stats = {}\n    for d in possible_delimiters:\n        dcounts = [ln.delim_count(d) for ln in lines]\n        min_count = min(dcounts)\n        delim_stats[d] = (min_count, dcounts.count(min_count))\n    # Remove delimiters that were never found.\n    for k in delim_stats.keys():\n        if delim_stats[k][0] == 0:\n            del delim_stats[k]\n\n    def all_well_quoted(delim, qchar):\n        # Returns a tuple of boolean, int, and boolean indicating: 1) whether the line is\n        # well-quoted, 2) the total number of lines and elements for which the quote character\n        # is used, and 3) the escape character used.\n        wq = [l.well_quoted_line(delim, qchar) for l in lines]\n        return (\n            all([b[0] for b in wq]),\n            sum([b[1] for b in wq]),\n            CsvLine.escchar if any([b[2] for b in wq]) else None,\n        )\n\n    def eval_quotes(delim):\n        # Returns a tuple of the form to be returned by 'diagnose_delim()'.\n        ok_quotes = {}\n        for q in possible_quotechars:\n            allwq = all_well_quoted(delim, q)\n            if allwq[0]:\n                ok_quotes[q] = (allwq[1], allwq[2])\n        if len(ok_quotes) == 0:\n            return (delim, None, None)  # No quotes, no escapechar\n        else:\n            max_use = max([v[0] for v in ok_quotes.values()])\n            if max_use == 0:\n                return (delim, None, None)\n            # If multiple quote characters have the same usage, return (arbitrarily) the first one.\n            for q in ok_quotes.keys():\n                if ok_quotes[q][0] == max_use:\n                    return (delim, q, ok_quotes[q][1])\n\n    if len(delim_stats) == 0:\n        # None of the delimiters were found.  Some other delimiter may apply,\n        # or the input may contain a single value on each line.\n        # Identify possible quote characters.\n        return eval_quotes(None)\n    else:\n        if len(delim_stats) &gt; 1:\n            # If one of them is a space, prefer the non-space\n            if \" \" in delim_stats.keys():\n                del delim_stats[\" \"]\n        if len(delim_stats) == 1:\n            return eval_quotes(delim_stats.keys()[0])\n        # Assign weights to the delimiters.  The weight is the square of the minimum number of delimiters\n        # on a line times the number of lines with that delimiter.\n        delim_wts = {}\n        for d in delim_stats.keys():\n            delim_wts[d] = delim_stats[d][0] ** 2 * delim_stats[d][1]\n        # Evaluate quote usage for each delimiter, from most heavily weighted to least.\n        # Return the first good pair where the quote character is used.\n        delim_order = sorted(delim_wts, key=delim_wts.get, reverse=True)\n        for d in delim_order:\n            quote_check = eval_quotes(d)\n            if quote_check[0] and quote_check[1]:\n                return quote_check\n        # There are no delimiters for which quotes are OK.\n        return (delim_order[0], None, None)\n    # Should never get here\n    raise CsvDiagError(\"CSV diagnosis error: an untested set of conditions are present\")"
  },
  {
    "objectID": "content/development/code/python.html#import-csv-to-a-database",
    "href": "content/development/code/python.html#import-csv-to-a-database",
    "title": "Python",
    "section": "Import CSV to a Database",
    "text": "Import CSV to a Database\nclass CsvFile(object):\n    \"\"\"CsvFile class automatically opens a file and creates a CSV reader, reads the first row containing column headers, and stores those headers so that they can be used to construct the INSERT statement.\"\"\"\n    def __init__(self, filename):\n        self.fn = filename\n        self.f = None\n        self.open()\n        self.rdr = csv.reader(self.f)\n        self.headers = next(self.rdr)\n    def open(self):\n        if self.f is None:\n            mode = \"rb\" if sys.version_info &lt; (3,) else \"r\"\n            self.f = open(self.fn, mode)\n    def reader(self):\n        return self.rdr\n    def close(self):\n        self.rdr = None\n        self.f.close()\n        self.f = None\n\n\nclass Database(object):\n    \"\"\"The Database class and subclasses provide a database connection for each type of DBMS, and a method to construct an INSERT statement for a given CsvFile object, using that DBMS's parameter substitution string.  The conn_info argument is a dictionary containing the host name, user name, and password.\"\"\"\n    def __init__(self, conn_info):\n        self.paramstr = '%s'\n        self.conn = None\n    def insert_sql(self, tablename, csvfile):\n        return \"insert into %s (%s) values (%s);\" % (\n                tablename,\n                \",\".join(csvfile.headers),\n                \",\".join([self.paramstr] * len(csvfile.headers))\n                )\n\n\nclass PgDb(Database):\n    def __init__(self, conn_info):\n        self.db_type = \"p\"\n        import psycopg2\n        self.paramstr = \"%s\"\n        connstr = \"host=%(server)s dbname=%(db)s user=%(user)s password=%(pw)s\" % conn_info\n        self.conn = psycopg2.connect(connstr)\n\n    def postgres_copy(csvfile, db):\n        \"\"\"Postgres COPY command. Fastest implementation\"\"\"\n        curs = db.conn.cursor()\n        rf = open(csvfile.fn, \"rt\")\n        # Read and discard headers\n        hdrs = rf.readline()\n        copy_cmd = \"copy copy_test from stdin with (format csv)\"\n        curs.copy_expert(copy_cmd, rf)\n\n    def simple_copy(csvfile, db):\n        \"\"\"Row-by-row reading and writing\"\"\"\n        ins_sql = db.insert_sql(\"copy_test\", csvfile)\n        curs = db.conn.cursor()\n        rdr = csvfile.reader()\n        for line in rdr:\n            curs.execute(ins_sql, clean_line(line))\n        db.conn.commit()\n\n    def buffer1_copy(csvfile, db, buflines):\n        \"\"\"Buffered reading and writing\"\"\"\n        ins_sql = db.insert_sql(\"copy_test\", csvfile)\n        curs = db.conn.cursor()\n        rdr = csvfile.reader()\n        eof = False\n        while True:\n            b = []\n            for j in range(buflines):\n                try:\n                    line = next(rdr)\n                except StopIteration:\n                    eof = True\n                else:\n                    b.append(clean_line(line))\n            if len(b) &gt; 0:\n                curs.executemany(ins_sql, b)\n            if eof:\n                break\n        db.conn.commit()"
  },
  {
    "objectID": "content/development/code/sql.html",
    "href": "content/development/code/sql.html",
    "title": "SQL",
    "section": "",
    "text": "The code snippets on this page are primarily geared towards the PostgreSQL dialect."
  },
  {
    "objectID": "content/development/code/sql.html#fundamentals",
    "href": "content/development/code/sql.html#fundamentals",
    "title": "SQL",
    "section": "Fundamentals",
    "text": "Fundamentals\n\nPrimary Key Relationships\nColumns\n\ntable_schema: PK schema name\ntable_name: PK table name\nconstraint_name: PK constraint name\nposition: index of column in table (1, 2, …). 2 or higher means key is composite (contains more than one column)\nkey_column: PK column name\n\nRows\n\nOne row represents one primary key column\nScope of rows: columns of all PK constraints in a database\nOrdered by table schema, table name, column position\n\nselect * from (\n -- Main query. Returns all tables\n select kcu.table_schema,\n     kcu.table_name,\n     tco.constraint_name,\n     kcu.ordinal_position as position,\n     kcu.column_name as key_column\n from information_schema.table_constraints tco\n join information_schema.key_column_usage kcu \n   on kcu.constraint_name = tco.constraint_name\n   and kcu.constraint_schema = tco.constraint_schema\n   and kcu.constraint_name = tco.constraint_name\n where tco.constraint_type = 'PRIMARY KEY'\n order by kcu.table_schema,\n    kcu.table_name,\n    position\n) main\nwhere table_name = 'd_location'\n\n\nForeign Key Relationships\nColumns\n\nforeign_table: foreign table schema and name\nrel: relationship symbol implicating direction\nprimary_table: primary (rerefenced) table schema and name\nfk_columns: list of FK colum names, separated with “,”\nconstraint_name: foreign key constraint name\n\nRows\n\nOne row represents one foreign key.\nIf foreign key consists of multiple columns (composite key) it is still represented as one row.\nScope of rows: all foregin keys in a database.\nOrdered by foreign table schema name and table name.\n\n\n\nOrdering Tables by FK\ndrop table if exists dependencies cascade;\ncreate temporary table dependencies as\nselect \n  tc.table_name as child,\n  tu.table_name as parent\nfrom \n  information_schema.table_constraints as tc\n  inner join information_schema.constraint_table_usage as tu\n    on tu.constraint_name = tc.constraint_name\nwhere \n  tc.constraint_type = 'FOREIGN KEY'\n  and tc.table_name &lt;&gt; tu.table_name;\n\nwith recursive dep_depth as (\nselect\n  dep.child, dep.parent, 1 as lvl\nfrom\n  dependencies as dep\nunion all\nselect\n  dep.child, dep.parent, dd.lvl + 1 as lvl\nfrom\n  dep_depth as dd\n  inner join dependencies as dep on dep.parent = dd.child\n)\nselect\n  table_name, table_order\nfrom (\n  select\n    dd.parent as table_name, max(lvl) as table_order\n  from\n    dep_depth as dd\n  group by\n    table_name\n  union\n  select\n    dd.child as table_name, max(lvl) + 1 as level\n  from\n    dep_depth as dd\n    left join dependencies as dp on dp.parent = dd.child\n  where\n    dp.parent is null\n  group by dd.child\n) as all_levels\norder by table_order;\n\n\nReturn FK relationships for all tables\nselect * from (\n -- Main query. Returns FK relationships for all tables\n select \n   kcu.table_schema as table_schema,\n   kcu.table_name as foreign_table,\n     '&gt;-' as relationship,\n     rel_tco.table_name as primary_table,\n     string_agg(kcu.column_name, ', ') as fk_columns,\n     kcu.constraint_name\n from information_schema.table_constraints tco\n join information_schema.key_column_usage kcu\n     on tco.constraint_schema = kcu.constraint_schema\n     and tco.constraint_name = kcu.constraint_name\n join information_schema.referential_constraints rco\n     on tco.constraint_schema = rco.constraint_schema\n     and tco.constraint_name = rco.constraint_name\n join information_schema.table_constraints rel_tco\n     on rco.unique_constraint_schema = rel_tco.constraint_schema\n     and rco.unique_constraint_name = rel_tco.constraint_name\n where tco.constraint_type = 'FOREIGN KEY'\n group by kcu.table_schema,\n    kcu.table_name,\n    rel_tco.table_name,\n    rel_tco.table_schema,\n    kcu.constraint_name\n order by kcu.table_schema,\n    kcu.table_name\n) main\nwhere primary_table = 'd_location'\n\n\nMost Table Relationships\nList tables with most relationships.\nselect * from\n(select relations.table_name as table_name, -- schema name and table name\n       count(relations.table_name) as relationships, -- number of table relationships\n       count(relations.referenced_tables) as foreign_keys, -- number of foreign keys in a table\n       count(relations.referencing_tables) as references, -- number of foreign keys that are refering to this table\n       count(distinct related_table) as related_tables, -- number of related tables\n       count(distinct relations.referenced_tables) as referenced_tables, -- number of different tables referenced with FKs (multiple FKs can refer to one table, so number of FKs might be different than number of referenced tables)\n       count(distinct relations.referencing_tables) as referencing_tables -- number of different tables that are refering to this table (similar to referenced_tables)\nfrom(\n     select pk_tco.table_schema || '.' || pk_tco.table_name as table_name,\n            fk_tco.table_schema || '.' || fk_tco.table_name as related_table,\n            fk_tco.table_name as referencing_tables,\n            null::varchar(100) as referenced_tables\n     from information_schema.referential_constraints rco\n     join information_schema.table_constraints fk_tco\n          on rco.constraint_name = fk_tco.constraint_name\n          and rco.constraint_schema = fk_tco.table_schema\n     join information_schema.table_constraints pk_tco\n          on rco.unique_constraint_name = pk_tco.constraint_name\n          and rco.unique_constraint_schema = pk_tco.table_schema\n    union all\n    select fk_tco.table_schema || '.' || fk_tco.table_name as table_name,\n           pk_tco.table_schema || '.' || pk_tco.table_name as related_table,\n           null as referencing_tables,\n           pk_tco.table_name as referenced_tables\n    from information_schema.referential_constraints rco\n    join information_schema.table_constraints fk_tco \n         on rco.constraint_name = fk_tco.constraint_name\n         and rco.constraint_schema = fk_tco.table_schema\n    join information_schema.table_constraints pk_tco\n         on rco.unique_constraint_name = pk_tco.constraint_name\n         and rco.unique_constraint_schema = pk_tco.table_schema\n) relations\ngroup by table_name\norder by relationships asc) results\n\nwhere substring(table_name, 5, 2) = 'd_'; -- substring(string, start_position, length)\n\n\nList column definitions and order between a set of tables\nwith recursive\nmain_tables (\n    table_schema,\n    table_name\n) as (\nvalues\n    ('idb', 'd_document'),\n    ('idb', 'd_docfile'),\n    ('idb', 'd_study'),\n    ('idb', 'd_location'),\n    ('idb', 'd_studylocation'),\n    ('idb', 'd_sampcoll'),\n    ('idb', 'd_sampmain'),\n    ('idb', 'd_sampsplit'),\n    ('idb', 'd_labsample'),\n    ('idb', 'd_labpkg'),\n    ('idb', 'd_labresult')\n),\nexclude_columns (column_name) as (\n    values\n    ('gid'),\n    ('studyloc_alias'),\n    ('sampcoll_alias'),\n    ('sample_alias'),\n    ('analresult_alias'),\n    ('rev_user'),\n    ('rev_time')\n),\ndependencies as (\n    select\n        tc.table_name as child,\n        tu.table_name as parent\n    from\n        information_schema.table_constraints as tc\n        inner join information_schema.constraint_table_usage as tu\n            on tu.constraint_name = tc.constraint_name\n    where\n        tc.constraint_type = 'FOREIGN KEY'\n        and tc.table_name &lt;&gt; tu.table_name\n),\ndep_depth as (\n    select\n        dep.child, dep.parent, 1 as lvl\n    from\n        dependencies as dep\n    union all\n    select\n        dep.child, dep.parent, dd.lvl + 1 as lvl\n    from\n        dep_depth as dd\n        inner join dependencies as dep on dep.parent = dd.child\n    ),\ntable_order as (\n    select\n        all_levels.table_name, all_levels.table_order\n    from (\n        select\n            dd.parent as table_name, max(lvl) as table_order\n        from\n            dep_depth as dd\n        group by\n            table_name\n        union\n        select\n            dd.child as table_name, max(lvl) + 1 as level\n        from\n            dep_depth as dd\n            left join dependencies as dp on dp.parent = dd.child\n        where\n            dp.parent is null\n        group by dd.child\n    ) as all_levels\n    order by table_order\n),\ncols as (\n    select\n        cc.table_name,\n        -- Some column names need changing because they exist in multiple tables\n        case\n            when cc.column_name in ('comments', 'description', 'fraction')\n            then replace(cc.table_name, 'd_', '') || '_' || cc.column_name\n            else cc.column_name end as column_name,\n        data_type,\n        character_maximum_length,\n        is_nullable,\n        table_order,\n        ordinal_position,\n        row_number() over () as col_order\n    from\n        information_schema.columns as cc\n        inner join table_order on table_order.table_name=cc.table_name\n        inner join main_tables on \n            main_tables.table_schema=cc.table_schema \n            and main_tables.table_name=cc.table_name\n        left join exclude_columns on cc.column_name=exclude_columns.column_name\n    where exclude_columns.column_name is null\n    order by table_order.table_order, table_order.table_name, cc.ordinal_position\n)\nselect distinct\n    cols.column_name,\n    cols.column_name\n        || ' '\n        || data_type\n        || case\n                when character_maximum_length is null\n                then ''\n                else '(' || character_maximum_length || ')'\n                end\n     -- || case when not is_nullable::boolean then ' NOT NULL' else '' end as col_def,\n        as col_def,\n    cols.table_order, cols.table_name, cols.ordinal_position, row_number() over () as col_order\nfrom cols\norder by cols.table_order, cols.table_name, cols.ordinal_position\n\n\nCommon Functions\n\nLENGTH(string): Returns the length of the provided string\nPOSITION(string IN substring): Returns the position of the substring within the specified string.\nCAST(expression AS datatype): Converts an expression into the specified data type.\n`NOW: Returns the current date, including time.\nCEIL(input_val): Returns the smallest integer greater than the provided number.\nFLOOR(input_val): Returns the largest integer less than the provided number.\nROUND(input_val, [round_to]): Rounds a number to a specified number of decimal places.\nTRUNC(input_value, num_decimals): Truncates a number to a number of decimals.\nREPLACE(whole_string, string_to_replace, replacement_string): Replaces one string inside the whole string with another string.\nSUBSTRING(string, [start_pos], [length]): Returns part of a value, based on a position and length.\n\n\n\nAdd Role\n1create user &lt;username&gt; with password 'password';\ngrant connect on database &lt;db_name&gt; to &lt;username&gt;;\ngrant usage on schema &lt;schema_name&gt; to &lt;username&gt;;\ngrant select on all tables in schema &lt;schema_name&gt; to &lt;username&gt;;\nalter default privileges in schema &lt;schema_name&gt; grant select on tables to &lt;username&gt;;\n\n2grant create on database &lt;db_name&gt; to &lt;username&gt;;\n3grant insert on database &lt;db_name&gt; to &lt;username&gt;;\n4grant update on database &lt;db_name&gt; to &lt;username&gt;;\n5grant update on database &lt;db_name&gt; to &lt;username&gt;;\n\n1\n\nCreate a read-only user.\n\n2\n\nAllow user to create database objects.\n\n3\n\nAllow user to insert rows to any schema and table.\n\n4\n\nAllow user to update rows in any schema and table.\n\n5\n\nAllow user to delete rows in any schema and table.\n\n\n\n\nCreate read only user - shorthand\nOnce already creating a specific user role, you can user the `pg_read_all_data` to grant read only access to all tables.\nGRANT pg_read_all_data TO username;\n\n\nFinding Temporary Objects\nSELECT\n n.nspname as SchemaName,\n c.relname as RelationName,\n CASE c.relkind\n  WHEN 'r' THEN 'table'\n  WHEN 'v' THEN 'view'\n  WHEN 'i' THEN 'index'\n  WHEN 'S' THEN 'sequence'\n  WHEN 's' THEN 'special'\n  END as RelationType,\n pg_catalog.pg_get_userbyid(c.relowner) as RelationOwner,             \n pg_size_pretty(pg_relation_size(n.nspname ||'.'|| c.relname)) as RelationSize\nFROM pg_catalog.pg_class c\nLEFT JOIN pg_catalog.pg_namespace AS n ON n.oid = c.relnamespace\n WHERE  c.relkind IN ('r','s') \n AND  (n.nspname !~ '^pg_toast' and nspname like 'pg_temp%')\nORDER BY pg_relation_size(n.nspname ||'.'|| c.relname) DESC;"
  },
  {
    "objectID": "content/development/code/sql.html#sql",
    "href": "content/development/code/sql.html#sql",
    "title": "SQL",
    "section": "SQL",
    "text": "SQL\n\nTerminate backends of a particular user (“test”)\nThis query will kill every backend user “test” is connected to.\nWITH pids AS (\n  SELECT pid\n  FROM pg_stat_activity\n  WHERE usename='test'\n)\n\nSELECT pg_terminate_backend(pid)\nFROM pids;\n\n\nCancel every running SQL commands from a particular user (“test”)\nThis query will cancel every running query issued by the particular user “test”.\nWITH pids AS ( \n  SELECT pid \n  FROM pg_stat_activity \n  WHERE username='test' \n) \nSELECT pg_cancel_backend(pid) \nFROM pids;\n\n\nList tables in database\nSELECT table_schema, table_name \nFROM information_schema.tables \nORDER BY table_schema,table_name;\n\n\nList columns in table\nSELECT column_name\nFROM   information_schema.columns\nWHERE  table_schema = 'schema'\nAND    table_name = 'table';\n\n\nCreate a read-only user\ngrant connect on database db_name to user;\ngrant usage on schema schema_name to user;\ngrant select on all tables in schema schema_name to user;\nalter default privileges in schema schema_name grant select on tables to user;\n\n\nCreate DB\ncreate database &lt;new_db_name&gt; owner &lt;user_or_group&gt; template &lt;name_of_db_to_use_as_template&gt;;\n-- show search_path;\nset search_path to &lt;default_schema&gt;,public;\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\n-- Database Creation\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\ncreate database &lt;new_db_name&gt; owner &lt;user_or_group&gt; template &lt;name_of_db_to_use_as_template&gt;;\n-- show search_path;\nset search_path to idb, public;\n\ngrant connect, temporary on database &lt;new_db_name&gt; to public;\ngrant all on database &lt;new_db_name&gt; to &lt;user&gt;;\ngrant all on database &lt;new_db_name&gt; to &lt;group&gt;;\n\ncreate extension if not exists postgis;\ncreate extension if not exists dblink;\n\ncreate schema staging;\n\n-- Add a unique constraint to e_analyte.full_name and e_analyte.cas_rn so that\n-- no full name or cas_rn can be used for more than one analyte\n-- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nalter table e_analyte\n  add constraint uc_fullname unique(full_name),\n  add constraint uc_casrn unique(cas_rn);\n\n\nDBLink\nselect\n  a.*, b.*\nfrom\n  table1 as a\n  left join (\n    select * from dblink(\n      'dbname=&lt;database&gt;',\n      'select col1, col2, col3 from &lt;table&gt;'\n    ) as d (\n      col1 text, col2 text, col3 text\n    )\n  ) as b\n  on a.col1 = b.col2\n\n\nPartitioning\nselect * from (\n    select *, row_number() over(\n        partition by\n            col1, col2, col3\n        order by col1 desc\n    ) rowid\n    from sometable\n) someid\nwhere rowid &gt; 1;\n\n\nCurrent Database\nselect * from pg_catalog.current_database()\n\n\nCurrent user/role\nselect * from current_role\nselect * from current_user\n\n\nProcess ID\nselect * from pg_catalog.pg_backend_pid()\n\n\nList functions/defs/args\nselect \n pg_get_userbyid(p.proowner) as owner,\n n.nspname as function_schema,\n p.proname as function_name,\n l.lanname as function_language,\n case when l.lanname = 'internal' then p.prosrc\n  else pg_get_functiondef(p.oid)\n  end as definition,\n pg_get_function_arguments(p.oid) as function_arguments,\n t.typname as return_type\nfrom pg_proc p\n left join pg_namespace n on p.pronamespace = n.oid\n left join pg_language l on p.prolang = l.oid\n left join pg_type t on t.oid = p.prorettype \nwhere n.nspname not in ('pg_catalog', 'information_schema')\nand n.nspname = 'idb'\norder by function_schema, function_name;\n\n\nWhos logged in\nselect * from pg_stat_activity\nwhere usename != '' and usename != 'postgres'\norder by usename, pid\n\n\nAggregate Functions\npg_aggregate catalog\n-- pg_proc contains data for aggregate functions as well as plain functions\nselect * from pg_proc\n-- pg_aggregate is an extension of pg_proc.\nselect * from pg_aggregate\n\n\nList users\nSELECT rolname FROM pg_roles;\n\n\nUpdate From\nUPDATE tablename\nSET columnname = someothervalue\nFROM ...\nWHERE ...\n\n\nMaterialized View\nReference\nCREATE MATERIALIZED VIEW view_name\nAS\nquery\nWITH [NO] DATA;\nWhen you refresh data for a materialized view, PostgreSQL locks the entire table therefore you cannot query data against it. To avoid this, you can use the CONCURRENTLY option.\nWith CONCURRENTLY option, PostgreSQL creates a temporary updated version of the materialized view, compares two versions, and performs INSERT and UPDATE only the differences.\nREFRESH MATERIALIZED VIEW CONCURRENTLY view_name;\n\n\nConstants\nWITH myconstants (analyte_search) as (\n   values ('%Hexachlorocyclopentadiene%')\n)\n\nSELECT *\nFROM e_analyte, myconstants\nWHERE analyte ilike analyte_search\n   OR full_name ilike analyte_search\n   OR aliases ilike analyte_search;\n\n\nSequential Keys\nseq_key bigint NOT NULL DEFAULT nextval('seq_key'::regclass)\n\nALTER SEQUENCE seq_key RESTART WITH 3;\n\n\nCross-Database Search\nThese could be refined further by creating a function.\nwith\nconst (param) as (\n    values ('%solid%')\n),\ndbrows as (\n   select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte\n  union\n    select * from dblink(\n        'dbname=database1',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n        'dbname=database2',\n        'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n        ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n  union\n    select * from dblink(\n     'dbname=database3',\n     'select analyte, full_name, chem_class, aliases, cas_rn, current_database() as db from e_analyte'\n ) as d (analyte text, full_name text, chem_class text, aliases text, cas_rn text, db text)\n)\nselect \n analyte, full_name, chem_class, cas_rn, aliases, \n count(*) as num_instances, string_agg(db, '; ') as db\nfrom dbrows, const\nwhere analyte ilike param\n   or full_name ilike param\n   or aliases ilike param\ngroup by analyte, full_name, chem_class, cas_rn, aliases\norder by chem_class, analyte;\n\n\nLogging\n\nLog slow queries by setting log_min_duration_statement\nALTER database postgres SET log_min_duration_statement = '250ms';\n\n\nControl which statement types get logged\nControl the types of statements that are logged for your database.\nALTER DATABASE postgres SET log_statement = 'all';\nValid values include all, ddl, none, mod\n\n\nLog when waiting on a lock\nLog when database is waiting on a lock.\nALTER DATABASE postgres SET log_lock_waits = 'on';\n\n\n\nPerformance\n\nUse statement timeouts to control runaway queries\nSetting a statement timeout prevents queries from running longer than the specified time. You can set a statement timeout on the database, user, or session level. We recommend you set a global timeout on Postgres and then override that one specific users or sessions that need a longer allowed time to run.\nALTER DATABASE mydatabase SET statement_timeout = '60s';\n\n\nUse pg_stat_statements to find the queries and processes that use the most resources\nSELECT\n total_exec_time,\n mean_exec_time as avg_ms,\n calls,\n query\nFROM pg_stat_statements\nORDER BY mean_exec_time DESC\nLIMIT 10;\n\n\nMonitor connections in Postgres\nThis query will provide the number of connection based on type.\nSELECT count(*),\n    state\nFROM pg_stat_activity\nGROUP BY state;\nIf you see idle connections is above 20, it is recommended to explore using PgBouncer.\n\n\nQuery size of specific table\nWill give you the size of the specific relation you pass in.\nSELECT pg_relation_size('table_name');\n\n-- For prettier formatting you can wrap with:\n\nSELECT pg_size_pretty(pg_relation_size('table_name'));\n\n\nQuery all relation sizes\nWill report on all table sizes in descending order\nSELECT relname AS relation,\n       pg_size_pretty (\n         pg_total_relation_size (C .oid)\n       ) AS total_size\nFROM pg_class C\nLEFT JOIN pg_namespace N ON (N.oid = C .relnamespace)\nWHERE nspname NOT IN (\n        'pg_catalog',\n        'information_schema'\n      )\n  AND C .relkind &lt;&gt; 'i'\n  AND nspname !~ '^pg_toast'\n  ORDER BY pg_total_relation_size (C .oid) DESC\n\n\nCheck for unused indexes\nWill return the unused indexes in descending order of size. Keep in mind you want to also check replicas before dropping indexes.\nSELECT schemaname || '.' || relname AS table,\n       indexrelname AS index,\n       pg_size_pretty(pg_relation_size(i.indexrelid)) AS \"index size\",\n       idx_scan as \"index scans\"\nFROM pg_stat_user_indexes ui\nJOIN pg_index i ON ui.indexrelid = i.indexrelid\nWHERE NOT indisunique\n  AND idx_scan &lt; 50\n  AND pg_relation_size(relid) &gt; 5 * 8192\nORDER BY \n  pg_relation_size(i.indexrelid) / nullif(idx_scan, 0) DESC NULLS FIRST,\n  pg_relation_size(i.indexrelid) DESC;\n\n\nGet approximate counts for a table\nWill return the approximate count for a table based on PostgreSQL internal statistics. Useful for large tables where performing a `SELECT count(*)` is costly on performance.\nSELECT reltuples::numeric as count\nFROM pg_class\nWHERE relname='table_name';\n\n\nNon-blocking index creation\nAdding `CONCURRENTLY` during index creation, while not permitted in a transaction, will not hold a lock on the table while creating your index.\nCREATE INDEX CONCURRENTLY foobar ON foo (bar);\n\n\n\nReplace nulls with other value\nCoalesce will use the value and if the value is null display your specified string.\nSELECT id, \n       coalesce(ip, 'no IP') \nFROM logs;\nYou can supply two columns as well prior to your replacement value and the function will use first not null value.\n\n\nImport Schema with Mapping a Foreign Data Wrapper (FDW)\nImport foreign schema creates foreign tables representing those from the foreign server.\nIMPORT FOREIGN SCHEMA \"public\";\nYou can IMPORT FOREIGN SCHEMA when mapping a foreign data wrapper to save you from building a new one\n\n\nGenerate data with generate_series\nGenerates values from the start to the end values supplied based on the interval. Values can be numbers or timestamps. Can be used in a FROM or JOIN clause or CTE. Commonly used when building charts and reports that require all dates to be filled.\nSELECT * FROM\ngenerate_series(now() - '3 month'::interval, now(), '1 day');\n\n\nRound dates with date_trunc\nWill truncate the date to the specified level of precision. Some example precision levels include: month, week, day, hour, minute.\nSELECT date_trunc('day', now());\n\n\nPerform time math with intervals\nYou can add or subtract specific amounts of time of a timestamp by casting the value you want as an interval.\nSELECT now() - '1 month'::interval;\n\n\nMake your session rest a bit\nThis function will make your session sleep for 2.5 seconds. Useful in any testing tool executing a script in a given loop where you want to pause a bit between iterations, as an example.\nselect pg_sleep(2.5);"
  },
  {
    "objectID": "content/development/code/sql.html#plpgsql",
    "href": "content/development/code/sql.html#plpgsql",
    "title": "SQL",
    "section": "PL/pgSQL",
    "text": "PL/pgSQL\n\nWipe Schema Tables\n-- DROP FUNCTION IF EXISTS idb.wipe_staging();\nCREATE OR REPLACE FUNCTION idb.wipe_staging()\nRETURNS TABLE(staging_schema text, deleted_tables integer) \nLANGUAGE 'plpgsql'\nCOST 100\nVOLATILE PARALLEL UNSAFE\nROWS 1000\nAS $BODY$\n#variable_conflict use_column\nDECLARE\n  staging_schema TEXT;\n  table_name TEXT;\n  deleted_tables INTEGER := 0;\nBEGIN\nstaging_schema = (select 'stg_' || user)::text;\nFOR table_name IN (\n    SELECT table_name \n    FROM information_schema.tables\n    WHERE table_schema = staging_schema\n)\nLOOP\nEXECUTE format('DROP TABLE %I.%I CASCADE', staging_schema, table_name );\ndeleted_tables := deleted_tables + 1;\nEND LOOP;\nRETURN query select staging_schema, deleted_tables;\nEND;\n$BODY$;\nALTER FUNCTION idb.wipe_staging()\n    OWNER TO envdb_dm;"
  },
  {
    "objectID": "content/development/code/sql.html#psql",
    "href": "content/development/code/sql.html#psql",
    "title": "SQL",
    "section": "psql",
    "text": "psql\nCheat sheet\n\n\nCode\nimport pandas as pd\n\nf = \"../../../static/development/commands.xlsx\"\n\nt4 = pd.read_excel(f, sheet_name=\"psql-commands\")\nt4.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nCommand\n\n\n\nDetail\n\n\n\n\n\n\n\n\n\n\n\nsudo -s postgres psql\n\n\n\nConnect to PostgreSQL as admin\n\n\n\n\n\n\n\npostgres=# \\l\n\n\n\nList all databases\n\n\n\n\n\n\n\npostgres=# \\c postgres\n\n\n\nConnect to the database named postgres\n\n\n\n\n\n\n\npostgres=# \\q\n\n\n\nDisconnect\n\n\n\n\n\n\n\npostgres=# -d mydb\n\n\n\nConnecting to database\n\n\n\n\n\n\n\npostgres=# -U john mydb\n\n\n\nConnecting as a specific user\n\n\n\n\n\n\n\npostgres=# -h localhost -p 5432 mydb\n\n\n\nConnecting to a host/port\n\n\n\n\n\n\n\npostgres=# -U admin -h 192.168.1.5 -p 2506 -d mydb\n\n\n\nConnect remote PostgreSQL\n\n\n\n\n\n\n\npostgres=# -W mydb\n\n\n\nForce password\n\n\n\n\n\n\n\npostgres=# -c '\\c postgres' -c '\\dt'\n\n\n\nExecute a SQL query or command\n\n\n\n\n\n\n\npostgres=# -c “\\l+” -H postgres &gt; database.html\n\n\n\nGenerate HTML report\n\n\n\n\n\n\n\npostgres=# -l\n\n\n\nList all databases\n\n\n\n\n\n\n\npostgres=# \\dt\n\n\n\nShow all tables in a database\n\n\n\n\n\n\n\npostgres=# mydb -f file.sql\n\n\n\nExecute commands from a file\n\n\n\n\n\n\n\npostgres=# -V\n\n\n\nPrint the psql version\n\n\n\n\n\n\n\npostgres=# \\df &lt;schema&gt;\n\n\n\nList functions in schema\n\n\n\n\n\n\n\npostgres=# \\du\n\n\n\nShow current user permission\n\n\n\n\n\n\n\npostgres=# \\d &lt;table&gt;\n\n\n\nDescribe table\n\n\n\n\n\n\n\npostgres=# \\d+ &lt;table&gt;\n\n\n\nDescribe table with details\n\n\n\n\n\n\n\npostgres=# \\di\n\n\n\nList indexes\n\n\n\n\n\n\n\npostgres=# \\du\n\n\n\nList roles\n\n\n\n\n\n\n\npostgres=# \\ds\n\n\n\nList sequences\n\n\n\n\n\n\n\npostgres=# \\copy …\n\n\n\nImport/export table\n\n\n\n\n\n\n\npostgres=# \\echo [string]\n\n\n\nPrint string\n\n\n\n\n\n\n\npostgres=# \\i [file]\n\n\n\nExecute file\n\n\n\n\n\n\n\npostgres=# \\o [file]\n\n\n\nExport all results to file\n\n\n\n\n\n\n\n\n\n\n\nExport table to CSV\n\n\\copy table TO '&lt;path&gt;' CSV\n\\copy table(col1,col1) TO '&lt;path&gt;' CSV\n\\copy (SELECT...) TO '&lt;path&gt;' CSV\n\n\n\nBackup\nUse pg_dumpall to backup all databases\npg_dumpall -U postgres &gt; all.sql\nUse pg_dump to backup a database\npg_dump -d mydb -f mydb_backup.sql\n\n-a   Dump only the data, not the schema\n-s   Dump only the schema, no data\n-c   Drop database before recreating\n-C   Create database before restoring\n-t   Dump the named table(s) only\n-F   Format (c: custom, d: directory, t: tar)\n\nUse pg_dump -? to get the full list of options\n\n\nRestore\npsql -U user mydb &lt; mydb_backup.sql\n\npg_restore\npg_restore -d mydb mydb_backup.sql -c\n\n-U   Specify a database user\n-c   Drop database before recreating\n-C   Create database before restoring\n-e   Exit if an error has encountered\n-F   Format (c: custom, d: directory, t: tar, p: plain text sql(default))\n\nUse pg_restore -? to get the full list of options\n\n\n\nAutomatically log query time in psql\nWill automatically print the time it took to run a query from within psql. *Of note this is the round trip time not simply query execution time.*\n\\timing\nYou can save this in your .psqlrc to be a default setting\n\n\nAutoformat query results in psql\nWill automatically reorganize the query output based on your terminal window for better readability.\n\\x auto\nYou can save this in your .psqlrc to be a default setting\n\n\nEdit your psql queries in editor of your choice\nWill automatically open your last run query in your default $EDITOR. When you save and close it will execute that query.\n\\e\n\n\nSet a value for nulls\nWill render the nulls as whatever character you specify. Handy for easier parsing of nulls vs. blank text.\n\\pset null 👻\nYou can save this in your .psqlrc to be a default setting\n\n\nSave your query history per database locally\nWill automatically save a history file for each **DBNAME**.\n\\set HISTFILE ~/.psql_history- :DBNAME\nYou can save this in your .psqlrc to be a default setting\n\n\nShow queries issued by internal psql commands\nAdd “-E” (or –echo-hidden) option to psql in the command line. This option will display queries that internal psql commands generate (like “\\dt mytable”). This is a cool way to learn more about system catalogs, or reuse queries issued by psql in your own tool.\npsql -E\n\n\nGet data back, and only the data\nAdd “-qtA” options to psql in the command line. Those options will have psql run in quiet mode (“-q”), return tuples only (“-t”) in an unaligned fashion (“-A”). Combined with “-c” option to send a single query, it can be useful for your scripts if you want the data and only that back from Postgres. Returns one line per row.\npsql -qtA\n\n\nGet results as an HTML table\nAdd “-qtH” options to psql in the command line. Those options will have psql run in quiet mode (“-q”), return tuples only (“-t”) in an HTML table (“-H”). Combined with “-c” option to send a single query, can be a fast way to embed the result of a query in an HTML page.\npsql -qtH\n\n\nClear your psql screen\nWill clear your screen in current psql session\n\\! clear\n\n\nContinually run a query with watch\nWill automatically run the last query every 2 seconds and display the output. You can also specify the query that will run after watch as well.\n\\watch\n\n\nRollback to previous statement on error when in interactive mode\nWhen you encounter an error when in interactive mode this will automatically rollback to just before the previous command, allowing you to continue working as you would expect.\n\\set ON_ERROR_ROLLBACK interactive\n\n\nExport a CSV from directly in psql\nWhen providing the --csv value with a query, this command will run the specific query and return CSV to STDOUT.\npsql &lt;connection-string&gt; --csv -c 'select * from test;'\n\n\nRun a query from a file in psql\nWill execute the specified file when inside psql.\n\\i filename\n\n\nProvide clean border within psql\nWill give you a border around the output of your queries when in psql\n\\pset border 2\nYou can save this in your .psqlrc to be a default setting\n\n\nSet linestyle to unicode\nChanges the linestyle to unicode, which when combined with above tip leads to much cleaner formatting\n\\pset linestyle unicode\nYou can save this in your .psqlrc to be a default setting"
  },
  {
    "objectID": "content/development/server/index.html",
    "href": "content/development/server/index.html",
    "title": "Server",
    "section": "",
    "text": "Initial Linux server setup steps:\n\nLogin to server as root: ssh root@&lt;ip&gt;\nCreate new admin user: sudo adduser &lt;user&gt;\nAdd user to sudo group: sudo usermod -aG sudo &lt;user&gt;\nDisable root login\n\nOpen SSH configuration: sudo vi /etc/ssh/sshd_config\nSet PermitRootLogin to no\n\nConfigure Uncomplicated Firewall (UFW)\n\nAllow SSH connections: sudo ufw allow OpenSSH\nOpen port 80 and 443 for Apache: sudo ufw allow 'Apache Full'\nTurn on firewall: sudo ufw enable\nCheck: sudo ufw status verbose\n\nReboot the server: sudo reboot\nLogin as new user: ssh &lt;user&gt;@&lt;ip&gt;\nCheck for package updates apt list --upgradable 1, Upgrade packages sudo apt upgrade\nShow system info: uname -a\nEven though the kernel was updated, need to reboot to take effect: sudo reboot\nJust to be sure: sudo apt update && sudo apt upgrade\nShutdown for backup/snapshot sudo shutdown -h now\nAfter snapshot, navigate to provider website and restart machine"
  },
  {
    "objectID": "content/development/server/index.html#server-initialization",
    "href": "content/development/server/index.html#server-initialization",
    "title": "Server",
    "section": "",
    "text": "Initial Linux server setup steps:\n\nLogin to server as root: ssh root@&lt;ip&gt;\nCreate new admin user: sudo adduser &lt;user&gt;\nAdd user to sudo group: sudo usermod -aG sudo &lt;user&gt;\nDisable root login\n\nOpen SSH configuration: sudo vi /etc/ssh/sshd_config\nSet PermitRootLogin to no\n\nConfigure Uncomplicated Firewall (UFW)\n\nAllow SSH connections: sudo ufw allow OpenSSH\nOpen port 80 and 443 for Apache: sudo ufw allow 'Apache Full'\nTurn on firewall: sudo ufw enable\nCheck: sudo ufw status verbose\n\nReboot the server: sudo reboot\nLogin as new user: ssh &lt;user&gt;@&lt;ip&gt;\nCheck for package updates apt list --upgradable 1, Upgrade packages sudo apt upgrade\nShow system info: uname -a\nEven though the kernel was updated, need to reboot to take effect: sudo reboot\nJust to be sure: sudo apt update && sudo apt upgrade\nShutdown for backup/snapshot sudo shutdown -h now\nAfter snapshot, navigate to provider website and restart machine"
  },
  {
    "objectID": "content/development/server/index.html#permissions",
    "href": "content/development/server/index.html#permissions",
    "title": "Server",
    "section": "Permissions",
    "text": "Permissions\n\nSyntax\nGeneral: _rwxrwxrwx 1 owner group\n_ | rwx | rwx | rwx = Special | Owner | Group | All Users\n\nShorthand\n\n4 | r = Read\n2 | w = Write\n1 | x = Execute\n\n\n\nDetailed\n\n0 = ---\n1 = --x\n2 = -w-\n3 = -wx\n4 = r-\n5 = r-x\n6 = rw-\n7 = rwx\n\n\n\n\nCommands\nchgrp = Change group\nExample: sudo chgrp -R &lt;group&gt; &lt;folder&gt;\nchown = Change ownership\nExample: sudo chown -R &lt;user&gt;:&lt;group&gt; &lt;file/folder&gt;\nchmod = Change permissions\nExample: sudo chmod -R 774 &lt;file/folder&gt;\nMake new files inherit the group: sudo chmod g+s &lt;folder&gt;\n\n\nExample\nCreate a shared directory for a group.\n\nCreate a shared directory for users to access: /share\nAssign users to a common group (staff): sudo usermod -a -G staff &lt;user&gt;\nVerify user groups: groups &lt;user&gt;\nCreate shared directory and assign permissions:\nsudo mkdir /share && \\\nsudo chgrp -R staff /share && \\  # assign group\nsudo chmod -R g+w /share && \\  # permissions\nsudo chmod -R +s /share  # inherit permissions for newly created files/folders"
  },
  {
    "objectID": "content/development/server/python-web-app.html",
    "href": "content/development/server/python-web-app.html",
    "title": "Python Web App",
    "section": "",
    "text": "Clone repository:\nsudo mkdir /etc/local/webs && \\\ncd /etc/local/webs && \\\nsudo git clone https://github.com/app.git && \\\nsudo chown -R &lt;user&gt;:&lt;user&gt; app && \\\ncd app\nCreate python virtual environment & install dependencies:\npython -m venv .venv && \\\nsource .venv/bin/activate && \\\npython -m pip install -r requirements.txt\nConfigure WSGI - /usr/local/webs/app/app.wsgi:\n#!/usr/bin/python\nimport sys\nimport logging\nimport os\n\nAPP_DIR = \"/usr/local/webs/app\"\nos.environ[\"APP_DIR\"] = APP_DIR\n\nlogging.basicConfig(stream=sys.stderr)\nsys.path.insert(0, APP_DIR)\n\nfrom app import app as application\nUpdate owner:group\ncd /usr/local/web && \\\nsudo chown -R www-data:www-data cmat"
  },
  {
    "objectID": "content/development/server/python-web-app.html#flask-app-configuration",
    "href": "content/development/server/python-web-app.html#flask-app-configuration",
    "title": "Python Web App",
    "section": "",
    "text": "Clone repository:\nsudo mkdir /etc/local/webs && \\\ncd /etc/local/webs && \\\nsudo git clone https://github.com/app.git && \\\nsudo chown -R &lt;user&gt;:&lt;user&gt; app && \\\ncd app\nCreate python virtual environment & install dependencies:\npython -m venv .venv && \\\nsource .venv/bin/activate && \\\npython -m pip install -r requirements.txt\nConfigure WSGI - /usr/local/webs/app/app.wsgi:\n#!/usr/bin/python\nimport sys\nimport logging\nimport os\n\nAPP_DIR = \"/usr/local/webs/app\"\nos.environ[\"APP_DIR\"] = APP_DIR\n\nlogging.basicConfig(stream=sys.stderr)\nsys.path.insert(0, APP_DIR)\n\nfrom app import app as application\nUpdate owner:group\ncd /usr/local/web && \\\nsudo chown -R www-data:www-data cmat"
  },
  {
    "objectID": "content/development/server/python-web-app.html#apache-configuration",
    "href": "content/development/server/python-web-app.html#apache-configuration",
    "title": "Python Web App",
    "section": "Apache Configuration",
    "text": "Apache Configuration\n\nCreate custom Apache log directory:\nsudo mkdir /usr/local/webs/apache-logs && \\\nsudo touch /usr/local/webs/apache-logs/app/error.log && \\\nsudo touch /usr/local/webs/apache-logs/app/access.log && \\\nsudo chown -R www-data:www-data /usr/local/webs/apache-logs\nCreate configuration file:\ncd /etc/apache2/sites-available && \\\nsudo vi app.conf\nConfiguration - app.conf:\n&lt;VirtualHost *:80&gt;\n    ServerName {DNS}\n\n    ServerSignature Off\n\n    RewriteEngine On\n    RewriteRule ^ https://%{SERVER_NAME}%{REQUEST_URI} [END,NE,R=permanent]\n\n    ErrorLog /var/log/apache2/redirect.error.log\n    LogLevel warn\n&lt;/VirtualHost&gt;\n\n&lt;VirtualHost *:443&gt;\n    ServerAdmin webmaster@localhost\n    ServerName {DNS}\n\n    DocumentRoot /usr/local/webs/app\n\n    WSGIDaemonProcess web-app threads=5 python-home=/usr/local/webs/app/.venv\n    WSGIProcessGroup web-app\n    WSGIScriptAlias / /usr/local/webs/app/app.wsgi\n    WSGIPassAuthorization On\n    &lt;Directory /usr/local/webs/app&gt;\n            Order allow,deny\n            Allow from all\n    &lt;/Directory&gt;\n\n    &lt;Location /&gt;\n            Require all granted\n    &lt;/Location&gt;\n\n    ErrorLog /usr/local/webs/apache-logs/app/error.log\n    CustomLog /usr/local/webs/apache-logs/app/access.log combined\n\n    SSLEngine on\n    SSLCertificateFile /etc/letsencrypt/live/{DNS}/fullchain.pem\n    SSLCertificateKeyFile /etc/letsencrypt/live/{DNS}/privkey.pem\n    Include /etc/letsencrypt/options-ssl-apache.conf\n&lt;/VirtualHost&gt;\nTest configuration:\nsudo apache2ctl configtest\nEnable the site\nsudo a2ensite app.conf\nRestart Apache service\nsudo systemctl restart apache2"
  },
  {
    "objectID": "content/development/tools/docker.html",
    "href": "content/development/tools/docker.html",
    "title": "Docker",
    "section": "",
    "text": "Steps are pulled from a Digital Ocean tutorial.\n\nUpdate list of packages and install prerequisites.\nsudo apt update && \\\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\nAdd the GPG key for the official Docker repository.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nAdd the Docker repository to APT sources and update package list from the new repo.\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"\nMake sure install is pulling from Docker repo instead of default Ubuntu.\napt-cache policy docker-ce\nInstall Docker.\nsudo apt install docker-ce\nCheck that Docker is running.\nsudo systemctl status docker\nExecute Docker commands without sudo.\nsudo usermod -aG docker ${USER} && \\\nsu - ${USER}\nVerify user is now aded to the docker group.\ngroups"
  },
  {
    "objectID": "content/development/tools/docker.html#installation",
    "href": "content/development/tools/docker.html#installation",
    "title": "Docker",
    "section": "",
    "text": "Steps are pulled from a Digital Ocean tutorial.\n\nUpdate list of packages and install prerequisites.\nsudo apt update && \\\nsudo apt install apt-transport-https ca-certificates curl software-properties-common\nAdd the GPG key for the official Docker repository.\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\nAdd the Docker repository to APT sources and update package list from the new repo.\nsudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\"\nMake sure install is pulling from Docker repo instead of default Ubuntu.\napt-cache policy docker-ce\nInstall Docker.\nsudo apt install docker-ce\nCheck that Docker is running.\nsudo systemctl status docker\nExecute Docker commands without sudo.\nsudo usermod -aG docker ${USER} && \\\nsu - ${USER}\nVerify user is now aded to the docker group.\ngroups"
  },
  {
    "objectID": "content/development/tools/docker.html#resources",
    "href": "content/development/tools/docker.html#resources",
    "title": "Docker",
    "section": "Resources",
    "text": "Resources\n\nCheatSheet\nConvert Docker command to docker-compose.yml"
  },
  {
    "objectID": "content/development/tools/docker.html#image-vs.-container",
    "href": "content/development/tools/docker.html#image-vs.-container",
    "title": "Docker",
    "section": "Image vs. Container",
    "text": "Image vs. Container\nImage - Application we want to run\nContainer - Instance of that image running as a process"
  },
  {
    "objectID": "content/development/tools/docker.html#docker-basics",
    "href": "content/development/tools/docker.html#docker-basics",
    "title": "Docker",
    "section": "Docker Basics",
    "text": "Docker Basics\n\nCreate an Nginx container\ndocker run -p 80:80 -d --name webhost nginx\n\nDownloads Nginx from Docker Hub\nStarts new container from that image\nOpened port 80 on host IP\nRoutes port 80 traffic to the container IP, port 80\nView container at http://localhost:80\n\n\n\nOther examples\ndocker run -p 80:80 -d --name nginx nginx\ndocker run -p 8080:80 -d --name httpd httpd\ndocker run -p 3306:3306 --platform linux/amd64 -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql\nCreate a JupyterLab instance and attach your current directory as a volume: docker run -it --rm -p 8888:8888 -v $(PWD):/home/jovyan jupyter/pyspark-notebook\n\n\nProcesses and configurations\nCheck processes running inside a container: docker top &lt;container&gt;\nContainer configuration: docker &lt;container&gt; inspect\nCheck container stats (memory, cpu, network): docker stats &lt;container&gt;\n\n\nGetting a shell inside containers\nStart a new container interactively: docker run -it &lt;container&gt;\nRun commands in existing container: docker exec -it &lt;container&gt;\n\nExample: Start a container interactively and launch bash within it\n\nStart container and launch bash: docker run -it --name ubuntu ubuntu bash\nRun some bash command: apt-get install -y curl\nExit the container: exit\nStart and re-enter the container: docker start -ai ubuntu\n\n\n\nExample: Launch shell in running container\ndocker exec -it &lt;container&gt; bash\n\n\n\nPull an image from docker hub\ndocker pull &lt;imagename&gt;"
  },
  {
    "objectID": "content/development/tools/docker.html#docker-networks",
    "href": "content/development/tools/docker.html#docker-networks",
    "title": "Docker",
    "section": "Docker Networks",
    "text": "Docker Networks\n\nEach container is connected to a private virtual network (called “bridge”).\nEach virtual network routes through NAT firewall on host IP.\nAll containers on a virtual network can talk to each other without -p\nBest practice: Create a new virtual network for each app.\nYou can skip virtual networks and use the host IP (--net=host).\n\nGet container IP: docker inspect --format '{{ .NetworkSettings.IPAddress }}' &lt;container&gt;\n\nPublishing (#:#)\nexample: 8080:80\nleft number: published/host port\nright number: listening/container port\nTraffic passing through port 8080 on the HOST will be directed to port 80 on the container.\n\n\nDNS\nDocker uses container names as host names.\nDont rely on IPs for inter-communication.\nBest Practice Always use custom networks.\n\nAssignment\nCheck different curl versions within current versions of Ubuntu and CentOS.\nRun “curl –version” on both operating systems.\n\nSteps\nubuntu: apt-get update && apt-get install curl\ncentos: yum update curl\nThen…\ncurl --version\nAlso:\nCheck out command docker --rm"
  },
  {
    "objectID": "content/development/tools/docker.html#dockerfiles",
    "href": "content/development/tools/docker.html#dockerfiles",
    "title": "Docker",
    "section": "Dockerfiles",
    "text": "Dockerfiles\nRecipe for creating images\nEach Dockerfile stanza such as “RUN”, “CMD”, etc. are stored as a single image layer. Docker caches each layer by giving it a unique SHA (hash), so whenever the image is (re)built, it can check to see if a layer has changed, and if not, it will use the cached layer.\nDocker builds images top down, so it is best practice to structure the Dockerfile in such a way that lines which will change the most are at the bottom, and lines that will change the least are at the top. If a line is changed (ie. source code changes) Docker will rebuild that line, and thus each line after that will also need to be rebuilt."
  },
  {
    "objectID": "content/development/tools/docker.html#keeping-the-docker-system-clean",
    "href": "content/development/tools/docker.html#keeping-the-docker-system-clean",
    "title": "Docker",
    "section": "Keeping the Docker system clean",
    "text": "Keeping the Docker system clean\ndocker system prune - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache"
  },
  {
    "objectID": "content/development/tools/docker.html#volumes-an-bind-mounts",
    "href": "content/development/tools/docker.html#volumes-an-bind-mounts",
    "title": "Docker",
    "section": "Volumes an Bind Mounts",
    "text": "Volumes an Bind Mounts\nVolumes - Special location outside of container UFS\nBind Mounts - Link container path to host path\nBuild an image and named volume (persistent): docker run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=True -v mysql:/var/lib/mysql --platform linux/amd64 mysql"
  },
  {
    "objectID": "content/development/tools/docker.html#rebuilding-a-compose-service",
    "href": "content/development/tools/docker.html#rebuilding-a-compose-service",
    "title": "Docker",
    "section": "Rebuilding a Compose Service",
    "text": "Rebuilding a Compose Service\ndocker compose up -d --no-deps --build &lt;service_name&gt;"
  },
  {
    "objectID": "content/development/tools/github.html",
    "href": "content/development/tools/github.html",
    "title": "GitHub",
    "section": "",
    "text": "Action to push app/site to remote host: https://claritydev.net/blog/automate-deployment-workflow-nextjs-digitalocean-github-actions"
  },
  {
    "objectID": "content/resources/bookmarks.html",
    "href": "content/resources/bookmarks.html",
    "title": "Bookmarks",
    "section": "",
    "text": "devdocs.io - Searchable documentations\nss64 - CLI reference guide\nReadTheDocs - Create, host, and browse documentation\nexecsql - Run SQL with metacommands\nBootstrap - Web framework\npgAdmin - PostgreSQL sandbox\nRegex - Regular expressions\nGoogle Colab - Collaborative Python notebooks\ngeojson.io - Create, view, and share maps\nrepl.it - Collaborative in-browser IDE. 50+ languages\nIntegromat - Online scenario automation\nPostgreSQL cheatsheet - PostgreSQL cheatsheet\npython-utils - Playground for Python utilities\nSpektran - Collection of useful color tools\nObservable - JavaScript notebooks\nHTML Dog - HTML tutorials\nCrontab - Crontab scheduler\nMedia Library\nUTF-8 Character Debug - UTF-8 character debugging chart\nLaTeX Basics\nQuarto - Scientific and technical publishing system built on Pandoc"
  },
  {
    "objectID": "content/resources/bookmarks.html#developer",
    "href": "content/resources/bookmarks.html#developer",
    "title": "Bookmarks",
    "section": "",
    "text": "devdocs.io - Searchable documentations\nss64 - CLI reference guide\nReadTheDocs - Create, host, and browse documentation\nexecsql - Run SQL with metacommands\nBootstrap - Web framework\npgAdmin - PostgreSQL sandbox\nRegex - Regular expressions\nGoogle Colab - Collaborative Python notebooks\ngeojson.io - Create, view, and share maps\nrepl.it - Collaborative in-browser IDE. 50+ languages\nIntegromat - Online scenario automation\nPostgreSQL cheatsheet - PostgreSQL cheatsheet\npython-utils - Playground for Python utilities\nSpektran - Collection of useful color tools\nObservable - JavaScript notebooks\nHTML Dog - HTML tutorials\nCrontab - Crontab scheduler\nMedia Library\nUTF-8 Character Debug - UTF-8 character debugging chart\nLaTeX Basics\nQuarto - Scientific and technical publishing system built on Pandoc"
  },
  {
    "objectID": "content/resources/bookmarks.html#gis",
    "href": "content/resources/bookmarks.html#gis",
    "title": "Bookmarks",
    "section": "GIS",
    "text": "GIS\n\nepsg.io - Spatial reference systems\nArcGIS - ArcGIS Python"
  },
  {
    "objectID": "content/resources/bookmarks.html#music",
    "href": "content/resources/bookmarks.html#music",
    "title": "Bookmarks",
    "section": "Music",
    "text": "Music\n\nSongsterr - Guitar tabs\nUltimate Guitar - Guitar tabs\nFlagrantior - Music theory\nmusictheory.net - Music theory\nTeoria - Music theory\nJustinGuitar - Guitar lessons\nHowToPlayPiano - Piano lessons\nSoundation - Broswer based music maker"
  },
  {
    "objectID": "content/resources/bookmarks.html#e-books",
    "href": "content/resources/bookmarks.html#e-books",
    "title": "Bookmarks",
    "section": "e-Books",
    "text": "e-Books\n\nProject Gutenberg\nLibriVox\nStandard eBooks"
  },
  {
    "objectID": "content/resources/bookmarks.html#miscellaneous",
    "href": "content/resources/bookmarks.html#miscellaneous",
    "title": "Bookmarks",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nFree Learning - List of free educational resources"
  },
  {
    "objectID": "content/resources/index.html",
    "href": "content/resources/index.html",
    "title": "Resources",
    "section": "",
    "text": "Bookmarks\n\n\nBookmarks to miscellaneous sites.\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Management\n\n\nData management guidelines and resources\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecipes\n\n\nRecipies\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorkouts\n\n\nWorkouts and routines\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/resources/workouts.html",
    "href": "content/resources/workouts.html",
    "title": "Workouts",
    "section": "",
    "text": "Code\nimport pandas as pd\n\nworkouts = \"../../static/resources/workouts.xlsx\"\nw1 = pd.read_excel(workouts, sheet_name=0)\nw2 = pd.read_excel(workouts, sheet_name=1)\nw3 = pd.read_excel(workouts, sheet_name=2)\nw4 = pd.read_excel(workouts, sheet_name=3)\nschedule = pd.read_excel(workouts, sheet_name=4)\n\n\nThe following regimen outlines a 4 week lift cycle. The routine is a slightly modified version of this. Non-lift days should be supplemented with active recovery workouts.\n\n\n\n\nCode\nschedule.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nWeek\n\n\n\nSunday\n\n\n\nMonday\n\n\n\nTuesday\n\n\n\nWednesday\n\n\n\nThursday\n\n\n\nFriday\n\n\n\nSaturday\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nBack & Biceps\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nChest & Triceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n2\n\n\n\nCardio\n\n\n\nShoulders & Traps, Run\n\n\n\nChest & Triceps\n\n\n\nBack & Biceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n3\n\n\n\nCardio\n\n\n\nBack & Biceps, Run\n\n\n\nShoulders & Traps\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n4\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw1.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nDeadlift\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nPull-Ups\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nLat Pull Downs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nRows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFace Pulls\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBarbell Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\n60 seconds\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n15\n\n\n\n\n\n\n\nSit Ups\n\n\n\n3\n\n\n\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw2.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBench Press\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nIncline Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFlys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDips\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nTricep Pushdowns\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nAb Roller\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nSitting Twists\n\n\n\n3\n\n\n\n30\n\n\n\n\n\n\n\nWipers\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw3.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBack Squat\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nFront Squat\n\n\n\n4\n\n\n\n6\n\n\n\n\n\n\n\nLunges\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nKettelbell Deadlift\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nLeg Kickbacks\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nGlute Bridges\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw4.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nMilitary Press\n\n\n\n4\n\n\n\n12, 8, 5, 3\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nRear Delt Flys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nBarbell Shrugs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nOne Arm Dumbbell Snatch\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nKettlebell Swing\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nFarmers Carry\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nScissor Kicks\n\n\n\n3\n\n\n\n30 seconds\n\n\n\n\n\n\n\nL-sit\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nToe Taps\n\n\n\n3\n\n\n\n20"
  },
  {
    "objectID": "content/resources/workouts.html#routines",
    "href": "content/resources/workouts.html#routines",
    "title": "Workouts",
    "section": "",
    "text": "Code\nimport pandas as pd\n\nworkouts = \"../../static/resources/workouts.xlsx\"\nw1 = pd.read_excel(workouts, sheet_name=0)\nw2 = pd.read_excel(workouts, sheet_name=1)\nw3 = pd.read_excel(workouts, sheet_name=2)\nw4 = pd.read_excel(workouts, sheet_name=3)\nschedule = pd.read_excel(workouts, sheet_name=4)\n\n\nThe following regimen outlines a 4 week lift cycle. The routine is a slightly modified version of this. Non-lift days should be supplemented with active recovery workouts.\n\n\n\n\nCode\nschedule.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nWeek\n\n\n\nSunday\n\n\n\nMonday\n\n\n\nTuesday\n\n\n\nWednesday\n\n\n\nThursday\n\n\n\nFriday\n\n\n\nSaturday\n\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nBack & Biceps\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nChest & Triceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n2\n\n\n\nCardio\n\n\n\nShoulders & Traps, Run\n\n\n\nChest & Triceps\n\n\n\nBack & Biceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n3\n\n\n\nCardio\n\n\n\nBack & Biceps, Run\n\n\n\nShoulders & Traps\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n4\n\n\n\nCardio\n\n\n\nChest & Triceps, Run\n\n\n\nLegs\n\n\n\nShoulders & Traps, Run\n\n\n\nLegs\n\n\n\nBack & Biceps, Run\n\n\n\nCardio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw1.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nDeadlift\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nPull-Ups\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nLat Pull Downs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nRows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFace Pulls\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBarbell Curls\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\n60 seconds\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n15\n\n\n\n\n\n\n\nSit Ups\n\n\n\n3\n\n\n\n25\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw2.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBench Press\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nIncline Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFlys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDips\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nTricep Pushdowns\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nDumbbell Press\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nAb Roller\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nSitting Twists\n\n\n\n3\n\n\n\n30\n\n\n\n\n\n\n\nWipers\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw3.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nBack Squat\n\n\n\n4\n\n\n\n10, 8, 5, 3\n\n\n\n\n\n\n\nFront Squat\n\n\n\n4\n\n\n\n6\n\n\n\n\n\n\n\nLunges\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nKettelbell Deadlift\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nLeg Kickbacks\n\n\n\n3\n\n\n\n12\n\n\n\n\n\n\n\nBack Hyperextension\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nCalf Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nGlute Bridges\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nw4.to_html(index=False)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\n\n\n\n\n\n\n\n\nMilitary Press\n\n\n\n4\n\n\n\n12, 8, 5, 3\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nRear Delt Flys\n\n\n\n4\n\n\n\n12\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nBarbell Shrugs\n\n\n\n4\n\n\n\n10\n\n\n\n\n\n\n\nOne Arm Dumbbell Snatch\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nKettlebell Swing\n\n\n\n3\n\n\n\n10\n\n\n\n\n\n\n\nFarmers Carry\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nScissor Kicks\n\n\n\n3\n\n\n\n30 seconds\n\n\n\n\n\n\n\nL-sit\n\n\n\n3\n\n\n\nFailure\n\n\n\n\n\n\n\nToe Taps\n\n\n\n3\n\n\n\n20"
  },
  {
    "objectID": "content/resources/workouts.html#introductory-routines",
    "href": "content/resources/workouts.html#introductory-routines",
    "title": "Workouts",
    "section": "Introductory Routines",
    "text": "Introductory Routines\n\n\nCode\nw5 = pd.read_excel(workouts, sheet_name=5)\nw6 = pd.read_excel(workouts, sheet_name=6)\nw7 = pd.read_excel(workouts, sheet_name=7)\n\n\n\nPush Intro\n\n\nCode\nw5.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nDumbbell Bench Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbell Incline Press\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Flys\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nLateral Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFront Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nTricep Push Down\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nPlanks\n\n\n\n3\n\n\n\nFailure\n\n\n\nView\n\n\n\n\n\n\n\nSit-ups\n\n\n\n3\n\n\n\n12\n\n\n\nView\n\n\n\n\n\n\n\nLeg Raises\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\nPull Intro\n\n\nCode\nw6.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nCable Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFarmer Carry\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\n\n\n\n\n\nLegs Intro\n\n\nCode\nw6.to_html(index=False, render_links=True)\n\n\n\n\n\n\n\n\n\n\nLift\n\n\n\nSets\n\n\n\nReps\n\n\n\nHow-To\n\n\n\n\n\n\n\n\n\n\n\nCable Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nUpright Rows\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nDumbbell Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nHammer Curls\n\n\n\n3\n\n\n\n10\n\n\n\nView\n\n\n\n\n\n\n\nFarmer Carry\n\n\n\n3\n\n\n\n10\n\n\n\nView"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html",
    "href": "content/development/code/notebooks/data-science.html",
    "title": "Data Science",
    "section": "",
    "text": "Convert Jupyter Notebook to static HTML: $ jupyter nbconvert --to html NOTEBOOK-NAME.ipynb\nimport os\nimport pandas as pd\ndata_dir = \"./data/titanic\"\nd_test = os.path.join(data_dir, \"test.csv\")\nd_train = os.path.join(data_dir, \"train.csv\")\ndf = pd.read_csv(d_train)\ndf\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n887\n0\n2\nMontvila, Rev. Juozas\nmale\n27.0\n0\n0\n211536\n13.0000\nNaN\nS\n\n\n887\n888\n1\n1\nGraham, Miss. Margaret Edith\nfemale\n19.0\n0\n0\n112053\n30.0000\nB42\nS\n\n\n888\n889\n0\n3\nJohnston, Miss. Catherine Helen \"Carrie\"\nfemale\nNaN\n1\n2\nW./C. 6607\n23.4500\nNaN\nS\n\n\n889\n890\n1\n1\nBehr, Mr. Karl Howell\nmale\n26.0\n0\n0\n111369\n30.0000\nC148\nC\n\n\n890\n891\n0\n3\nDooley, Mr. Patrick\nmale\n32.0\n0\n0\n370376\n7.7500\nNaN\nQ\n\n\n\n\n891 rows × 12 columns"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#question",
    "href": "content/development/code/notebooks/data-science.html#question",
    "title": "Data Science",
    "section": "Question",
    "text": "Question\nPredict who would survive and who won’t"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#explore-the-data",
    "href": "content/development/code/notebooks/data-science.html#explore-the-data",
    "title": "Data Science",
    "section": "Explore the data",
    "text": "Explore the data\n\n# rows, columns\ndf.shape\n\n(891, 12)\n\n\n\n# bits of data (rows x columns)\ndf.size\n\n10692\n\n\n\n# shows summary of numerical data types\ndf.describe()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\ndf.info()\n\n# dtype of \"object\" means column is incomplete (missing value) and datatype cannot be determined\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\ndf.sample()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n368\n369\n1\n3\nJermyn, Miss. Annie\nfemale\nNaN\n0\n0\n14313\n7.75\nNaN\nQ\n\n\n\n\n\n\n\n\ndf.columns\n\nIndex(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n      dtype='object')"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#accessing-pandas-dataframe-columns",
    "href": "content/development/code/notebooks/data-science.html#accessing-pandas-dataframe-columns",
    "title": "Data Science",
    "section": "Accessing pandas dataframe columns",
    "text": "Accessing pandas dataframe columns\n\ndf[\"Fare\"] = 0\ndf.sample()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n250\n251\n0\n3\nReed, Mr. James George\nmale\nNaN\n0\n0\n362316\n0\nNaN\nS"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#delete-a-column",
    "href": "content/development/code/notebooks/data-science.html#delete-a-column",
    "title": "Data Science",
    "section": "Delete a column",
    "text": "Delete a column\n\ndel df[\"Cabin\"]\ndf.sample()\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nEmbarked\n\n\n\n\n699\n700\n0\n3\nHumblen, Mr. Adolf Mathias Nicolai Olsen\nmale\n42.0\n0\n0\n348121\n0\nS\n\n\n\n\n\n\n\n\n# reset dataframe\ndf = pd.read_csv(d_train)"
  },
  {
    "objectID": "content/development/code/notebooks/data-science.html#variable-types",
    "href": "content/development/code/notebooks/data-science.html#variable-types",
    "title": "Data Science",
    "section": "Variable Types",
    "text": "Variable Types\nDependent vs Independent variable types\n\nCategorical\n\nNominal - Any number of categories, order not important (eg male or female)\nOrdinal - Order is important (eg educational background)\n\nNumerical\n\nDiscrete - Count, dice throwing, etc.\nContinuous - Height of a person, weight, height of a tree, speed of a car."
  },
  {
    "objectID": "content/development/code/notebooks/logging.html",
    "href": "content/development/code/notebooks/logging.html",
    "title": "Logging",
    "section": "",
    "text": ".\n├── script.py\n└── my_library\n    ├── __init__.py\n    ├── module.py\n    └── submodule.py"
  },
  {
    "objectID": "content/development/code/notebooks/logging.html#script.py",
    "href": "content/development/code/notebooks/logging.html#script.py",
    "title": "Logging",
    "section": "script.py",
    "text": "script.py\n\nimport os\nimport logging\nfrom datetime import datetime\n\nfrom my_library import module\n\n# Do not specify __name__ to use root log level\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.INFO)\nformatter = logging.Formatter(\n    \"%(asctime)s : %(msecs)04d : %(name)s : %(levelname)s : %(message)s\",\n    datefmt=\"%Y-%m-%d %H:%M:%S\",\n)\nlog_file = f\"log_{datetime.now().strftime('%Y-%m-%d_%H%M%S')}.log\"\nstream_handler = logging.StreamHandler()\nstream_handler.setFormatter(formatter)\nfile_handler = logging.FileHandler(filename=log_file)\nfile_handler.setFormatter(formatter)\nlogger.addHandler(stream_handler)\nlogger.addHandler(file_handler)\nlogger.info(\"Begin my test module\")\n\nmodule.main()\n\nos.remove(log_file)\n\n2022-12-17 06:03:54 : 0441 : __main__ : INFO : Begin my test module\n2022-12-17 06:03:54 : 0443 : __main__ : INFO : Log message from main function in module.py\n2022-12-17 06:03:54 : 0444 : __main__ : INFO : Hello from submodule.py in function bar()"
  },
  {
    "objectID": "content/development/code/notebooks/logging.html#module.py",
    "href": "content/development/code/notebooks/logging.html#module.py",
    "title": "Logging",
    "section": "module.py",
    "text": "module.py\nimport logging\nfrom my_library import submodule\n\n\ndef main():\n    logger = logging.getLogger(\"__main__\")\n    logger.info(\"Log message from main function in module.py\")\n    submodule.bar()\n\n\nif __name__ == \"__main__\":\n    main()"
  },
  {
    "objectID": "content/development/code/notebooks/logging.html#submodule.py",
    "href": "content/development/code/notebooks/logging.html#submodule.py",
    "title": "Logging",
    "section": "submodule.py",
    "text": "submodule.py\nimport logging\n\n\ndef bar():\n    logger = logging.getLogger()\n    logger.info(\"Hello from submodule.py in function bar()\")"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html",
    "href": "content/development/code/notebooks/ml-train-and-test.html",
    "title": "ML Training",
    "section": "",
    "text": "PROTIP - type function name with empty paranthesis and press shift+tab inside paranthesis to see documentation\nimport os\nimport pandas as pd\ndata_dir = \"./data/titanic\"\ndf = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ndf.sample(5)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n301\n302\n1\n3\nMcCoy, Mr. Bernard\nmale\nNaN\n2\n0\n367226\n23.25\nNaN\nQ\n\n\n179\n180\n0\n3\nLeonard, Mr. Lionel\nmale\n36.0\n0\n0\nLINE\n0.00\nNaN\nS\n\n\n865\n866\n1\n2\nBystrom, Mrs. (Karolina)\nfemale\n42.0\n0\n0\n236852\n13.00\nNaN\nS\n\n\n112\n113\n0\n3\nBarton, Mr. David John\nmale\n22.0\n0\n0\n324669\n8.05\nNaN\nS\n\n\n530\n531\n1\n2\nQuick, Miss. Phyllis May\nfemale\n2.0\n1\n1\n26360\n26.00\nNaN\nS\ny= dependent variable\nx = independent variable\ny = df[\"Survived\"]\nx = df.drop([\"Survived\", \"PassengerId\"], axis=1)\nx.sample()\n\n\n\n\n\n\n\n\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n732\n2\nKnight, Mr. Robert J\nmale\nNaN\n0\n0\n239855\n0.0\nNaN\nS\nfrom sklearn.model_selection import train_test_split\n# Order matters (train, test)\n\n# test_size = what percent of training data goes into test model\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\nx_train.shape\n\n(801, 10)\nx_test.shape\n\n(90, 10)\nDecision Trees - nodes, branches, leafs\nProne to overfitting. Overcome by using random forests and using multiple iterations"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html#first-ml-model",
    "href": "content/development/code/notebooks/ml-train-and-test.html#first-ml-model",
    "title": "ML Training",
    "section": "First ML Model",
    "text": "First ML Model\n\nGetting started\n\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata_dir = \"./data/titanic\"\ndf = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ndf.sample(5)\n\ny = df[\"Survived\"]\nx = df.drop([\"Survived\", \"PassengerId\"], axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1)\n\ndf.info()\ndf.describe()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.000000\n891.000000\n891.000000\n714.000000\n891.000000\n891.000000\n891.000000\n\n\nmean\n446.000000\n0.383838\n2.308642\n29.699118\n0.523008\n0.381594\n32.204208\n\n\nstd\n257.353842\n0.486592\n0.836071\n14.526497\n1.102743\n0.806057\n49.693429\n\n\nmin\n1.000000\n0.000000\n1.000000\n0.420000\n0.000000\n0.000000\n0.000000\n\n\n25%\n223.500000\n0.000000\n2.000000\n20.125000\n0.000000\n0.000000\n7.910400\n\n\n50%\n446.000000\n0.000000\n3.000000\n28.000000\n0.000000\n0.000000\n14.454200\n\n\n75%\n668.500000\n1.000000\n3.000000\n38.000000\n1.000000\n0.000000\n31.000000\n\n\nmax\n891.000000\n1.000000\n3.000000\n80.000000\n8.000000\n6.000000\n512.329200\n\n\n\n\n\n\n\n\n# Count number of null values per column\ndf.isnull().sum()\n\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n\n\n\n# List all of peoples tables\ndef get_title(name):\n    if \".\" in name:\n        return name.split(\",\")[1].split(\".\")[0].strip()\n    else:\n        return \"Unknown\"\n\n\ntitles = sorted(set([x for x in df.Name.map(lambda x: get_title(x))]))\nprint(\"Different titles found in the dataset: \")\nprint(len(titles), \":\", titles)\n\nDifferent titles found in the dataset: \n17 : ['Capt', 'Col', 'Don', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Master', 'Miss', 'Mlle', 'Mme', 'Mr', 'Mrs', 'Ms', 'Rev', 'Sir', 'the Countess']\n\n\n\n# Normalize the titles\ndef replace_titles(x):\n    title = x[\"Title\"]\n    if title in [\"Capt\", \"Col\", \"Major\"]:\n        return \"Officer\"\n    elif title in [\"Jonkheer\", \"Don\", \"the Countess\", \"Dona\", \"Lady\", \"Sir\"]:\n        return \"Royalty\"\n    elif title in [\"Mme\"]:\n        return \"Mrs\"\n    elif title in [\"Mlle\", \"Ms\"]:\n        return \"Miss\"\n    else:\n        return title\n\n\ndf[\"Title\"] = df[\"Name\"].map(lambda x: get_title(x))\ndf[\"Title\"] = df.apply(replace_titles, axis=1)\nprint(df.Title.value_counts())\n\nMr         517\nMiss       185\nMrs        126\nMaster      40\nDr           7\nRev          6\nOfficer      5\nRoyalty      5\nName: Title, dtype: int64\n\n\n\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\ndf[\"Embarked\"].fillna(\"S\", inplace=True)\ndf.drop(\"Cabin\", axis=1, inplace=True)\ndf.drop(\"Ticket\", axis=1, inplace=True)\ndf.drop(\"Name\", axis=1, inplace=True)\ndf.Sex.replace((\"male\", \"female\"), (0, 1), inplace=True)\ndf.Embarked.replace((\"S\", \"C\", \"Q\"), (0, 1, 2), inplace=True)\ndf.Title.replace(\n    (\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Office\", \"Royalty\"),\n    (0, 1, 2, 3, 4, 5, 6, 7),\n    inplace=True,\n)\n\nprint(df.isnull().sum())\nprint(df[\"Sex\"].sample(5))\nprint(df.columns)\n\nPassengerId    0\nSurvived       0\nPclass         0\nSex            0\nAge            0\nSibSp          0\nParch          0\nFare           0\nEmbarked       0\nTitle          0\ndtype: int64\n791    0\n161    1\n284    0\n323    1\n127    0\nName: Sex, dtype: int64\nIndex(['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch',\n       'Fare', 'Embarked', 'Title'],\n      dtype='object')\n\n\n\n# Correlate 2 columns\ncorr = df.corr()\ncorr.Survived\n\nPassengerId   -0.005007\nSurvived       1.000000\nPclass        -0.338481\nSex            0.543351\nAge           -0.064910\nSibSp         -0.035322\nParch          0.081629\nFare           0.257307\nEmbarked       0.106811\nName: Survived, dtype: float64"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html#machine-learning-model---putting-it-all-together",
    "href": "content/development/code/notebooks/ml-train-and-test.html#machine-learning-model---putting-it-all-together",
    "title": "ML Training",
    "section": "Machine Learning Model - Putting it all together",
    "text": "Machine Learning Model - Putting it all together\n\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndata_dir = \"../10_Data Science/data/titanic\"\ndf = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\ndf.sample(5)\n\n\n# List all of peoples tables\ndef get_title(name):\n    if \".\" in name:\n        return name.split(\",\")[1].split(\".\")[0].strip()\n    else:\n        return \"Unknown\"\n\n\ntitles = sorted(set([x for x in df.Name.map(lambda x: get_title(x))]))\n\n\n# Normalize the titles\ndef replace_titles(x):\n    title = x[\"Title\"]\n    if title in [\"Capt\", \"Col\", \"Major\"]:\n        return \"Officer\"\n    elif title in [\"Jonkheer\", \"Don\", \"the Countess\", \"Dona\", \"Lady\", \"Sir\"]:\n        return \"Royalty\"\n    elif title in [\"Mme\"]:\n        return \"Mrs\"\n    elif title in [\"Mlle\", \"Ms\"]:\n        return \"Miss\"\n    else:\n        return title\n\n\ndf[\"Title\"] = df[\"Name\"].map(lambda x: get_title(x))\ndf[\"Title\"] = df.apply(replace_titles, axis=1)\n\n# Normalize data\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\ndf[\"Embarked\"].fillna(\"S\", inplace=True)\ndf.drop(\"Cabin\", axis=1, inplace=True)\ndf.drop(\"Ticket\", axis=1, inplace=True)\ndf.drop(\"Name\", axis=1, inplace=True)\ndf.Sex.replace((\"male\", \"female\"), (0, 1), inplace=True)\ndf.Embarked.replace((\"S\", \"C\", \"Q\"), (0, 1, 2), inplace=True)\ndf.Title.replace(\n    (\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Officer\", \"Royalty\"),\n    (0, 1, 2, 3, 4, 5, 6, 7),\n    inplace=True,\n)\n\ny = df[\"Survived\"]\nx = df.drop([\"Survived\", \"PassengerId\"], axis=1)\n\n# print(x.sample())\n# print(y.sample())\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1)\n\n\n# Saving the model\nimport pickle\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrandomforest = RandomForestClassifier()  # initiate random forest classification\nrandomforest.fit(x_train, y_train)  # train the model\ny_pred = randomforest.predict(x_val)  # Make some predicions using the x validation\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(\"Accuracy: {}\".format(acc_randomforest))\n\npickle.dump(randomforest, open(\"titanic_model.sav\", \"wb\"))\n\nAccuracy: 83.33"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html#make-some-ml-predictions",
    "href": "content/development/code/notebooks/ml-train-and-test.html#make-some-ml-predictions",
    "title": "ML Training",
    "section": "Make some ML predictions",
    "text": "Make some ML predictions\n\ndf_test = pd.read_csv(os.path.join(data_dir, \"test.csv\"))\n\ndf_test[\"Title\"] = df_test[\"Name\"].map(lambda x: get_title(x))\ndf_test[\"Title\"] = df_test.apply(replace_titles, axis=1)\n\nids = df_test[\"PassengerId\"]\n\ndf_test[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf_test[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\ndf_test[\"Embarked\"].fillna(\"S\", inplace=True)\ndf_test.drop(\"Cabin\", axis=1, inplace=True)\ndf_test.drop(\"Ticket\", axis=1, inplace=True)\ndf_test.drop(\"Name\", axis=1, inplace=True)\ndf_test.drop(\"PassengerId\", axis=1, inplace=True)\ndf_test.Sex.replace((\"male\", \"female\"), (0, 1), inplace=True)\ndf_test.Embarked.replace((\"S\", \"C\", \"Q\"), (0, 1, 2), inplace=True)\ndf_test.Title.replace(\n    (\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Officer\", \"Royalty\"),\n    (0, 1, 2, 3, 4, 5, 6, 7),\n    inplace=True,\n)\n\ndf_test.sample()\n\n\n\n\n\n\n\n\nPclass\nSex\nAge\nSibSp\nParch\nFare\nEmbarked\nTitle\n\n\n\n\n110\n2\n0\n41.0\n0\n0\n15.0458\n1\n0\n\n\n\n\n\n\n\n\npredictions = randomforest.predict(df_test)\noutput = pd.DataFrame({\"PassengerId\": ids, \"Survived\": predictions})\noutput.to_csv(\"submission.csv\", index=False)\n\n\nimport numpy as np\n\nx = [1, 2, 3, 4, 5]\ny = [5, 7, 9, 13, 23]\nm, b = np.polyfit(x, y, 1)\nprint(m, b)\n\n4.2 -1.2000000000000026"
  },
  {
    "objectID": "content/development/code/notebooks/ml-train-and-test.html#ml-compiled-predictor",
    "href": "content/development/code/notebooks/ml-train-and-test.html#ml-compiled-predictor",
    "title": "ML Training",
    "section": "ML Compiled Predictor",
    "text": "ML Compiled Predictor\nCompiled machine learning predictor.\n\nimport os\nimport pickle\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\ndata_dir = \"./data/titanic\"\ndf = pd.read_csv(os.path.join(data_dir, \"train.csv\"))\n\n\n# List all of peoples tables\ndef get_title(name):\n    if \".\" in name:\n        return name.split(\",\")[1].split(\".\")[0].strip()\n    else:\n        return \"Unknown\"\n\n\n# Normalize the titles\ndef replace_titles(x):\n    title = x[\"Title\"]\n    if title in [\"Capt\", \"Col\", \"Major\"]:\n        return \"Officer\"\n    elif title in [\"Jonkheer\", \"Don\", \"the Countess\", \"Dona\", \"Lady\", \"Sir\"]:\n        return \"Royalty\"\n    elif title in [\"the Countess\", \"Mme\", \"Lady\"]:\n        return \"Mrs\"\n    elif title in [\"Mlle\", \"Ms\"]:\n        return \"Miss\"\n    else:\n        return title\n\n\ndf[\"Title\"] = df[\"Name\"].map(lambda x: get_title(x))\ndf[\"Title\"] = df.apply(replace_titles, axis=1)\n\n# Normalize data\ndf[\"Age\"].fillna(df[\"Age\"].median(), inplace=True)\ndf[\"Fare\"].fillna(df[\"Fare\"].median(), inplace=True)\ndf[\"Embarked\"].fillna(\"S\", inplace=True)\ndf.drop(\"Cabin\", axis=1, inplace=True)\ndf.drop(\"Ticket\", axis=1, inplace=True)\ndf.drop(\"Name\", axis=1, inplace=True)\ndf.Sex.replace((\"male\", \"female\"), (0, 1), inplace=True)\ndf.Embarked.replace((\"S\", \"C\", \"Q\"), (0, 1, 2), inplace=True)\ndf.Title.replace(\n    (\"Mr\", \"Miss\", \"Mrs\", \"Master\", \"Dr\", \"Rev\", \"Officer\", \"Royalty\"),\n    (0, 1, 2, 3, 4, 5, 6, 7),\n    inplace=True,\n)\n\ny = df[\"Survived\"]\nx = df.drop([\"Survived\", \"PassengerId\"], axis=1)\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1)\n\nrandomforest = RandomForestClassifier()  # initiate random forest classification\nrandomforest.fit(x_train, y_train)  # train the model\n\npickle.dump(randomforest, open(\"titanic_model.sav\", \"wb\"))\n\n\ndef prediction_model(pclass, sex, age, sibsp, parch, fare, embarked, title):\n    import pickle\n\n    x = [[pclass, sex, age, sibsp, parch, fare, embarked, title]]\n    randomforest = pickle.load(open(\"titanic_model.sav\", \"rb\"))\n    predictions = randomforest.predict(x)\n    print(predictions)\n\n\nprediction_model(1, 1, 11, 1, 1, 19, 1, 1)"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html",
    "href": "content/development/code/notebooks/python-fundamentals.html",
    "title": "Python Fundamentals",
    "section": "",
    "text": "Concepts and methods on the fundamentals of Python."
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#creating-a-class",
    "href": "content/development/code/notebooks/python-fundamentals.html#creating-a-class",
    "title": "Python Fundamentals",
    "section": "Creating a class",
    "text": "Creating a class\n\nclass MyClass:\n    x = 55\n    name = \"Allie Trent\"\n    attr_list = [\"Bones\", \"Food\", \"Frisbee\"]\n\n\nMyClass.name\n\n'Allie Trent'"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#methods-in-a-class",
    "href": "content/development/code/notebooks/python-fundamentals.html#methods-in-a-class",
    "title": "Python Fundamentals",
    "section": "Methods in a class",
    "text": "Methods in a class\n\nclass Methods_Class:\n    x = \"Hello World\"\n\n    def my_method():\n        print(\"Contents of my_method\")\n\n    def subtractor(num_one, num_two):\n        product = num_one - num_two\n        print(\"{} - {} = {}\".format(num_two, num_one, product))\n\n    def print_stuff(a, b):\n        print(a, b)\n\n\nprint(Methods_Class)\nprint(Methods_Class.x)\nprint(Methods_Class.my_method)\nprint(Methods_Class.my_method())\nprint(Methods_Class.subtractor(20, 5))\nprint(Methods_Class.print_stuff(3, \"Hello\"))\n\n&lt;class '__main__.Methods_Class'&gt;\nHello World\n&lt;function Methods_Class.my_method at 0xffffab79b400&gt;\nContents of my_method\nNone\n5 - 20 = 15\nNone\n3 Hello\nNone"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#init__",
    "href": "content/development/code/notebooks/python-fundamentals.html#init__",
    "title": "Python Fundamentals",
    "section": "__init__",
    "text": "__init__\nFunction defined at the start of a class\n\nclass Animals:\n    def __init__(self, size, noise, color, num_of_legs):\n        self.size = size\n        self.noise = noise\n        self.color = color\n        self.num_of_legs = num_of_legs\n\n\ndog = Animals(\"Large\", \"Woof\", \"Liver & White\", 4)\ndog\n\n&lt;__main__.Animals at 0xffffab81e590&gt;\n\n\n\ndog.noise\n\n'Woof'"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#using-attributes-in-a-method",
    "href": "content/development/code/notebooks/python-fundamentals.html#using-attributes-in-a-method",
    "title": "Python Fundamentals",
    "section": "Using attributes in a method",
    "text": "Using attributes in a method\n\nclass Animals:\n    def __init__(self, species, name, noise, color):\n        self.species = species\n        self.name = name\n        self.noise = noise\n        self.color = color\n\n    def describe(self):\n        print(\n            \"{} is {} which makes a {} noise\".format(self.name, self.color, self.noise)\n        )\n\n\ndog = Animals(\"Dog\", \"Allie\", \"Woof\", \"Liver & White\")\ncat = Animals(\"Cat\", \"Misty\", \"Meow\", \"Black\")\n\n\ndog.describe()\n\nAllie is Liver & White which makes a Woof noise"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#changing-variable-names-in-a-class-object",
    "href": "content/development/code/notebooks/python-fundamentals.html#changing-variable-names-in-a-class-object",
    "title": "Python Fundamentals",
    "section": "Changing variable names in a class object",
    "text": "Changing variable names in a class object\n\nclass Ages:\n    def __init__(self, age):\n        self.age = age\n\n    def plus_year(self):\n        self.age += 1\n\n    def show_age(self):\n        print(\"Your age is {}\".format(self.age))\n\n\nme = Ages(27)\n\n\nme.plus_year()\nme.show_age()\n\nYour age is 28\n\n\n\nclass Warrior:\n    def __init__(self, name, strength, health):\n        self.name = name\n        self.strength = strength\n        self.health = health\n\n    def report(self):\n        print(\n            \"Hi {}, your strength is {} and health is {}\".format(\n                self.name, self.strength, self.health\n            )\n        )\n\n    def heal(self):\n        self.health += 1\n\n    def damage(self):\n        self.health -= 1\n\n    def workout(self):\n        self.strength += 1\n\n\ncharacter = Warrior(\"Geocoug\", 60, 100)\n\n\n[character.damage() for i in range(8)]\ncharacter.report()\n[character.heal() for i in range(5)]\ncharacter.report()\ncharacter.workout()\ncharacter.report()\n\nHi Geocoug, your strength is 60 and health is 92\nHi Geocoug, your strength is 60 and health is 97\nHi Geocoug, your strength is 61 and health is 97"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#practical-example---payfriend",
    "href": "content/development/code/notebooks/python-fundamentals.html#practical-example---payfriend",
    "title": "Python Fundamentals",
    "section": "Practical Example - PayFriend",
    "text": "Practical Example - PayFriend\nCreate an online bank where: - User can create a new account that includes: Account type (currnet, savings, etc), name of account holder, account balance, etc. - User can withdraw or deposit money, or check balance\n\nclass Banking:\n    def __init__(self, name, account, balance):\n        self.name = name\n        self.account = account\n        self.balance = balance\n\n    def deposit(self, amount):\n        self.balance += amount\n\n    def withdraw(self, amount):\n        self.balance -= amount\n\n    def report(self):\n        print(\n            \"{}, the balance of {} is ${}\".format(self.name, self.account, self.balance)\n        )\n\n\nmy_account = Banking(\"Geocoug\", \"Savings\", 1000)\n\n\nmy_account.deposit(50)\nmy_account.report()\nmy_account.withdraw(500)\nmy_account.report()\n\nGeocoug, the balance of Savings is $1050\nGeocoug, the balance of Savings is $550"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#lambda",
    "href": "content/development/code/notebooks/python-fundamentals.html#lambda",
    "title": "Python Fundamentals",
    "section": "Lambda",
    "text": "Lambda\nQuicker way of creating functions\n\ndef some_func(x):\n    y = x + 2\n\n\nlambda x: x + 2\n\n&lt;function __main__.&lt;lambda&gt;(x)&gt;\n\n\n\nlambda_func = lambda x: x + 2\nlambda_func(6)\n\n8\n\n\n\nother_func = lambda name: \"Hello there, {}\".format(name)\nother_func(\"Geocoug\")\n\n'Hello there, Geocoug'\n\n\n\nanother_func = lambda x, y: x + y\nanother_func(3, 4)\n\n7"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#map",
    "href": "content/development/code/notebooks/python-fundamentals.html#map",
    "title": "Python Fundamentals",
    "section": "Map",
    "text": "Map\nApply the same function/operation on all elements in a list\nmap(name of function, name of list)\n\n# using map as a normal function\nsomelist = [1, 10, 20, 15]\n\n\ndef divider(x):\n    y = x / 5\n    return y\n\n\nnewlist = list(map(divider, somelist))\nnewlist\n\n[0.2, 2.0, 4.0, 3.0]\n\n\n\nsomelist = [1, 10, 20, 15]\nnewlist = list(map(lambda x: x / 5, somelist))\nnewlist\n\n[0.2, 2.0, 4.0, 3.0]\n\n\n\nnum_tuple = (1, 10, 20, 15)\nnew_tuple = tuple(map(lambda x: x / 5, num_tuple))\nnew_tuple\n\n(0.2, 2.0, 4.0, 3.0)\n\n\n\nlist_one = [1, 2, 3, 4]\nlist_two = [90, 80, 70, 60]\n\nnew_list = map(lambda x, y: x + y, list_one, list_two)\nlist(new_list)\n\n[91, 82, 73, 64]"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#filter",
    "href": "content/development/code/notebooks/python-fundamentals.html#filter",
    "title": "Python Fundamentals",
    "section": "Filter",
    "text": "Filter\n\nmy_list = [1, 14, 53, 72, 22, 99]\nfiltered_list = filter(lambda x: x &gt;= 50, my_list)\nlist(filtered_list)\n\n[53, 72, 99]"
  },
  {
    "objectID": "content/development/code/notebooks/python-fundamentals.html#generators",
    "href": "content/development/code/notebooks/python-fundamentals.html#generators",
    "title": "Python Fundamentals",
    "section": "Generators",
    "text": "Generators\nGenerators - yield instead of return. Return terminates local variables, yield ‘pauses’.\nSee - How do python generators work?\n\ndef a():\n    x = 5\n    yield x\n\n\na()\n\n&lt;generator object a at 0xffffab9637d0&gt;\n\n\n\ndef a():\n    x = 5\n    yield x\n    x += 1\n    yield x\n\n\nexample = a()\nprint(next(example))\nprint(next(example))\nprint(next(example))\n\n5\n6\n\n\nStopIteration: \n\n\n\ndef generator_fn(x):\n    for i in range(x):\n        yield i\n\n\nimport sys\n\nsys.getsizeof(example)  # bytes\n\n104\n\n\n\ng = generator_fn(100_000_000)\n\n\ndir(g)\n\n['__class__',\n '__del__',\n '__delattr__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__iter__',\n '__le__',\n '__lt__',\n '__name__',\n '__ne__',\n '__new__',\n '__next__',\n '__qualname__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n 'close',\n 'gi_code',\n 'gi_frame',\n 'gi_running',\n 'gi_yieldfrom',\n 'send',\n 'throw']\n\n\n\nnext(g)\n\n0\n\n\n\nnext(g)\n\n1\n\n\n\nnext(g)\n\n2\n\n\n\nhelp(g)\n\nHelp on generator object:\n\ngenerator_fn = class generator(object)\n |  Methods defined here:\n |  \n |  __del__(...)\n |  \n |  __getattribute__(self, name, /)\n |      Return getattr(self, name).\n |  \n |  __iter__(self, /)\n |      Implement iter(self).\n |  \n |  __next__(self, /)\n |      Implement next(self).\n |  \n |  __repr__(self, /)\n |      Return repr(self).\n |  \n |  close(...)\n |      close() -&gt; raise GeneratorExit inside generator.\n |  \n |  send(...)\n |      send(arg) -&gt; send 'arg' into generator,\n |      return next yielded value or raise StopIteration.\n |  \n |  throw(...)\n |      throw(value)\n |      throw(type[,value[,tb]])\n |      \n |      Raise exception in generator, return next yielded value or raise\n |      StopIteration.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  gi_code\n |  \n |  gi_frame\n |  \n |  gi_running\n |  \n |  gi_yieldfrom\n |      object being iterated by yield from, or None\n\n\n\n\nNested Generators\n\ndef inner():\n    print(\"We're inside\")\n    value = yield 2\n    print(\"Received\", value)\n    return 4\n\n\ndef outer():\n    yield 1\n    retval = yield from inner()\n    print(\"Returned\", retval)\n    yield 5\n\n\ng = outer()\n\n\nnext(g)\n\n1\n\n\n\nnext(g)\n\nWe're inside\n\n\n2\n\n\n\ng.send(3)\n\nReceived 3\nReturned 4\n\n\n5\n\n\n\n\nGenerator vs. List Comprehension\n\nimport time\n\n\nn = 100_000_000\n\n\nstart = time.time()\nx = [i * i for i in range(n)]\nend = time.time()\nprint(type(x))\nprint(\"{:.4f}\".format(end - start))\n\n&lt;class 'list'&gt;\n4.3685\n\n\n\nstart = time.time()\ng = (i * i for i in range(n))\nend = time.time()\nprint(type(g))\nprint(\"{:.4f}\".format(end - start))\n\n&lt;class 'generator'&gt;\n0.0001"
  },
  {
    "objectID": "content/development/code/notebooks/visualization-with-seaborn.html",
    "href": "content/development/code/notebooks/visualization-with-seaborn.html",
    "title": "Visualization with Seaborn",
    "section": "",
    "text": "Matplotlib has proven to be an incredibly useful and popular visualization tool, but even avid users will admit it often leaves much to be desired. There are several valid complaints about Matplotlib that often come up:\nAn answer to these problems is Seaborn. Seaborn provides an API on top of Matplotlib that offers sane choices for plot style and color defaults, defines simple high-level functions for common statistical plot types, and integrates with the functionality provided by Pandas DataFrames.\nTo be fair, the Matplotlib team is addressing this: it has recently added the plt.style tools discussed in Customizing Matplotlib: Configurations and Style Sheets, and is starting to handle Pandas data more seamlessly. The 2.0 release of the library will include a new default stylesheet that will improve on the current status quo. But for all the reasons just discussed, Seaborn remains an extremely useful addon."
  },
  {
    "objectID": "content/development/code/notebooks/visualization-with-seaborn.html#seaborn-versus-matplotlib",
    "href": "content/development/code/notebooks/visualization-with-seaborn.html#seaborn-versus-matplotlib",
    "title": "Visualization with Seaborn",
    "section": "Seaborn Versus Matplotlib",
    "text": "Seaborn Versus Matplotlib\nHere is an example of a simple random-walk plot in Matplotlib, using its classic plot formatting and colors. We start with the typical imports:\n\nimport matplotlib.pyplot as plt\nplt.style.use('classic')\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\nNow we create some random walk data:\n\n# Create some data\nrng = np.random.RandomState(0)\nx = np.linspace(0, 10, 500)\ny = np.cumsum(rng.randn(500, 6), 0)\n\nAnd do a simple plot:\n\n# Plot the data with Matplotlib defaults\nplt.plot(x, y)\nplt.legend(\"ABCDEF\", ncol=2, loc=\"upper left\")\n\n\n\n\nAlthough the result contains all the information we’d like it to convey, it does so in a way that is not all that aesthetically pleasing, and even looks a bit old-fashioned in the context of 21st-century data visualization.\nNow let’s take a look at how it works with Seaborn. As we will see, Seaborn has many of its own high-level plotting routines, but it can also overwrite Matplotlib’s default parameters and in turn get even simple Matplotlib scripts to produce vastly superior output. We can set the style by calling Seaborn’s set() method. By convention, Seaborn is imported as sns:\n\nimport seaborn as sns\n\nsns.set()\n\nNow let’s rerun the same two lines as before:\n\n# same plotting code as above!\nplt.plot(x, y)\nplt.legend(\"ABCDEF\", ncol=2, loc=\"upper left\")\n\n\n\n\nAh, much better!"
  },
  {
    "objectID": "content/development/code/notebooks/visualization-with-seaborn.html#exploring-seaborn-plots",
    "href": "content/development/code/notebooks/visualization-with-seaborn.html#exploring-seaborn-plots",
    "title": "Visualization with Seaborn",
    "section": "Exploring Seaborn Plots",
    "text": "Exploring Seaborn Plots\nThe main idea of Seaborn is that it provides high-level commands to create a variety of plot types useful for statistical data exploration, and even some statistical model fitting.\nLet’s take a look at a few of the datasets and plot types available in Seaborn. Note that all of the following could be done using raw Matplotlib commands (this is, in fact, what Seaborn does under the hood) but the Seaborn API is much more convenient.\n\nHistograms, KDE, and densities\nOften in statistical data visualization, all you want is to plot histograms and joint distributions of variables. We have seen that this is relatively straightforward in Matplotlib:\n\ndata = np.random.multivariate_normal([0, 0], [[5, 2], [2, 2]], size=2000)\ndata = pd.DataFrame(data, columns=[\"x\", \"y\"])\n\nfor col in \"xy\":\n    plt.hist(data[col], normed=True, alpha=0.5)\n\n\n\n\nRather than a histogram, we can get a smooth estimate of the distribution using a kernel density estimation, which Seaborn does with sns.kdeplot:\n\nfor col in \"xy\":\n    sns.kdeplot(data[col], shade=True)\n\n\n\n\nHistograms and KDE can be combined using distplot:\n\nsns.distplot(data[\"x\"])\nsns.distplot(data[\"y\"])\n\n\n\n\nIf we pass the full two-dimensional dataset to kdeplot, we will get a two-dimensional visualization of the data:\n\nsns.kdeplot(data)\n\n\n\n\nWe can see the joint distribution and the marginal distributions together using sns.jointplot. For this plot, we’ll set the style to a white background:\n\nwith sns.axes_style(\"white\"):\n    sns.jointplot(\"x\", \"y\", data, kind=\"kde\")\n\n\n\n\nThere are other parameters that can be passed to jointplot—for example, we can use a hexagonally based histogram instead:\n\nwith sns.axes_style(\"white\"):\n    sns.jointplot(\"x\", \"y\", data, kind=\"hex\")\n\n\n\n\n\n\nPair plots\nWhen you generalize joint plots to datasets of larger dimensions, you end up with pair plots. This is very useful for exploring correlations between multidimensional data, when you’d like to plot all pairs of values against each other.\nWe’ll demo this with the well-known Iris dataset, which lists measurements of petals and sepals of three iris species:\n\niris = sns.load_dataset(\"iris\")\niris.head()\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n\n\nVisualizing the multidimensional relationships among the samples is as easy as calling sns.pairplot:\n\nsns.pairplot(iris, hue=\"species\", size=2.5)\n\n\n\n\n\n\nFaceted histograms\nSometimes the best way to view data is via histograms of subsets. Seaborn’s FacetGrid makes this extremely simple. We’ll take a look at some data that shows the amount that restaurant staff receive in tips based on various indicator data:\n\ntips = sns.load_dataset(\"tips\")\ntips.head()\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n\n\n\n\n\n\ntips[\"tip_pct\"] = 100 * tips[\"tip\"] / tips[\"total_bill\"]\n\ngrid = sns.FacetGrid(tips, row=\"sex\", col=\"time\", margin_titles=True)\ngrid.map(plt.hist, \"tip_pct\", bins=np.linspace(0, 40, 15))\n\n\n\n\n\n\nFactor plots\nFactor plots can be useful for this kind of visualization as well. This allows you to view the distribution of a parameter within bins defined by any other parameter:\n\nwith sns.axes_style(style=\"ticks\"):\n    g = sns.factorplot(\"day\", \"total_bill\", \"sex\", data=tips, kind=\"box\")\n    g.set_axis_labels(\"Day\", \"Total Bill\")\n\n\n\n\n\n\nJoint distributions\nSimilar to the pairplot we saw earlier, we can use sns.jointplot to show the joint distribution between different datasets, along with the associated marginal distributions:\n\nwith sns.axes_style(\"white\"):\n    sns.jointplot(\"total_bill\", \"tip\", data=tips, kind=\"hex\")\n\n\n\n\nThe joint plot can even do some automatic kernel density estimation and regression:\n\nsns.jointplot(\"total_bill\", \"tip\", data=tips, kind=\"reg\")\n\n\n\n\n\n\nBar plots\nTime series can be plotted using sns.factorplot. In the following example, we’ll use the Planets data that we first saw in Aggregation and Grouping:\n\nplanets = sns.load_dataset(\"planets\")\nplanets.head()\n\n\n\n\n\n\n\nmethod\nnumber\norbital_period\nmass\ndistance\nyear\n\n\n\n\n0\nRadial Velocity\n1\n269.300\n7.10\n77.40\n2006\n\n\n1\nRadial Velocity\n1\n874.774\n2.21\n56.95\n2008\n\n\n2\nRadial Velocity\n1\n763.000\n2.60\n19.84\n2011\n\n\n3\nRadial Velocity\n1\n326.030\n19.40\n110.62\n2007\n\n\n4\nRadial Velocity\n1\n516.220\n10.50\n119.47\n2009\n\n\n\n\n\n\n\n\nwith sns.axes_style(\"white\"):\n    g = sns.factorplot(\"year\", data=planets, aspect=2, kind=\"count\", color=\"steelblue\")\n    g.set_xticklabels(step=5)\n\n\n\n\nWe can learn more by looking at the method of discovery of each of these planets:\n\nwith sns.axes_style(\"white\"):\n    g = sns.factorplot(\n        \"year\",\n        data=planets,\n        aspect=4.0,\n        kind=\"count\",\n        hue=\"method\",\n        order=range(2001, 2015),\n    )\n    g.set_ylabels(\"Number of Planets Discovered\")\n\n\n\n\nFor more information on plotting with Seaborn, see the Seaborn documentation, a tutorial, and the Seaborn gallery."
  },
  {
    "objectID": "content/development/code/notebooks/visualization-with-seaborn.html#example-exploring-marathon-finishing-times",
    "href": "content/development/code/notebooks/visualization-with-seaborn.html#example-exploring-marathon-finishing-times",
    "title": "Visualization with Seaborn",
    "section": "Example: Exploring Marathon Finishing Times",
    "text": "Example: Exploring Marathon Finishing Times\nHere we’ll look at using Seaborn to help visualize and understand finishing results from a marathon. I’ve scraped the data from sources on the Web, aggregated it and removed any identifying information, and put it on GitHub where it can be downloaded (if you are interested in using Python for web scraping, I would recommend Web Scraping with Python by Ryan Mitchell). We will start by downloading the data from the Web, and loading it into Pandas:\n\n# !curl -O https://raw.githubusercontent.com/jakevdp/marathon-data/master/marathon-data.csv\n\n\ndata = pd.read_csv(\"marathon-data.csv\")\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n\n\n1\n32\nM\n01:06:26\n02:09:28\n\n\n2\n31\nM\n01:06:49\n02:10:42\n\n\n3\n38\nM\n01:06:16\n02:13:45\n\n\n4\n31\nM\n01:06:32\n02:13:59\n\n\n\n\n\n\n\nBy default, Pandas loaded the time columns as Python strings (type object); we can see this by looking at the dtypes attribute of the DataFrame:\n\ndata.dtypes\n\nage        int64\ngender    object\nsplit     object\nfinal     object\ndtype: object\n\n\nLet’s fix this by providing a converter for the times:\n\nimport datetime\n\n\ndef convert_time(s):\n    h, m, s = map(int, s.split(\":\"))\n    return datetime.timedelta(hours=h, minutes=m, seconds=s)\n\n\ndata = pd.read_csv(\n    \"marathon-data.csv\", converters={\"split\": convert_time, \"final\": convert_time}\n)\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n\n\n1\n32\nM\n01:06:26\n02:09:28\n\n\n2\n31\nM\n01:06:49\n02:10:42\n\n\n3\n38\nM\n01:06:16\n02:13:45\n\n\n4\n31\nM\n01:06:32\n02:13:59\n\n\n\n\n\n\n\n\ndata.dtypes\n\nage                 int64\ngender             object\nsplit     timedelta64[ns]\nfinal     timedelta64[ns]\ndtype: object\n\n\nThat looks much better. For the purpose of our Seaborn plotting utilities, let’s next add columns that give the times in seconds:\n\ndata[\"split_sec\"] = data[\"split\"].astype(int) / 1e9\ndata[\"final_sec\"] = data[\"final\"].astype(int) / 1e9\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\nsplit_sec\nfinal_sec\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n3938.0\n7731.0\n\n\n1\n32\nM\n01:06:26\n02:09:28\n3986.0\n7768.0\n\n\n2\n31\nM\n01:06:49\n02:10:42\n4009.0\n7842.0\n\n\n3\n38\nM\n01:06:16\n02:13:45\n3976.0\n8025.0\n\n\n4\n31\nM\n01:06:32\n02:13:59\n3992.0\n8039.0\n\n\n\n\n\n\n\nTo get an idea of what the data looks like, we can plot a jointplot over the data:\n\nwith sns.axes_style(\"white\"):\n    g = sns.jointplot(\"split_sec\", \"final_sec\", data, kind=\"hex\")\n    g.ax_joint.plot(np.linspace(4000, 16000), np.linspace(8000, 32000), \":k\")\n\n\n\n\nThe dotted line shows where someone’s time would lie if they ran the marathon at a perfectly steady pace. The fact that the distribution lies above this indicates (as you might expect) that most people slow down over the course of the marathon. If you have run competitively, you’ll know that those who do the opposite—run faster during the second half of the race—are said to have “negative-split” the race.\nLet’s create another column in the data, the split fraction, which measures the degree to which each runner negative-splits or positive-splits the race:\n\ndata[\"split_frac\"] = 1 - 2 * data[\"split_sec\"] / data[\"final_sec\"]\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\nsplit_sec\nfinal_sec\nsplit_frac\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n3938.0\n7731.0\n-0.018756\n\n\n1\n32\nM\n01:06:26\n02:09:28\n3986.0\n7768.0\n-0.026262\n\n\n2\n31\nM\n01:06:49\n02:10:42\n4009.0\n7842.0\n-0.022443\n\n\n3\n38\nM\n01:06:16\n02:13:45\n3976.0\n8025.0\n0.009097\n\n\n4\n31\nM\n01:06:32\n02:13:59\n3992.0\n8039.0\n0.006842\n\n\n\n\n\n\n\nWhere this split difference is less than zero, the person negative-split the race by that fraction. Let’s do a distribution plot of this split fraction:\n\nsns.distplot(data[\"split_frac\"], kde=False)\nplt.axvline(0, color=\"k\", linestyle=\"--\")\n\n\n\n\n\nsum(data.split_frac &lt; 0)\n\n251\n\n\nOut of nearly 40,000 participants, there were only 250 people who negative-split their marathon.\nLet’s see whether there is any correlation between this split fraction and other variables. We’ll do this using a pairgrid, which draws plots of all these correlations:\n\ng = sns.PairGrid(\n    data,\n    vars=[\"age\", \"split_sec\", \"final_sec\", \"split_frac\"],\n    hue=\"gender\",\n    palette=\"RdBu_r\",\n)\ng.map(plt.scatter, alpha=0.8)\ng.add_legend()\n\n\n\n\nIt looks like the split fraction does not correlate particularly with age, but does correlate with the final time: faster runners tend to have closer to even splits on their marathon time. (We see here that Seaborn is no panacea for Matplotlib’s ills when it comes to plot styles: in particular, the x-axis labels overlap. Because the output is a simple Matplotlib plot, however, the methods in Customizing Ticks can be used to adjust such things if desired.)\nThe difference between men and women here is interesting. Let’s look at the histogram of split fractions for these two groups:\n\nsns.kdeplot(data.split_frac[data.gender == \"M\"], label=\"men\", shade=True)\nsns.kdeplot(data.split_frac[data.gender == \"W\"], label=\"women\", shade=True)\nplt.xlabel(\"split_frac\")\n\n\n\n\nThe interesting thing here is that there are many more men than women who are running close to an even split! This almost looks like some kind of bimodal distribution among the men and women. Let’s see if we can suss-out what’s going on by looking at the distributions as a function of age.\nA nice way to compare distributions is to use a violin plot\n\nsns.violinplot(\"gender\", \"split_frac\", data=data, palette=[\"lightblue\", \"lightpink\"])\n\n\n\n\nThis is yet another way to compare the distributions between men and women.\nLet’s look a little deeper, and compare these violin plots as a function of age. We’ll start by creating a new column in the array that specifies the decade of age that each person is in:\n\ndata[\"age_dec\"] = data.age.map(lambda age: 10 * (age // 10))\ndata.head()\n\n\n\n\n\n\n\nage\ngender\nsplit\nfinal\nsplit_sec\nfinal_sec\nsplit_frac\nage_dec\n\n\n\n\n0\n33\nM\n01:05:38\n02:08:51\n3938.0\n7731.0\n-0.018756\n30\n\n\n1\n32\nM\n01:06:26\n02:09:28\n3986.0\n7768.0\n-0.026262\n30\n\n\n2\n31\nM\n01:06:49\n02:10:42\n4009.0\n7842.0\n-0.022443\n30\n\n\n3\n38\nM\n01:06:16\n02:13:45\n3976.0\n8025.0\n0.009097\n30\n\n\n4\n31\nM\n01:06:32\n02:13:59\n3992.0\n8039.0\n0.006842\n30\n\n\n\n\n\n\n\n\nmen = data.gender == \"M\"\nwomen = data.gender == \"W\"\n\nwith sns.axes_style(style=None):\n    sns.violinplot(\n        \"age_dec\",\n        \"split_frac\",\n        hue=\"gender\",\n        data=data,\n        split=True,\n        inner=\"quartile\",\n        palette=[\"lightblue\", \"lightpink\"],\n    )\n\n\n\n\nLooking at this, we can see where the distributions of men and women differ: the split distributions of men in their 20s to 50s show a pronounced over-density toward lower splits when compared to women of the same age (or of any age, for that matter).\nAlso surprisingly, the 80-year-old women seem to outperform everyone in terms of their split time. This is probably due to the fact that we’re estimating the distribution from small numbers, as there are only a handful of runners in that range:\n\n(data.age &gt; 80).sum()\n\n7\n\n\nBack to the men with negative splits: who are these runners? Does this split fraction correlate with finishing quickly? We can plot this very easily. We’ll use regplot, which will automatically fit a linear regression to the data:\n\ng = sns.lmplot(\n    \"final_sec\",\n    \"split_frac\",\n    col=\"gender\",\n    data=data,\n    markers=\".\",\n    scatter_kws=dict(color=\"c\"),\n)\ng.map(plt.axhline, y=0.1, color=\"k\", ls=\":\")\n\n\n\n\nApparently the people with fast splits are the elite runners who are finishing within ~15,000 seconds, or about 4 hours. People slower than that are much less likely to have a fast second split."
  }
]